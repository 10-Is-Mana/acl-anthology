<volume id='Q17'>
  <paper id='1000'>
    <title>Transactions of the Association of Computational Linguistics – Volume 5, Issue 1</title>
  </paper>

  <paper id='1001'>
    <title>Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Labels</title>
    <author><first>Alison</first><last>Smith</last></author>
    <author><first>Tak Yeon</first><last>Lee</last></author>
    <author><first>Forough</first><last>Poursabzi-Sangdeh</last></author>
    <author><first>Jordan</first><last>Boyd-Graber</last></author>
    <author><first>Niklas</first><last>Elmqvist</last></author>
    <author><first>Leah</first><last>Findlater</last></author>
    <year>2017</year>
    <abstract>Probabilistic topic models are important tools for indexing, summarizing, and analyzing large document collections by their themes.  However, promoting end-user understanding of topics remains an open research problem.  We compare labels generated by users given four topic visualization techniques—word lists, word lists with bars, word clouds, and network graphs—against each other and against automatically generated labels.  Our basis of comparison is participant ratings of how well labels describe documents from the topic. Our study has two phases: a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics' documents.  Although all visualizations produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure.  Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/887/222</href>
    <pages>1--15</pages>
    <url>http://www.aclweb.org/anthology/Q17-1001</url>
  </paper>

  <paper id='1002'>
    <title>Visually Grounded and Textual Semantic Models Differentially Decode Brain Activity Associated with Concrete and Abstract Nouns</title>
    <author><first>Andrew J.</first><last>Anderson</last></author>
    <author><first>Douwe</first><last>Kiela</last></author>
    <author><first>Stephen</first><last>Clark</last></author>
    <author><first>Massimo</first><last>Poesio</last></author>
    <year>2017</year>
    <abstract>Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almost exclusively focused on concrete nouns. How well these models extend to decoding abstract nouns is largely unknown. We address this question by applying state-of-the-art computational models to decode functional Magnetic Resonance Imaging (fMRI) activity patterns, elicited by participants reading and imagining a diverse set of both concrete and abstract nouns. One of the models we use is linguistic, exploiting the recent word2vec skipgram approach trained on Wikipedia. The second is visually grounded, using deep convolutional neural networks trained on Google Images. Dual coding theory considers concrete concepts to be encoded in the brain both linguistically and visually, and abstract concepts only linguistically. Splitting the fMRI data according to human concreteness ratings, we indeed observe that both models significantly decode the most concrete nouns; however, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current computational models are sufficiently advanced to assist in investigating the representational structure of abstract concepts in the brain.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/879/223</href>
    <pages>17--30</pages>
    <url>http://www.aclweb.org/anthology/Q17-1002</url>
  </paper>

  <paper id='1003'>
    <title>Modelling Semantic Expectation: Using Script Knowledge for Referent Prediction</title>
    <author><first>Ashutosh</first><last>Modi</last></author>
    <author><first>Ivan</first><last>Titov</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <author><first>Asad</first><last>Sayeed</last></author>
    <author><first>Manfred</first><last>Pinkal</last></author>
    <year>2017</year>
    <abstract></abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/968/224</href>
    <pages>31--44</pages>
    <url>http://www.aclweb.org/anthology/Q17-1003</url>
  </paper>


</volume>
  