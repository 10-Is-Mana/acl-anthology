<volume id='Q17'>
  <paper id='1000'>
    <title>Transactions of the Association of Computational Linguistics – Volume 5, Issue 1</title>
  </paper>

  <paper id='1001'>
    <title>Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Labels</title>
    <author><first>Alison</first><last>Smith</last></author>
    <author><first>Tak Yeon</first><last>Lee</last></author>
    <author><first>Forough</first><last>Poursabzi-Sangdeh</last></author>
    <author><first>Jordan</first><last>Boyd-Graber</last></author>
    <author><first>Niklas</first><last>Elmqvist</last></author>
    <author><first>Leah</first><last>Findlater</last></author>
    <year>2017</year>
    <abstract>Probabilistic topic models are important tools for indexing, summarizing, and analyzing large document collections by their themes.  However, promoting end-user understanding of topics remains an open research problem.  We compare labels generated by users given four topic visualization techniques—word lists, word lists with bars, word clouds, and network graphs—against each other and against automatically generated labels.  Our basis of comparison is participant ratings of how well labels describe documents from the topic. Our study has two phases: a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics' documents.  Although all visualizations produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure.  Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/887/222</href>
    <pages>1--15</pages>
    <url>http://www.aclweb.org/anthology/Q17-1001</url>
  </paper>

  <paper id='1002'>
    <title>Visually Grounded and Textual Semantic Models Differentially Decode Brain Activity Associated with Concrete and Abstract Nouns</title>
    <author><first>Andrew J.</first><last>Anderson</last></author>
    <author><first>Douwe</first><last>Kiela</last></author>
    <author><first>Stephen</first><last>Clark</last></author>
    <author><first>Massimo</first><last>Poesio</last></author>
    <year>2017</year>
    <abstract>Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almost exclusively focused on concrete nouns. How well these models extend to decoding abstract nouns is largely unknown. We address this question by applying state-of-the-art computational models to decode functional Magnetic Resonance Imaging (fMRI) activity patterns, elicited by participants reading and imagining a diverse set of both concrete and abstract nouns. One of the models we use is linguistic, exploiting the recent word2vec skipgram approach trained on Wikipedia. The second is visually grounded, using deep convolutional neural networks trained on Google Images. Dual coding theory considers concrete concepts to be encoded in the brain both linguistically and visually, and abstract concepts only linguistically. Splitting the fMRI data according to human concreteness ratings, we indeed observe that both models significantly decode the most concrete nouns; however, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current computational models are sufficiently advanced to assist in investigating the representational structure of abstract concepts in the brain.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/879/223</href>
    <pages>17--30</pages>
    <url>http://www.aclweb.org/anthology/Q17-1002</url>
  </paper>

  <paper id='1003'>
    <title>Modelling Semantic Expectation: Using Script Knowledge for Referent Prediction</title>
    <author><first>Ashutosh</first><last>Modi</last></author>
    <author><first>Ivan</first><last>Titov</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <author><first>Asad</first><last>Sayeed</last></author>
    <author><first>Manfred</first><last>Pinkal</last></author>
    <year>2017</year>
    <abstract>Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/968/224</href>
    <pages>31--44</pages>
    <url>http://www.aclweb.org/anthology/Q17-1003</url>
  </paper>

  <paper id='1004'>
    <title>Shift-Reduce Constituent Parsing with Neural Lookahead Features</title>
    <author><first>Jiangming</first><last>Liu</last></author>
    <author><first>Yue</first><last>Zhang</last></author>
    <year>2017</year>
    <abstract>Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which consists of a sequence of non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, giving the highest reported accuracies for fully-supervised parsing.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/927/225</href>
    <pages>45--58</pages>
    <url>http://www.aclweb.org/anthology/Q17-1004</url>
  </paper>

  <paper id='1005'>
    <title>A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit</title>
    <author><first>Yin-Wen</first><last>Chang</last></author>
    <author><first>Michael</first><last>Collins</last></author>
    <year>2017</year>
    <abstract>Decoding of phrase-based translation models in the general case is known to be NP-complete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation. However, the impact on complexity after imposing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lh^{d+1}) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/1020/226</href>
    <pages>59--71</pages>
    <url>http://www.aclweb.org/anthology/Q17-1005</url>
  </paper>

  <paper id='1006'>
    <title>A Generative Model of Phonotactics</title>
    <author><first>Richard</first><last>Futrell</last></author>
    <author><first>Adam</first><last>Albright</last></author>
    <author><first>Peter</first><last>Graff</last></author>
    <author><first>Timothy J.</first><last>O'Donnell</last></author>
    <year>2017</year>
    <abstract>We present a probabilistic model of phonotactics, the set of well-formed phoneme sequences in a language. Unlike most computational models of phonotactics (Hayes and Wilson, 2008; Goldsmith and Riggle, 2012), we take a fully generative approach, modeling a process where forms are built up out of subparts by phonologically-informed structure building operations. We learn an inventory of subparts by applying stochastic memoization (Johnson et al., 2006; Goodman et al., 2008) to a generative process for phonemes structured as an and-or graph, based on concepts of feature hierarchy from generative phonology (Clements, 1985; Dresher, 2009). Subparts are combined in a way that allows tier-based feature interactions. We evaluate our models’ ability to capture phonotactic distributions in the lexicons of 14 languages drawn from the WOLEX corpus (Graff, 2012). Our full model robustly assigns higher probabilities to held-out forms than a sophisticated N-gram model for all languages. We also present novel analyses that probe model behavior in more detail.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/971/227</href>
    <pages>73--86</pages>
    <url>http://www.aclweb.org/anthology/Q17-1006</url>
  </paper>

  <paper id='1007'>
    <title>Context Gates for Neural Machine Translation</title>
    <author><first>Zhaopeng</first><last>Tu</last></author>
    <author><first>Yang</first><last>Liu</last></author>
    <author><first>Zhengdong</first><last>Lu</last></author>
    <author><first>Xiaohua</first><last>Liu</last></author>
    <author><first>Hang</first><last>Li</last></author>
    <year>2017</year>
    <abstract>In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts affect the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to the lack of effective control over the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose context gates which dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance both the adequacy and fluency of NMT with more careful control of the information flow from contexts. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/948/228</href>
    <pages>87-99</pages>
    <url>http://www.aclweb.org/anthology/Q17-1007</url>
  </paper>

  <paper id='1008'>
    <title>Cross-Sentence N-ary Relation Extraction with Graph LSTMs</title>
    <author><first>Nanyun</first><last>Peng</last></author>
    <author><first>Hoifung</first><last>Poon</last></author>
    <author><first>Chris</first><last>Quirk</last></author>
    <author><first>Kristina</first><last>Toutanova</last></author>
    <author><first>Wen-tau</first><last>Yih</last></author>
    <year>2017</year>
    <abstract>Past work in relation extraction focuses on binary relations in single sentences. Recent NLP inroads in high-valued domains have kindled strong interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory (graph LSTM), which can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a unifying way to explore different LSTM approaches and incorporate various intra-sentential and inter-sentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classifier, making it easy for scaling to arbitrary relation arity n, as well as for multi-task learning with related relations. We evaluated this framework in two important domains in precision medicine and demonstrated its effectiveness with both supervised learning and distant supervision. Cross-sentence extraction produced far more knowledge, and multi-task learning significantly improved extraction accuracy. A thorough analysis comparing various LSTM approaches yielded interesting insight on how linguistic analysis impacts the performance.</abstract>
    <href>https://transacl.org/ojs/index.php/tacl/article/download/1028/229</href>
    <pages>101--115</pages>
    <url>http://www.aclweb.org/anthology/Q17-1008</url>
  </paper>

</volume>
  