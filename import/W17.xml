<?xml version='1.0' encoding='UTF-8'?> 
<volume id='W17'>
  <paper id='0100'>
    <title>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </title>
    <editor>Antti Arppe</editor>
    <editor>Jeff Good</editor>
    <editor>Mans Hulden</editor>
    <editor>Jordan Lachler</editor>
    <editor>Alexis Palmer</editor>
    <editor>Lane Schwartz</editor>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-01</url>
    <doi>10.18653/v1/W17-01</doi>
    <bibtype>book</bibtype>
    <bibkey>W17-01:2017</bibkey>
  </paper>
  <paper id='0101'>
    <title>A Morphological Parser for Odawa</title>
    <author>
      <first>Dustin</first>
      <last>Bowers</last>
    </author>
    <author>
      <first>Antti</first>
      <last>Arppe</last>
    </author>
    <author>
      <first>Jordan</first>
      <last>Lachler</last>
    </author>
    <author>
      <first>Sjur</first>
      <last>Moshagen</last>
    </author>
    <author>
      <first>Trond</first>
      <last>Trosterud</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W17-0101</url>
    <doi>10.18653/v1/W17-0101</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bowers-EtAl:2017:W17-01</bibkey>
  </paper>
  <paper id='0102'>
    <title>Creating lexical resources for polysynthetic languages–-the case of Arapaho</title>
    <author>
      <first>Ghazaleh</first>
      <last>Kazeminejad</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Cowell</last>
    </author>
    <author>
      <first>Mans</first>
      <last>Hulden</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–18</pages>
    <url>http://www.aclweb.org/anthology/W17-0102</url>
    <doi>10.18653/v1/W17-0102</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kazeminejad-cowell-hulden:2017:W17-01</bibkey>
  </paper>
  <paper id='0103'>
    <title>
      From Small to Big Data: paper manuscripts to RDF triples of Australian
      Indigenous Vocabularies
    </title>
    <author>
      <first>Nick</first>
      <last>Thieberger</last>
    </author>
    <author>
      <first>Conal</first>
      <last>Tuohy</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>19–23</pages>
    <url>http://www.aclweb.org/anthology/W17-0103</url>
    <doi>10.18653/v1/W17-0103</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>thieberger-tuohy:2017:W17-01</bibkey>
  </paper>
  <paper id='0104'>
    <title>
      Issues in digital text representation, on-line dissemination, sharing and
      re-use for African minority languages
    </title>
    <author>
      <first>Emmanuel Ngué</first>
      <last>Um</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–32</pages>
    <url>http://www.aclweb.org/anthology/W17-0104</url>
    <doi>10.18653/v1/W17-0104</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>um:2017:W17-01</bibkey>
  </paper>
  <paper id='0105'>
    <title>
      Developing collection management tools to create more robust and reliable
      linguistic data
    </title>
    <author>
      <first>Gary</first>
      <last>Holton</last>
    </author>
    <author>
      <first>Kavon</first>
      <last>Hooshiar</last>
    </author>
    <author>
      <first>Nick</first>
      <last>Thieberger</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33–38</pages>
    <url>http://www.aclweb.org/anthology/W17-0105</url>
    <doi>10.18653/v1/W17-0105</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>holton-hooshiar-thieberger:2017:W17-01</bibkey>
  </paper>
  <paper id='0106'>
    <title>STREAMLInED Challenges: Aligning Research Interests with Shared Tasks</title>
    <author>
      <first>Gina-Anne</first>
      <last>Levow</last>
    </author>
    <author>
      <first>Emily M.</first>
      <last>Bender</last>
    </author>
    <author>
      <first>Patrick</first>
      <last>Littell</last>
    </author>
    <author>
      <first>Kristen</first>
      <last>Howell</last>
    </author>
    <author>
      <first>Shobhana</first>
      <last>Chelliah</last>
    </author>
    <author>
      <first>Joshua</first>
      <last>Crowgey</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Garrette</last>
    </author>
    <author>
      <first>Jeff</first>
      <last>Good</last>
    </author>
    <author>
      <first>Sharon</first>
      <last>Hargus</last>
    </author>
    <author>
      <first>David</first>
      <last>Inman</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Maxwell</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Tjalve</last>
    </author>
    <author>
      <first>Fei</first>
      <last>Xia</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–47</pages>
    <url>http://www.aclweb.org/anthology/W17-0106</url>
    <doi>10.18653/v1/W17-0106</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>levow-EtAl:2017:W17-01</bibkey>
  </paper>
  <paper id='0107'>
    <title>Work With What You've Got</title>
    <author>
      <first>Lucy</first>
      <last>Bell</last>
    </author>
    <author>
      <first>Lawrence</first>
      <last>Bell</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–51</pages>
    <url>http://www.aclweb.org/anthology/W17-0107</url>
    <doi>10.18653/v1/W17-0107</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bell-bell:2017:W17-01</bibkey>
  </paper>
  <paper id='0108'>
    <title>
      Converting a comprehensive lexical database into a computational model:
      The case of East Cree verb inflection
    </title>
    <author>
      <first>Antti</first>
      <last>Arppe</last>
    </author>
    <author>
      <first>Marie-Odile</first>
      <last>Junker</last>
    </author>
    <author>
      <first>Delasie</first>
      <last>Torkornoo</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–56</pages>
    <url>http://www.aclweb.org/anthology/W17-0108</url>
    <doi>10.18653/v1/W17-0108</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>arppe-junker-torkornoo:2017:W17-01</bibkey>
  </paper>
  <paper id='0109'>
    <title>
      Instant annotations in ELAN corpora of spoken and written Komi, an
      endangered language of the Barents Sea region
    </title>
    <author>
      <first>Ciprian</first>
      <last>Gerstenberger</last>
    </author>
    <author>
      <first>Niko</first>
      <last>Partanen</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Rießler</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–66</pages>
    <url>http://www.aclweb.org/anthology/W17-0109</url>
    <doi>10.18653/v1/W17-0109</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gerstenberger-partanen-riessler:2017:W17-01</bibkey>
  </paper>
  <paper id='0110'>
    <title>Inferring Case Systems from IGT: Enriching the Enrichment</title>
    <author>
      <first>Kristen</first>
      <last>Howell</last>
    </author>
    <author>
      <first>Emily M.</first>
      <last>Bender</last>
    </author>
    <author>
      <first>Michel</first>
      <last>Lockwood</last>
    </author>
    <author>
      <first>Fei</first>
      <last>Xia</last>
    </author>
    <author>
      <first>Olga</first>
      <last>Zamaraeva</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <url>http://www.aclweb.org/anthology/W17-0110</url>
    <doi>10.18653/v1/W17-0110</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>howell-EtAl:2017:W17-01</bibkey>
  </paper>
  <paper id='0111'>
    <title>Case Studies in the Automatic Characterization of Grammars from Small Wordlists</title>
    <author>
      <first>Jordan</first>
      <last>Kodner</last>
    </author>
    <author>
      <first>Spencer</first>
      <last>Kaplan</last>
    </author>
    <author>
      <first>Hongzhi</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Mitchell P.</first>
      <last>Marcus</last>
    </author>
    <author>
      <first>Charles</first>
      <last>Yang</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–84</pages>
    <url>http://www.aclweb.org/anthology/W17-0111</url>
    <doi>10.18653/v1/W17-0111</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kodner-EtAl:2017:W17-01</bibkey>
  </paper>
  <paper id='0112'>
    <title>Endangered Data for Endangered Languages: Digitizing Print dictionaries</title>
    <author>
      <first>Michael</first>
      <last>Maxwell</last>
    </author>
    <author>
      <first>Aric</first>
      <last>Bills</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>85–91</pages>
    <url>http://www.aclweb.org/anthology/W17-0112</url>
    <doi>10.18653/v1/W17-0112</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>maxwell-bills:2017:W17-01</bibkey>
  </paper>
  <paper id='0113'>
    <title>
      A computationally-assisted procedure for discovering poetic organization
      within oral tradition
    </title>
    <author>
      <first>David</first>
      <last>Meyer</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>92–100</pages>
    <url>http://www.aclweb.org/anthology/W17-0113</url>
    <doi>10.18653/v1/W17-0113</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>meyer:2017:W17-01</bibkey>
  </paper>
  <paper id='0114'>
    <title>
      Improving Coverage of an Inuktitut Morphological Analyzer Using a
      Segmental Recurrent Neural Network
    </title>
    <author>
      <first>Jeffrey</first>
      <last>Micher</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–106</pages>
    <url>http://www.aclweb.org/anthology/W17-0114</url>
    <doi>10.18653/v1/W17-0114</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>micher:2017:W17-01</bibkey>
  </paper>
  <paper id='0115'>
    <title>
      Click reduction in fluent speech: a semi-automated analysis of Mangetti
      Dune !Xung
    </title>
    <author>
      <first>Amanda</first>
      <last>Miller</last>
    </author>
    <author>
      <first>Micha</first>
      <last>Elsner</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>107–115</pages>
    <url>http://www.aclweb.org/anthology/W17-0115</url>
    <doi>10.18653/v1/W17-0115</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>miller-elsner:2017:W17-01</bibkey>
  </paper>
  <paper id='0116'>
    <title>
      DECCA Repurposed: Detecting transcription inconsistencies without an
      orthographic standard
    </title>
    <author>
      <first>C. Anton</first>
      <last>Rytting</last>
    </author>
    <author>
      <first>Julie</first>
      <last>Yelle</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>116–121</pages>
    <url>http://www.aclweb.org/anthology/W17-0116</url>
    <doi>10.18653/v1/W17-0116</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rytting-yelle:2017:W17-01</bibkey>
  </paper>
  <paper id='0117'>
    <title>
      Jejueo talking dictionary: A collaborative online database for language
      revitalization
    </title>
    <author>
      <first>Moira</first>
      <last>Saltzman</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>122–129</pages>
    <url>http://www.aclweb.org/anthology/W17-0117</url>
    <doi>10.18653/v1/W17-0117</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>saltzman:2017:W17-01</bibkey>
  </paper>
  <paper id='0118'>
    <title>Computational Support for Finding Word Classes: A Case Study of Abui</title>
    <author>
      <first>Olga</first>
      <last>Zamaraeva</last>
    </author>
    <author>
      <first>František</first>
      <last>Kratochvíl</last>
    </author>
    <author>
      <first>Emily M.</first>
      <last>Bender</last>
    </author>
    <author>
      <first>Fei</first>
      <last>Xia</last>
    </author>
    <author>
      <first>Kristen</first>
      <last>Howell</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>130–140</pages>
    <url>http://www.aclweb.org/anthology/W17-0118</url>
    <doi>10.18653/v1/W17-0118</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>zamaraeva-EtAl:2017:W17-01</bibkey>
  </paper>
  <paper id='0119'>
    <title>
      Waldayu and Waldayu Mobile: Modern digital dictionary interfaces for
      endangered languages
    </title>
    <author>
      <first>Patrick</first>
      <last>Littell</last>
    </author>
    <author>
      <first>Aidan</first>
      <last>Pine</last>
    </author>
    <author>
      <first>Henry</first>
      <last>Davis</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>141–150</pages>
    <url>http://www.aclweb.org/anthology/W17-0119</url>
    <doi>10.18653/v1/W17-0119</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>littell-pine-davis:2017:W17-01</bibkey>
  </paper>
  <paper id='0120'>
    <title>Connecting Documentation and Revitalization: A New Approach to Language Apps</title>
    <author>
      <first>Alexa N.</first>
      <last>Little</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>151–155</pages>
    <url>http://www.aclweb.org/anthology/W17-0120</url>
    <doi>10.18653/v1/W17-0120</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>little:2017:W17-01</bibkey>
  </paper>
  <paper id='0121'>
    <title>
      Developing a Suite of Mobile Applications for Collaborative Language
      Documentation
    </title>
    <author>
      <first>Mat</first>
      <last>Bettinson</last>
    </author>
    <author>
      <first>Steven</first>
      <last>Bird</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>156–164</pages>
    <url>http://www.aclweb.org/anthology/W17-0121</url>
    <doi>10.18653/v1/W17-0121</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>bettinson-bird:2017:W17-01</bibkey>
  </paper>
  <paper id='0122'>
    <title>
      Cross-language forced alignment to assist community-based linguistics for
      low resource languages
    </title>
    <author>
      <first>Timothy</first>
      <last>Kempton</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>165–169</pages>
    <url>http://www.aclweb.org/anthology/W17-0122</url>
    <doi>10.18653/v1/W17-0122</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kempton:2017:W17-01</bibkey>
  </paper>
  <paper id='0123'>
    <title>
      A case study on using speech-to-translation alignments for language
      documentation
    </title>
    <author>
      <first>Antonios</first>
      <last>Anastasopoulos</last>
    </author>
    <author>
      <first>David</first>
      <last>Chiang</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on the Use of Computational Methods in the
      Study of Endangered Languages
    </booktitle>
    <month>March</month>
    <year>2017</year>
    <address>Honolulu</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>170–178</pages>
    <url>http://www.aclweb.org/anthology/W17-0123</url>
    <doi>10.18653/v1/W17-0123</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>anastasopoulos-chiang:2017:W17-01</bibkey>
  </paper>
  <paper id='0200'>
    <title>Proceedings of the 21st Nordic Conference on Computational Linguistics</title>
    <editor>Jörg Tiedemann</editor>
    <editor>Nina Tahmasebi</editor>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-02</url>
    <bibtype>book</bibtype>
    <bibkey>NoDaLiDa:2017</bibkey>
  </paper>
  <paper id='0201'>
    <title>Joint UD Parsing of Norwegian Bokmål and Nynorsk</title>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <author>
      <first>Lilja</first>
      <last>Øvrelid</last>
    </author>
    <author>
      <first>Petter</first>
      <last>Hohle</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-0201</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>velldal-ovrelid-hohle:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0202'>
    <title>Replacing OOV Words For Dependency Parsing With Distributional Semantics</title>
    <author>
      <first>Prasanth</first>
      <last>Kolachina</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Riedl</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Biemann</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–19</pages>
    <url>http://www.aclweb.org/anthology/W17-0202</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kolachina-riedl-biemann:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0203'>
    <title>Real-valued Syntactic Word Vectors (RSV) for Greedy Neural Dependency Parsing</title>
    <author>
      <first>Ali</first>
      <last>Basirat</last>
    </author>
    <author>
      <first>Joakim</first>
      <last>Nivre</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20–28</pages>
    <url>http://www.aclweb.org/anthology/W17-0203</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>basirat-nivre:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0204'>
    <title>
      Tagging Named Entities in 19th Century and Modern Finnish Newspaper
      Material with a Finnish Semantic Tagger
    </title>
    <author>
      <first>Kimmo</first>
      <last>Kettunen</last>
    </author>
    <author>
      <first>Laura</first>
      <last>Löfberg</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–36</pages>
    <url>http://www.aclweb.org/anthology/W17-0204</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kettunen-lofberg:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0205'>
    <title>
      Machine Learning for Rhetorical Figure Detection: More Chiasmus with Less
      Annotation
    </title>
    <author>
      <first>Marie</first>
      <last>Dubremetz</last>
    </author>
    <author>
      <first>Joakim</first>
      <last>Nivre</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–45</pages>
    <url>http://www.aclweb.org/anthology/W17-0205</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>dubremetz-nivre:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0206'>
    <title>Coreference Resolution for Swedish and German using Distant Supervision</title>
    <author>
      <first>Alexander</first>
      <last>Wallin</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Nugues</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–55</pages>
    <url>http://www.aclweb.org/anthology/W17-0206</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wallin-nugues:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0207'>
    <title>Aligning phonemes using finte-state methods</title>
    <author>
      <first>Kimmo</first>
      <last>Koskenniemi</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–64</pages>
    <url>http://www.aclweb.org/anthology/W17-0207</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>koskenniemi:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0208'>
    <title>Acoustic Model Compression with MAP adaptation</title>
    <author>
      <first>Katri</first>
      <last>Leino</last>
    </author>
    <author>
      <first>Mikko</first>
      <last>Kurimo</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>65–69</pages>
    <url>http://www.aclweb.org/anthology/W17-0208</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>leino-kurimo:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0209'>
    <title>OCR and post-correction of historical Finnish texts</title>
    <author>
      <first>Senka</first>
      <last>Drobac</last>
    </author>
    <author>
      <first>Pekka</first>
      <last>Kauppinen</last>
    </author>
    <author>
      <first>Krister</first>
      <last>Lindén</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>70–76</pages>
    <url>http://www.aclweb.org/anthology/W17-0209</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>drobac-kauppinen-linden:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0210'>
    <title>Twitter Topic Modeling by Tweet Aggregation</title>
    <author>
      <first>Asbjørn</first>
      <last>Steinskog</last>
    </author>
    <author>
      <first>Jonas</first>
      <last>Therkelsen</last>
    </author>
    <author>
      <first>Björn</first>
      <last>Gambäck</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>77–86</pages>
    <url>http://www.aclweb.org/anthology/W17-0210</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>steinskog-therkelsen-gamback:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0211'>
    <title>A Multilingual Entity Linker Using PageRank and Semantic Graphs</title>
    <author>
      <first>Anton</first>
      <last>Södergren</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Nugues</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–95</pages>
    <url>http://www.aclweb.org/anthology/W17-0211</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>sodergren-nugues:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0212'>
    <title>Linear Ensembles of Word Embedding Models</title>
    <author>
      <first>Avo</first>
      <last>Muromägi</last>
    </author>
    <author>
      <first>Kairit</first>
      <last>Sirts</last>
    </author>
    <author>
      <first>Sven</first>
      <last>Laur</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>96–104</pages>
    <url>http://www.aclweb.org/anthology/W17-0212</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>muromagi-sirts-laur:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0213'>
    <title>
      Using Pseudowords for Algorithm Comparison: An Evaluation Framework for
      Graph-based Word Sense Induction
    </title>
    <author>
      <first>Flavio</first>
      <last>Massimiliano Cecchini</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Biemann</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Riedl</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>105–114</pages>
    <url>http://www.aclweb.org/anthology/W17-0213</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>massimilianocecchini-biemann-riedl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0214'>
    <title>North-Sámi to Finnish rule-based machine translation system</title>
    <author>
      <first>Tommi</first>
      <last>Pirinen</last>
    </author>
    <author>
      <first>Francis M.</first>
      <last>Tyers</last>
    </author>
    <author>
      <first>Trond</first>
      <last>Trosterud</last>
    </author>
    <author>
      <first>Ryan</first>
      <last>Johnson</last>
    </author>
    <author>
      <first>Kevin</first>
      <last>Unhammer</last>
    </author>
    <author>
      <first>Tiina</first>
      <last>Puolakainen</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115–122</pages>
    <url>http://www.aclweb.org/anthology/W17-0214</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pirinen-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0215'>
    <title>Machine translation with North Saami as a pivot language</title>
    <author>
      <first>Lene</first>
      <last>Antonsen</last>
    </author>
    <author>
      <first>Ciprian</first>
      <last>Gerstenberger</last>
    </author>
    <author>
      <first>Maja</first>
      <last>Kappfjell</last>
    </author>
    <author>
      <first>Sandra</first>
      <last>Nystø Ráhka</last>
    </author>
    <author>
      <first>Marja-Liisa</first>
      <last>Olthuis</last>
    </author>
    <author>
      <first>Trond</first>
      <last>Trosterud</last>
    </author>
    <author>
      <first>Francis M.</first>
      <last>Tyers</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>123–131</pages>
    <url>http://www.aclweb.org/anthology/W17-0215</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>antonsen-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0216'>
    <title>
      SWEGRAM ėxtendash A Web-Based Tool for Automatic Annotation and Analysis
      of Swedish Texts
    </title>
    <author>
      <first>Jesper</first>
      <last>Näsman</last>
    </author>
    <author>
      <first>Beáta</first>
      <last>Megyesi</last>
    </author>
    <author>
      <first>Anne</first>
      <last>Palmér</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>132–141</pages>
    <url>http://www.aclweb.org/anthology/W17-0216</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>nasman-megyesi-palmer:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0217'>
    <title>Optimizing a PoS Tagset for Norwegian Dependency Parsing</title>
    <author>
      <first>Petter</first>
      <last>Hohle</last>
    </author>
    <author>
      <first>Lilja</first>
      <last>Øvrelid</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>142–151</pages>
    <url>http://www.aclweb.org/anthology/W17-0217</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>hohle-ovrelid-velldal:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0218'>
    <title>Creating register sub-corpora for the Finnish Internet Parsebank</title>
    <author>
      <first>Veronika</first>
      <last>Laippala</last>
    </author>
    <author>
      <first>Juhani</first>
      <last>Luotolahti</last>
    </author>
    <author>
      <first>Aki-Juhani</first>
      <last>Kyröläinen</last>
    </author>
    <author>
      <first>Tapio</first>
      <last>Salakoski</last>
    </author>
    <author>
      <first>Filip</first>
      <last>Ginter</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>152–161</pages>
    <url>http://www.aclweb.org/anthology/W17-0218</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>laippala-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0219'>
    <title>
      KILLE: a Framework for Situated Agents for Learning Language Through
      Interaction
    </title>
    <author>
      <first>Simon</first>
      <last>Dobnik</last>
    </author>
    <author>
      <first>Erik</first>
      <last>de Graaf</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>162–171</pages>
    <url>http://www.aclweb.org/anthology/W17-0219</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>dobnik-degraaf:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0220'>
    <title>
      Data Collection from Persons with Mild Forms of Cognitive Impairment and
      Healthy Controls - Infrastructure for Classification and Prediction of
      Dementia
    </title>
    <author>
      <first>Dimitrios</first>
      <last>Kokkinakis</last>
    </author>
    <author>
      <first>Kristina</first>
      <last>Lundholm Fors</last>
    </author>
    <author>
      <first>Eva</first>
      <last>Björkner</last>
    </author>
    <author>
      <first>Arto</first>
      <last>Nordlund</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>172–182</pages>
    <url>http://www.aclweb.org/anthology/W17-0220</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kokkinakis-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0221'>
    <title>Evaluation of language identification methods using 285 languages</title>
    <author>
      <first>Tommi</first>
      <last>Jauhiainen</last>
    </author>
    <author>
      <first>Krister</first>
      <last>Lindén</last>
    </author>
    <author>
      <first>Heidi</first>
      <last>Jauhiainen</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>183–191</pages>
    <url>http://www.aclweb.org/anthology/W17-0221</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>jauhiainen-linden-jauhiainen:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0222'>
    <title>Can We Create a Tool for General Domain Event Analysis?</title>
    <author>
      <first>Siim</first>
      <last>Orasmaa</last>
    </author>
    <author>
      <first>Heiki-Jaan</first>
      <last>Kaalep</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>192–201</pages>
    <url>http://www.aclweb.org/anthology/W17-0222</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>orasmaa-kaalep:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0223'>
    <title>From Treebank to Propbank: A Semantic-Role and VerbNet Corpus for Danish</title>
    <author>
      <first>Eckhard</first>
      <last>Bick</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>202–210</pages>
    <url>http://www.aclweb.org/anthology/W17-0223</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bick:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0224'>
    <title>
      Cross-lingual Learning of Semantic Textual Similarity with Multilingual
      Word Representations
    </title>
    <author>
      <first>Johannes</first>
      <last>Bjerva</last>
    </author>
    <author>
      <first>Robert</first>
      <last>Östling</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>211–215</pages>
    <url>http://www.aclweb.org/anthology/W17-0224</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bjerva-ostling:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0225'>
    <title>
      Will my auxiliary tagging task help? Estimating Auxiliary Tasks
      Effectivity in Multi-Task Learning
    </title>
    <author>
      <first>Johannes</first>
      <last>Bjerva</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>216–220</pages>
    <url>http://www.aclweb.org/anthology/W17-0225</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bjerva:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0226'>
    <title>
      Iconic Locations in Swedish Sign Language: Mapping Form to Meaning with
      Lexical Databases
    </title>
    <author>
      <first>Carl</first>
      <last>Börstell</last>
    </author>
    <author>
      <first>Robert</first>
      <last>Östling</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>221–225</pages>
    <url>http://www.aclweb.org/anthology/W17-0226</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>borstell-ostling:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0227'>
    <title>Docforia: A Multilayer Document Model</title>
    <author>
      <first>Marcus</first>
      <last>Klang</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Nugues</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>226–230</pages>
    <url>http://www.aclweb.org/anthology/W17-0227</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>klang-nugues:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0228'>
    <title>Finnish resources for evaluating language model semantics</title>
    <author>
      <first>Viljami</first>
      <last>Venekoski</last>
    </author>
    <author>
      <first>Jouko</first>
      <last>Vankka</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>231–236</pages>
    <url>http://www.aclweb.org/anthology/W17-0228</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>venekoski-vankka:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0229'>
    <title>Málrómur: A Manually Verified Corpus of Recorded Icelandic Speech</title>
    <author>
      <first>Steinþór</first>
      <last>Steingrímsson</last>
    </author>
    <author>
      <first>Jón</first>
      <last>Guðnason</last>
    </author>
    <author>
      <first>Sigrún</first>
      <last>Helgadóttir</last>
    </author>
    <author>
      <first>Eiríkur</first>
      <last>Rögnvaldsson</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>237–240</pages>
    <url>http://www.aclweb.org/anthology/W17-0229</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>steingrimsson-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0230'>
    <title>The Effect of Translationese on Tuning for Statistical Machine Translation</title>
    <author>
      <first>Sara</first>
      <last>Stymne</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>241–246</pages>
    <url>http://www.aclweb.org/anthology/W17-0230</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>stymne:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0231'>
    <title>Multilingwis2 ėxtendash Explore Your Parallel Corpus</title>
    <author>
      <first>Johannes</first>
      <last>Graën</last>
    </author>
    <author>
      <first>Dominique</first>
      <last>Sandoz</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Volk</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>247–250</pages>
    <url>http://www.aclweb.org/anthology/W17-0231</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>graen-sandoz-volk:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0232'>
    <title>A modernised version of the Glossa corpus search system</title>
    <author>
      <first>Anders</first>
      <last>Nøklestad</last>
    </author>
    <author>
      <first>Kristin</first>
      <last>Hagen</last>
    </author>
    <author>
      <first>Janne</first>
      <last>Bondi Johannessen</last>
    </author>
    <author>
      <first>Michał</first>
      <last>Kosek</last>
    </author>
    <author>
      <first>Joel</first>
      <last>Priestley</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>251–254</pages>
    <url>http://www.aclweb.org/anthology/W17-0232</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>noklestad-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0233'>
    <title>Dep_search: Efficient Search Tool for Large Dependency Parsebanks</title>
    <author>
      <first>Juhani</first>
      <last>Luotolahti</last>
    </author>
    <author>
      <first>Jenna</first>
      <last>Kanerva</last>
    </author>
    <author>
      <first>Filip</first>
      <last>Ginter</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>255–258</pages>
    <url>http://www.aclweb.org/anthology/W17-0233</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>luotolahti-kanerva-ginter:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0234'>
    <title>
      Proto-Indo-European Lexicon: The Generative Etymological Dictionary of
      Indo-European Languages
    </title>
    <author>
      <first>Jouna</first>
      <last>Pyysalo</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>259–262</pages>
    <url>http://www.aclweb.org/anthology/W17-0234</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pyysalo:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0235'>
    <title>Tilde MODEL - Multilingual Open Data for EU Languages</title>
    <author>
      <first>Roberts</first>
      <last>Rozis</last>
    </author>
    <author>
      <first>Raivis</first>
      <last>Skadiņš</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>263–265</pages>
    <url>http://www.aclweb.org/anthology/W17-0235</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rozis-skadicnvs:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0236'>
    <title>Mainstreaming August Strindberg with Text Normalization</title>
    <author>
      <first>Adam</first>
      <last>Ek</last>
    </author>
    <author>
      <first>Sofia</first>
      <last>Knuutinen</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>266–270</pages>
    <url>http://www.aclweb.org/anthology/W17-0236</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ek-knuutinen:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0237'>
    <title>
      Word vectors, reuse, and replicability: Towards a community repository of
      large-text resources
    </title>
    <author>
      <first>Murhaf</first>
      <last>Fares</last>
    </author>
    <author>
      <first>Andrey</first>
      <last>Kutuzov</last>
    </author>
    <author>
      <first>Stephan</first>
      <last>Oepen</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>271–276</pages>
    <url>http://www.aclweb.org/anthology/W17-0237</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>fares-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0238'>
    <title>
      Improving Optical Character Recognition of Finnish Historical Newspapers
      with a Combination of Fraktur & Antiqua Models and Image Preprocessing
    </title>
    <author>
      <first>Mika</first>
      <last>Koistinen</last>
    </author>
    <author>
      <first>Kimmo</first>
      <last>Kettunen</last>
    </author>
    <author>
      <first>Tuula</first>
      <last>Pääkkönen</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>277–283</pages>
    <url>http://www.aclweb.org/anthology/W17-0238</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>koistinen-kettunen-paakkonen:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0239'>
    <title>Redefining Context Windows for Word Embedding Models: An Experimental Study</title>
    <author>
      <first>Pierre</first>
      <last>Lison</last>
    </author>
    <author>
      <first>Andrey</first>
      <last>Kutuzov</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>284–288</pages>
    <url>http://www.aclweb.org/anthology/W17-0239</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lison-kutuzov:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0240'>
    <title>
      The Effect of Excluding Out of Domain Training Data from Supervised
      Named-Entity Recognition
    </title>
    <author>
      <first>Adam</first>
      <last>Persson</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>289–292</pages>
    <url>http://www.aclweb.org/anthology/W17-0240</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>persson:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0241'>
    <title>Quote Extraction and Attribution from Norwegian Newspapers</title>
    <author>
      <first>Andrew</first>
      <last>Salway</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Meurer</last>
    </author>
    <author>
      <first>Knut</first>
      <last>Hofland</last>
    </author>
    <author>
      <first>Øystein</first>
      <last>Reigem</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>293–297</pages>
    <url>http://www.aclweb.org/anthology/W17-0241</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>salway-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0242'>
    <title>Wordnet extension via word embeddings: Experiments on the Norwegian Wordnet</title>
    <author>
      <first>Heidi</first>
      <last>Sand</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <author>
      <first>Lilja</first>
      <last>Øvrelid</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>298–302</pages>
    <url>http://www.aclweb.org/anthology/W17-0242</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>sand-velldal-ovrelid:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0243'>
    <title>Universal Dependencies for Swedish Sign Language</title>
    <author>
      <first>Robert</first>
      <last>Östling</last>
    </author>
    <author>
      <first>Carl</first>
      <last>Börstell</last>
    </author>
    <author>
      <first>Moa</first>
      <last>Gärdenfors</last>
    </author>
    <author>
      <first>Mats</first>
      <last>Wirén</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>303–308</pages>
    <url>http://www.aclweb.org/anthology/W17-0243</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ostling-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0244'>
    <title>Services for text simplification and analysis</title>
    <author>
      <first>Johan</first>
      <last>Falkenjack</last>
    </author>
    <author>
      <first>Evelina</first>
      <last>Rennes</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Fahlborg</last>
    </author>
    <author>
      <first>Vida</first>
      <last>Johansson</last>
    </author>
    <author>
      <first>Arne</first>
      <last>Jönsson</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>309–313</pages>
    <url>http://www.aclweb.org/anthology/W17-0244</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>falkenjack-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0245'>
    <title>
      Exploring Properties of Intralingual and Interlingual Association Measures
      Visually
    </title>
    <author>
      <first>Johannes</first>
      <last>Graën</last>
    </author>
    <author>
      <first>Christof</first>
      <last>Bless</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>314–317</pages>
    <url>http://www.aclweb.org/anthology/W17-0245</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>graen-bless:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0246'>
    <title>TALERUM - Learning Danish by Doing Danish</title>
    <author>
      <first>Peter</first>
      <last>Juel Henrichsen</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>318–321</pages>
    <url>http://www.aclweb.org/anthology/W17-0246</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>juelhenrichsen:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0247'>
    <title>
      Cross-Lingual Syntax: Relating Grammatical Framework with Universal
      Dependencies
    </title>
    <author>
      <first>Aarne</first>
      <last>Ranta</last>
    </author>
    <author>
      <first>Prasanth</first>
      <last>Kolachina</last>
    </author>
    <author>
      <first>Thomas</first>
      <last>Hallgren</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>322–325</pages>
    <url>http://www.aclweb.org/anthology/W17-0247</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ranta-kolachina-hallgren:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0248'>
    <title>Exploring Treebanks with INESS Search</title>
    <author>
      <first>Victoria</first>
      <last>Rosén</last>
    </author>
    <author>
      <first>Helge</first>
      <last>Dyvik</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Meurer</last>
    </author>
    <author>
      <first>Koenraad</first>
      <last>De Smedt</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>326–329</pages>
    <url>http://www.aclweb.org/anthology/W17-0248</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rosen-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0249'>
    <title>
      A System for Identifying and Exploring Text Repetition in Large Historical
      Document Corpora
    </title>
    <author>
      <first>Aleksi</first>
      <last>Vesanto</last>
    </author>
    <author>
      <first>Filip</first>
      <last>Ginter</last>
    </author>
    <author>
      <first>Hannu</first>
      <last>Salmi</last>
    </author>
    <author>
      <first>Asko</first>
      <last>Nivala</last>
    </author>
    <author>
      <first>Tapio</first>
      <last>Salakoski</last>
    </author>
    <booktitle>Proceedings of the 21st Nordic Conference on Computational Linguistics</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>330–333</pages>
    <url>http://www.aclweb.org/anthology/W17-0249</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>vesanto-EtAl:2017:NoDaLiDa</bibkey>
  </paper>
  <paper id='0300'>
    <title>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </title>
    <editor>Elena Volodina</editor>
    <editor>Gintarė Grigonytė</editor>
    <editor>Ildikó Pilán</editor>
    <editor>Kristina Nilsson Björkenstam</editor>
    <editor>Lars Borin</editor>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <url>http://www.aclweb.org/anthology/W17-0300</url>
    <bibtype>book</bibtype>
    <bibkey>W17-03:2017</bibkey>
  </paper>
  <paper id='0301'>
    <title>Learning with learner corpora: Using the TLE for native language identification</title>
    <author>
      <first>Allison</first>
      <last>Adams</last>
    </author>
    <author>
      <first>Sara</first>
      <last>Stymne</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>1-7</pages>
    <url>http://www.aclweb.org/anthology/W17-0301</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>adams:2017:W17-03-01</bibkey>
  </paper>
  <paper id='0302'>
    <title>
      Challenging learners in their individual zone of proximal development
      using pedagogic developmental benchmarks of syntactic complexity
    </title>
    <author>
      <first>Xiaobin</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>8-17</pages>
    <url>http://www.aclweb.org/anthology/W17-0302</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen:2017:W17-03-02</bibkey>
  </paper>
  <paper id='0303'>
    <title>
      Crossing the border twice: Reimporting prepositions to alleviate
      L1-specific transfer errors
    </title>
    <author>
      <first>Johannes</first>
      <last>Graën</last>
    </author>
    <author>
      <first>Gerold</first>
      <last>Schneider</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>18-26</pages>
    <url>http://www.aclweb.org/anthology/W17-0303</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>graen:2017:W17-03-03</bibkey>
  </paper>
  <paper id='0304'>
    <title>Revita: a system for language learning and supporting endangered languages</title>
    <author>
      <first>Anisia</first>
      <last>Katinskaia</last>
    </author>
    <author>
      <first>Javad</first>
      <last>Nouri</last>
    </author>
    <author>
      <first>Roman</first>
      <last>Yangarber</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>27-35</pages>
    <url>http://www.aclweb.org/anthology/W17-0304</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>katinskaia:2017:W17-03-04</bibkey>
  </paper>
  <paper id='0305'>
    <title>
      Developing a web-based workbook for English supporting the interaction of
      students and teachers
    </title>
    <author>
      <first>Björn</first>
      <last>Rudzewitz</last>
    </author>
    <author>
      <first>Ramon</first>
      <last>Ziai</last>
    </author>
    <author>
      <first>Kordula</first>
      <last>De Kuthy</last>
    </author>
    <author>
      <first>Detmar</first>
      <last>Meurers</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>36-46</pages>
    <url>http://www.aclweb.org/anthology/W17-0305</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rudzewitz:2017:W17-03-05</bibkey>
  </paper>
  <paper id='0306'>
    <title>Annotating errors in student texts: First experiences and experiments</title>
    <author>
      <first>Sara</first>
      <last>Stymne</last>
    </author>
    <author>
      <first>Eva</first>
      <last>Pettersson</last>
    </author>
    <author>
      <first>Beáta</first>
      <last>Megyesi</last>
    </author>
    <author>
      <first>Anne</first>
      <last>Palmér</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>47-60</pages>
    <url>http://www.aclweb.org/anthology/W17-0306</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>surname:2017:W17-03-06</bibkey>
  </paper>
  <paper id='0307'>
    <title>
      Building and using language resources and infrastructure to develop
      e-learning programs for a minority language
    </title>
    <author>
      <first>Heli</first>
      <last>Uibo</last>
    </author>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <author>
      <first>Sulev</first>
      <last>Iva</last>
    </author>
    <booktitle>
      Proceedings of the joint workshop on NLP for Computer Assisted Language
      Learning and NLP for Language Acquisition
    </booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>LiU Electronic Press</publisher>
    <pages>61-67</pages>
    <url>http://www.aclweb.org/anthology/W17-0307</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>surname:2017:W17-03-07</bibkey>
  </paper>
  <paper id='0400'>
    <title>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</title>
    <editor>Marie-Catherine de Marneffe</editor>
    <editor>Joakim Nivre</editor>
    <editor>Sebastian Schuster</editor>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-04</url>
    <bibtype>book</bibtype>
    <bibkey>UDW:2017</bibkey>
  </paper>
  <paper id='0401'>
    <title>Cross-Lingual Parser Selection for Low-Resource Languages</title>
    <author>
      <first>Željko</first>
      <last>Agić</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-0401</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>agic:2017:UDW</bibkey>
  </paper>
  <paper id='0402'>
    <title>Swedish Prepositions are not Pure Function Words</title>
    <author>
      <first>Lars</first>
      <last>Ahrenberg</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–18</pages>
    <url>http://www.aclweb.org/anthology/W17-0402</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ahrenberg:2017:UDW</bibkey>
  </paper>
  <paper id='0403'>
    <title>
      Increasing Return on Annotation Investment: The Automatic Construction of
      a Universal Dependency Treebank for Dutch
    </title>
    <author>
      <first>Gosse</first>
      <last>Bouma</last>
    </author>
    <author>
      <first>Gertjan</first>
      <last>Van Noord</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>19–26</pages>
    <url>http://www.aclweb.org/anthology/W17-0403</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bouma-vannoord:2017:UDW</bibkey>
  </paper>
  <paper id='0404'>
    <title>Converting the TüBa-D/Z Treebank of German to Universal Dependencies</title>
    <author>
      <first>Çağrı</first>
      <last>Çöltekin</last>
    </author>
    <author>
      <first>Ben</first>
      <last>Campbell</last>
    </author>
    <author>
      <first>Erhard</first>
      <last>Hinrichs</last>
    </author>
    <author>
      <first>Heike</first>
      <last>Telljohann</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–37</pages>
    <url>http://www.aclweb.org/anthology/W17-0404</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ccoltekin-EtAl:2017:UDW</bibkey>
  </paper>
  <paper id='0405'>
    <title>Universal Dependencies for Afrikaans</title>
    <author>
      <first>Peter</first>
      <last>Dirix</last>
    </author>
    <author>
      <first>Liesbeth</first>
      <last>Augustinus</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>van Niekerk</last>
    </author>
    <author>
      <first>Frank</first>
      <last>Van Eynde</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–47</pages>
    <url>http://www.aclweb.org/anthology/W17-0405</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>dirix-EtAl:2017:UDW</bibkey>
  </paper>
  <paper id='0406'>
    <title>Elliptic Constructions: Spotting Patterns in UD Treebanks</title>
    <author>
      <first>Kira</first>
      <last>Droganova</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Zeman</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–57</pages>
    <url>http://www.aclweb.org/anthology/W17-0406</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>droganova-zeman:2017:UDW</bibkey>
  </paper>
  <paper id='0407'>
    <title>Dependency Tree Transformation with Tree Transducers</title>
    <author>
      <first>Felix</first>
      <last>Hennig</last>
    </author>
    <author>
      <first>Arne</first>
      <last>Köhn</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>58–66</pages>
    <url>http://www.aclweb.org/anthology/W17-0407</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>hennig-kohn:2017:UDW</bibkey>
  </paper>
  <paper id='0408'>
    <title>Towards Universal Dependencies for Learner Chinese</title>
    <author>
      <first>John</first>
      <last>Lee</last>
    </author>
    <author>
      <first>Herman</first>
      <last>Leung</last>
    </author>
    <author>
      <first>Keying</first>
      <last>Li</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–71</pages>
    <url>http://www.aclweb.org/anthology/W17-0408</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>lee-leung-li:2017:UDW</bibkey>
  </paper>
  <paper id='0409'>
    <title>
      Does Syntactic Informativity Predict Word Length? A Cross-Linguistic Study
      Based on the Universal Dependencies Corpora
    </title>
    <author>
      <first>Natalia</first>
      <last>Levshina</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72–78</pages>
    <url>http://www.aclweb.org/anthology/W17-0409</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>levshina:2017:UDW</bibkey>
  </paper>
  <paper id='0410'>
    <title>Estonian Copular and Existential Constructions as an UD Annotation Problem</title>
    <author>
      <first>Kadri</first>
      <last>Muischnek</last>
    </author>
    <author>
      <first>Kaili</first>
      <last>Müürisep</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–85</pages>
    <url>http://www.aclweb.org/anthology/W17-0410</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>muischnek-muurisep:2017:UDW</bibkey>
  </paper>
  <paper id='0411'>
    <title>Universal Dependency Evaluation</title>
    <author>
      <first>Joakim</first>
      <last>Nivre</last>
    </author>
    <author>
      <first>Chiao-Ting</first>
      <last>Fang</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>86–95</pages>
    <url>http://www.aclweb.org/anthology/W17-0411</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>nivre-fang:2017:UDW</bibkey>
  </paper>
  <paper id='0412'>
    <title>Udapi: Universal API for Universal Dependencies</title>
    <author>
      <first>Martin</first>
      <last>Popel</last>
    </author>
    <author>
      <first>Zdenĕk</first>
      <last>Žabokrtský</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Vojtek</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>96–101</pages>
    <url>http://www.aclweb.org/anthology/W17-0412</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>popel-vzabokrtsky-vojtek:2017:UDW</bibkey>
  </paper>
  <paper id='0413'>
    <title>Universal Dependencies for Greek</title>
    <author>
      <first>Prokopis</first>
      <last>Prokopidis</last>
    </author>
    <author>
      <first>Haris</first>
      <last>Papageorgiou</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–106</pages>
    <url>http://www.aclweb.org/anthology/W17-0413</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>prokopidis-papageorgiou:2017:UDW</bibkey>
  </paper>
  <paper id='0414'>
    <title>From Universal Dependencies to Abstract Syntax</title>
    <author>
      <first>Aarne</first>
      <last>Ranta</last>
    </author>
    <author>
      <first>Prasanth</first>
      <last>Kolachina</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>107–116</pages>
    <url>http://www.aclweb.org/anthology/W17-0414</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ranta-kolachina:2017:UDW</bibkey>
  </paper>
  <paper id='0415'>
    <title>Empirically Sampling Universal Dependencies</title>
    <author>
      <first>Natalie</first>
      <last>Schluter</last>
    </author>
    <author>
      <first>Željko</first>
      <last>Agić</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>117–122</pages>
    <url>http://www.aclweb.org/anthology/W17-0415</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>schluter-agic:2017:UDW</bibkey>
  </paper>
  <paper id='0416'>
    <title>Gapping Constructions in Universal Dependencies v2</title>
    <author>
      <first>Sebastian</first>
      <last>Schuster</last>
    </author>
    <author>
      <first>Matthew</first>
      <last>Lamm</last>
    </author>
    <author>
      <first>Christopher D.</first>
      <last>Manning</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>123–132</pages>
    <url>http://www.aclweb.org/anthology/W17-0416</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>schuster-lamm-manning:2017:UDW</bibkey>
  </paper>
  <paper id='0417'>
    <title>Toward Universal Dependencies for Ainu</title>
    <author>
      <first>Hajime</first>
      <last>Senuma</last>
    </author>
    <author>
      <first>Akiko</first>
      <last>Aizawa</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>133–139</pages>
    <url>http://www.aclweb.org/anthology/W17-0417</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>senuma-aizawa:2017:UDW</bibkey>
  </paper>
  <paper id='0418'>
    <title>
      Automatic Morpheme Segmentation and Labeling in Universal Dependencies
      Resources
    </title>
    <author>
      <first>Miikka</first>
      <last>Silfverberg</last>
    </author>
    <author>
      <first>Mans</first>
      <last>Hulden</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>140–145</pages>
    <url>http://www.aclweb.org/anthology/W17-0418</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>silfverberg-hulden:2017:UDW</bibkey>
  </paper>
  <paper id='0419'>
    <title>A Systematic Comparison of Syntactic Representations of Dependency Parsing</title>
    <author>
      <first>Guillaume</first>
      <last>Wisniewski</last>
    </author>
    <author>
      <first>Ophélie</first>
      <last>Lacroix</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</booktitle>
    <month>May</month>
    <year>2017</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>146–152</pages>
    <url>http://www.aclweb.org/anthology/W17-0419</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wisniewski-lacroix:2017:UDW</bibkey>
  </paper>
  <paper id='0500'>
    <title>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</title>
    <editor>Gerlof Bouma</editor>
    <editor>Yvonne Adesam</editor>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <url>http://www.aclweb.org/anthology/W17-05</url>
    <bibtype>book</bibtype>
  </paper>
  <paper id='0501'>
    <title>
      Variance in Historical Data: How bad is it and how can we profit from it
      for historical linguistics?
    </title>
    <author>
      <first>Stefanie</first>
      <last>Dipper</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>1–1</pages>
    <url>http://www.aclweb.org/anthology/W17-0501</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0502'>
    <title>Improving POS Tagging in Old Spanish Using TEITOK</title>
    <author>
      <first>Maarten</first>
      <last>Janssen</last>
    </author>
    <author>
      <first>Josep</first>
      <last>Ausensi</last>
    </author>
    <author>
      <first>Josep</first>
      <last>Fontana</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>2–6</pages>
    <url>http://www.aclweb.org/anthology/W17-0502</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0503'>
    <title>The Making of the Royal Society Corpus</title>
    <author>
      <first>Jörg</first>
      <last>Knappen</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Fischer</last>
    </author>
    <author>
      <first>Hannah</first>
      <last>Kermes</last>
    </author>
    <author>
      <first>Elke</first>
      <last>Teich</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Fankhauser</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>7–11</pages>
    <url>http://www.aclweb.org/anthology/W17-0503</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0504'>
    <title>Normalizing Medieval German Texts: from rules to deep learning</title>
    <author>
      <first>Natalia</first>
      <last>Korchagina</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>12–17</pages>
    <url>http://www.aclweb.org/anthology/W17-0504</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0505'>
    <title>
      Ambiguity in Semantically Related Word Substitutions: an investigation in
      historical Bible translations
    </title>
    <author>
      <first>Maria</first>
      <last>Moritz</last>
    </author>
    <author>
      <first>Marco</first>
      <last>Büchler</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>18–23</pages>
    <url>http://www.aclweb.org/anthology/W17-0505</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0506'>
    <title>The Lemlat 3.0 Package for Morphological Analysis of Latin</title>
    <author>
      <first>Marco</first>
      <last>Passarotti</last>
    </author>
    <author>
      <first>Marco</first>
      <last>Budassi</last>
    </author>
    <author>
      <first>Eleonora</first>
      <last>Litta</last>
    </author>
    <author>
      <first>Paolo</first>
      <last>Ruffolo</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>24–31</pages>
    <url>http://www.aclweb.org/anthology/W17-0506</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0507'>
    <title>HistoBankVis: Detecting Language Change via Data Visualization</title>
    <author>
      <first>Christin</first>
      <last>Schätzle</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Hund</last>
    </author>
    <author>
      <first>Frederik</first>
      <last>Dennig</last>
    </author>
    <author>
      <first>Miriam</first>
      <last>Butt</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Keim</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>32–39</pages>
    <url>http://www.aclweb.org/anthology/W17-0507</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0508'>
    <title>
      Comparing Rule-based and SMT-based Spelling Normalisation for English
      Historical Texts
    </title>
    <author>
      <first>Gerold</first>
      <last>Schneider</last>
    </author>
    <author>
      <first>Eva</first>
      <last>Pettersson</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Percillier</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>40–46</pages>
    <url>http://www.aclweb.org/anthology/W17-0508</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0509'>
    <title>Data-driven Morphology and Sociolinguistics for Early Modern Dutch</title>
    <author>
      <first>Marijn</first>
      <last>Schraagen</last>
    </author>
    <author>
      <first>Marjo</first>
      <von>van</von>
      <last>Koppen</last>
    </author>
    <author>
      <first>Feike</first>
      <last>Dietz</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>47–53</pages>
    <url>http://www.aclweb.org/anthology/W17-0509</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0510'>
    <title>
      Applying BLAST to Text Reuse Detection in Finnish Newspapers and Journals,
      1771-1910
    </title>
    <author>
      <first>Aleksi</first>
      <last>Vesanto</last>
    </author>
    <author>
      <first>Asko</first>
      <last>Nivala</last>
    </author>
    <author>
      <first>Heli</first>
      <last>Rantala</last>
    </author>
    <author>
      <first>Tapio</first>
      <last>Salakoski</last>
    </author>
    <author>
      <first>Hannu</first>
      <last>Salmi</last>
    </author>
    <author>
      <first>Filip</first>
      <last>Ginter</last>
    </author>
    <booktitle>Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language</booktitle>
    <month>May</month>
    <address>Gothenburg</address>
    <year>2017</year>
    <publisher>Linköping University Electronic Press</publisher>
    <pages>54–58</pages>
    <url>http://www.aclweb.org/anthology/W17-0509</url>
    <bibtype>inproceedings</bibtype>
  </paper>
  <paper id='0600'>
    <title>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </title>
    <editor>Francis M. Tyers</editor>
    <editor>Michael Rießler</editor>
    <editor>Tommi A. Pirinen</editor>
    <editor>Trond Trosterud</editor>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-06</url>
    <doi>10.18653/v1/W17-06</doi>
    <bibtype>book</bibtype>
    <bibkey>IWCLUL:2017</bibkey>
  </paper>
  <paper id='0601'>
    <title>Synchronized Mediawiki based analyzer dictionary development</title>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <author>
      <first>Mika</first>
      <last>Hämäläinen</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–7</pages>
    <url>http://www.aclweb.org/anthology/W17-0601</url>
    <doi>10.18653/v1/W17-0601</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rueter-hamalainen:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0602'>
    <title>
      DEMO: Giellatekno Open-source click-in-text dictionaries for bringing
      closely related languages into contact.
    </title>
    <author>
      <first>Jack</first>
      <last>Rueter</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–9</pages>
    <url>http://www.aclweb.org/anthology/W17-0602</url>
    <doi>10.18653/v1/W17-0602</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rueter:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0603'>
    <title>Languages under the influence: Building a database of Uralic languages</title>
    <author>
      <first>Eszter</first>
      <last>Simon</last>
    </author>
    <author>
      <first>Nikolett</first>
      <last>Mus</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–24</pages>
    <url>http://www.aclweb.org/anthology/W17-0603</url>
    <doi>10.18653/v1/W17-0603</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>simon-mus:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0604'>
    <title>
      Instant Annotations ėxtendash Applying NLP Methods to the Annotation of
      Spoken Language Documentation Corpora
    </title>
    <author>
      <first>Ciprian</first>
      <last>Gerstenberger</last>
    </author>
    <author>
      <first>Niko</first>
      <last>Partanen</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Rießler</last>
    </author>
    <author>
      <first>Joshua</first>
      <last>Wilbur</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–36</pages>
    <url>http://www.aclweb.org/anthology/W17-0604</url>
    <doi>10.18653/v1/W17-0604</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gerstenberger-EtAl:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0605'>
    <title>
      Preliminary Experiments concerning Verbal Predicative Structure Extraction
      from a Large Finnish Corpus
    </title>
    <author>
      <first>Guersande</first>
      <last>Chaminade</last>
    </author>
    <author>
      <first>Thierry</first>
      <last>Poibeau</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–55</pages>
    <url>http://www.aclweb.org/anthology/W17-0605</url>
    <doi>10.18653/v1/W17-0605</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>chaminade-poibeau:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0606'>
    <title>Language technology resources and tools for Mansi: an overview</title>
    <author>
      <first>Csilla</first>
      <last>Horváth</last>
    </author>
    <author>
      <first>Norbert</first>
      <last>Szilágyi</last>
    </author>
    <author>
      <first>Veronika</first>
      <last>Vincze</last>
    </author>
    <author>
      <first>Àgoston</first>
      <last>Nagy</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–65</pages>
    <url>http://www.aclweb.org/anthology/W17-0606</url>
    <doi>10.18653/v1/W17-0606</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>horvath-EtAl:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0607'>
    <title>Annotation schemes in North Sámi dependency parsing</title>
    <author>
      <first>Francis</first>
      <last>M. Tyers</last>
    </author>
    <author>
      <first>Mariya</first>
      <last>Sheyanova</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–75</pages>
    <url>http://www.aclweb.org/anthology/W17-0607</url>
    <doi>10.18653/v1/W17-0607</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>mtyers-sheyanova:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0608'>
    <title>A morphological analyser for Kven</title>
    <author>
      <first>Sindre</first>
      <last>Reino Trosterud</last>
    </author>
    <author>
      <first>Trond</first>
      <last>Trosterud</last>
    </author>
    <author>
      <first>Anna-Kaisa</first>
      <last>Räisänen</last>
    </author>
    <author>
      <first>Leena</first>
      <last>Niiranen</last>
    </author>
    <author>
      <first>Mervi</first>
      <last>Haavisto</last>
    </author>
    <author>
      <first>Kaisa</first>
      <last>Maliniemi</last>
    </author>
    <booktitle>
      Proceedings of the Third Workshop on Computational Linguistics for Uralic
      Languages
    </booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–88</pages>
    <url>http://www.aclweb.org/anthology/W17-0608</url>
    <doi>10.18653/v1/W17-0608</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>reinotrosterud-EtAl:2017:IWCLUL</bibkey>
  </paper>
  <paper id='0700'>
    <title>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </title>
    <editor>Ted Gibson</editor>
    <editor>Tal Linzen</editor>
    <editor>Asad Sayeed</editor>
    <editor>Martin van Schijndel</editor>
    <editor>William Schuler</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-07</url>
    <doi>10.18653/v1/W17-07</doi>
    <bibtype>book</bibtype>
    <bibkey>CMCL:2017</bibkey>
  </paper>
  <paper id='0701'>
    <title>Entropy Reduction correlates with temporal lobe activity</title>
    <author>
      <first>Matthew</first>
      <last>Nelson</last>
    </author>
    <author>
      <first>Stanislas</first>
      <last>Dehaene</last>
    </author>
    <author>
      <first>Christophe</first>
      <last>Pallier</last>
    </author>
    <author>
      <first>John</first>
      <last>Hale</last>
    </author>
    <booktitle>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-0701</url>
    <doi>10.18653/v1/W17-0701</doi>
    <abstract>
      Using the Entropy Reduction incremental complexity metric, we relate high
      gamma power signals from the brains of epileptic patients to incremental
      stages of syntactic analysis in English and French. We find that signals
      recorded intracranially from the anterior Inferior Temporal Sulcus (aITS)
      and the posterior Inferior Temporal Gyrus (pITG) correlate with
      word-by-word Entropy Reduction values derived from phrase structure
      grammars for those languages. In the anterior region, this correlation
      persists even in combination with surprisal co-predictors from PCFG and
      ngram models. The result confirms the idea that the brain's temporal lobe
      houses a parsing function, one whose incremental processing difficulty
      profile reflects changes in grammatical uncertainty.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nelson-EtAl:2017:CMCL</bibkey>
  </paper>
  <paper id='0702'>
    <title>Learning an Input Filter for Argument Structure Acquisition</title>
    <author>
      <first>Laurel</first>
      <last>Perkins</last>
    </author>
    <author>
      <first>Naomi</first>
      <last>Feldman</last>
    </author>
    <author>
      <first>Jeffrey</first>
      <last>Lidz</last>
    </author>
    <booktitle>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–19</pages>
    <url>http://www.aclweb.org/anthology/W17-0702</url>
    <doi>10.18653/v1/W17-0702</doi>
    <abstract>
      How do children learn a verb’s argument structure when their input
      contains nonbasic clauses that obscure verb transitivity? Here we present
      a new model that infers verb transitivity by learning to filter out
      non-basic clauses that were likely parsed in error. In simulations with
      child-directed speech, we show that this model accurately categorizes the
      majority of 50 frequent transitive, intransitive and alternating verbs,
      and jointly learns appropriate parameters for filtering parsing errors.
      Our model is thus able to filter out problematic data for verb learning
      without knowing in advance which data need to be filtered.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>perkins-feldman-lidz:2017:CMCL</bibkey>
  </paper>
  <paper id='0703'>
    <title>Grounding sound change in ideal observer models of perception</title>
    <author>
      <first>Zachary</first>
      <last>Burchill</last>
    </author>
    <author>
      <first>T. Florian</first>
      <last>Jaeger</last>
    </author>
    <booktitle>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20–28</pages>
    <url>http://www.aclweb.org/anthology/W17-0703</url>
    <doi>10.18653/v1/W17-0703</doi>
    <abstract>
      An important predictor of historical sound change, functional load, fails
      to capture insights from speech perception. Building on ideal observer
      models of word recognition, we devise a new definition of functional load
      that incorporates both a priori predictability and perceptual information.
      We explore this new measure with a simple model and find that it
      outperforms traditional measures.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>burchill-jaeger:2017:CMCL</bibkey>
  </paper>
  <paper id='0704'>
    <title>
      “Oh, I've Heard That Before": Modelling Own-Dialect Bias After Perceptual
      Learning by Weighting Training Data
    </title>
    <author>
      <first>Rachael</first>
      <last>Tatman</last>
    </author>
    <booktitle>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–34</pages>
    <url>http://www.aclweb.org/anthology/W17-0704</url>
    <doi>10.18653/v1/W17-0704</doi>
    <abstract>
      Human listeners are able to quickly and robustly adapt to new accents and
      do so by using information about speaker's identities. This paper will
      present experimental evidence that, even considering information about
      speaker's identities, listeners retain a strong bias towards the acoustics
      of their own dialect after dialect learning. Participants' behaviour was
      accurately mimicked by a classifier which was trained on more cases from
      the base dialect and fewer from the target dialect. This suggests that
      imbalanced training data may result in automatic speech recognition errors
      consistent with those of speakers from populations over-represented in the
      training data.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tatman:2017:CMCL</bibkey>
  </paper>
  <paper id='0705'>
    <title>
      Inherent Biases of Recurrent Neural Networks for Phonological Assimilation
      and Dissimilation
    </title>
    <author>
      <first>Amanda</first>
      <last>Doucette</last>
    </author>
    <booktitle>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–40</pages>
    <url>http://www.aclweb.org/anthology/W17-0705</url>
    <doi>10.18653/v1/W17-0705</doi>
    <abstract>
      A recurrent neural network model of phonological pattern learning is
      proposed. The model is a relatively simple neural network with one
      recurrent layer, and displays biases in learning that mimic observed
      biases in human learning. Single-feature patterns are learned faster than
      two-feature patterns, and vowel or consonant-only patterns are learned
      faster than patterns involving vowels and consonants, mimicking the
      results of laboratory learning experiments. In non-recurrent models,
      capturing these biases requires the use of alpha features or some other
      representation of repeated features, but with a recurrent neural network,
      these elaborations are not necessary.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>doucette:2017:CMCL</bibkey>
  </paper>
  <paper id='0706'>
    <title>Predicting Japanese scrambling in the wild</title>
    <author>
      <first>Naho</first>
      <last>Orita</last>
    </author>
    <booktitle>
      Proceedings of the 7th Workshop on Cognitive Modeling and Computational
      Linguistics (CMCL 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–45</pages>
    <url>http://www.aclweb.org/anthology/W17-0706</url>
    <doi>10.18653/v1/W17-0706</doi>
    <abstract>
      Japanese speakers have a choice between canonical SOV and scrambled OSV
      word order to express the same meaning. Although previous experiments
      examine the influence of one or two factors for scrambling in a controlled
      setting, it is not yet known what kinds of multiple effects contribute to
      scrambling. This study uses naturally distributed data to test the
      multiple effects on scrambling simultaneously. A regression analysis
      replicates the NP length effect and suggests the influence of noun types,
      but it provides no evidence for syntactic priming, given-new ordering, and
      the animacy effect. These findings only show evidence for
      sentence-internal factors, but we find no evidence that discourse level
      factors play a role.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>orita:2017:CMCL</bibkey>
  </paper>
  <paper id='0800'>
    <title>Proceedings of the 11th Linguistic Annotation Workshop</title>
    <editor>Nathan Schneider</editor>
    <editor>Nianwen Xue</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-08</url>
    <doi>10.18653/v1/W17-08</doi>
    <bibtype>book</bibtype>
    <bibkey>LAW:2017</bibkey>
  </paper>
  <paper id='0801'>
    <title>
      Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text
      Understanding in Emotion Annotation
    </title>
    <author>
      <first>Sven</first>
      <last>Buechel</last>
    </author>
    <author>
      <first>Udo</first>
      <last>Hahn</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <url>http://www.aclweb.org/anthology/W17-0801</url>
    <doi>10.18653/v1/W17-0801</doi>
    <abstract>
      We here examine how different perspectives of understanding written
      discourse, like the reader's, the writer's or the text's point of view,
      affect the quality of emotion annotations. We conducted a series of
      annotation experiments on two corpora, a popular movie review corpus and a
      genre- and domain-balanced corpus of standard English. We found
      statistical evidence that the writer's perspective yields superior
      annotation quality overall. However, the quality one perspective yields
      compared to the other(s) seems to depend on the domain the utterance
      originates from. Our data further suggest that the popular movie review
      data set suffers from an atypical bimodal distribution which may decrease
      model performance when used as a training resource.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>buechel-hahn:2017:LAW</bibkey>
  </paper>
  <paper id='0802'>
    <title>Finding Good Conversations Online: The Yahoo News Annotated Comments Corpus</title>
    <author>
      <first>Courtney</first>
      <last>Napoles</last>
    </author>
    <author>
      <first>Joel</first>
      <last>Tetreault</last>
    </author>
    <author>
      <first>Aasish</first>
      <last>Pappu</last>
    </author>
    <author>
      <first>Enrica</first>
      <last>Rosato</last>
    </author>
    <author>
      <first>Brian</first>
      <last>Provenzale</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–23</pages>
    <url>http://www.aclweb.org/anthology/W17-0802</url>
    <doi>10.18653/v1/W17-0802</doi>
    <abstract>
      This work presents a dataset and annotation scheme for the new task of
      identifying "good" conversations that occur online, which we call ERICs:
      Engaging, Respectful, and/or Informative Conversations. We develop a
      taxonomy to reflect features of entire threads and individual comments
      which we believe contribute to identifying ERICs; code a novel dataset of
      Yahoo News comment threads (2.4k threads and 10k comments) and 1k threads
      from the Internet Argument Corpus; and analyze the features characteristic
      of ERICs. This is one of the largest annotated corpora of online human
      dialogues, with the most detailed set of annotations. It will be valuable
      for identifying ERICs and other aspects of argumentation, dialogue, and
      discourse.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>napoles-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0803'>
    <title>
      Crowdsourcing discourse interpretations: On the influence of context and
      the reliability of a connective insertion task
    </title>
    <author>
      <first>Merel</first>
      <last>Scholman</last>
    </author>
    <author>
      <first>Vera</first>
      <last>Demberg</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–33</pages>
    <url>http://www.aclweb.org/anthology/W17-0803</url>
    <doi>10.18653/v1/W17-0803</doi>
    <abstract>
      Traditional discourse annotation tasks are considered costly and
      time-consuming, and the reliability and validity of these tasks is in
      question. In this paper, we investigate whether crowdsourcing can be used
      to obtain reliable discourse relation annotations. We also examine the
      influence of context on the reliability of the data. The results of a
      crowdsourced connective insertion task showed that the method can be used
      to obtain reliable annotations: The majority of the inserted connectives
      converged with the original label. Further, the method is sensitive to the
      fact that multiple senses can often be inferred for a single relation.
      Regarding the presence of context, the results show no significant
      difference in distributions of insertions between conditions overall.
      However, a by-item comparison revealed several characteristics of segments
      that determine whether the presence of context makes a difference in
      annotations. The findings discussed in this paper can be taken as evidence
      that crowdsourcing can be used as a valuable method to obtain insights
      into the sense(s) of relations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>scholman-demberg:2017:LAW</bibkey>
  </paper>
  <paper id='0804'>
    <title>A Code-Switching Corpus of Turkish-German Conversations</title>
    <author>
      <first>Özlem</first>
      <last>Çetinoğlu</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34–40</pages>
    <url>http://www.aclweb.org/anthology/W17-0804</url>
    <doi>10.18653/v1/W17-0804</doi>
    <abstract>
      We present a code-switching corpus of Turkish-German that is collected by
      recording conversations of bilinguals. The recordings are then transcribed
      in two layers following speech and orthography conventions, and annotated
      with sentence boundaries and intersentential, intrasentential, and
      intra-word switch points. The total amount of data is 5 hours of speech
      which corresponds to 3614 sentences. The corpus aims at serving as a
      resource for speech or text analysis, as well as a collection for
      linguistic inquiries.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ccetinouglu:2017:LAW</bibkey>
  </paper>
  <paper id='0805'>
    <title>Annotating omission in statement pairs</title>
    <author>
      <first>Héctor</first>
      <last>Martínez Alonso</last>
    </author>
    <author>
      <first>Amaury</first>
      <last>Delamaire</last>
    </author>
    <author>
      <first>Benoît</first>
      <last>Sagot</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–45</pages>
    <url>http://www.aclweb.org/anthology/W17-0805</url>
    <doi>10.18653/v1/W17-0805</doi>
    <abstract>
      We focus on the identification of omission in statement pairs. We compare
      three annotation schemes, namely two different crowdsourcing schemes and
      manual expert annotation. We show that the simplest of the two
      crowdsourcing approaches yields a better annotation quality than the more
      complex one. We use a dedicated classifier to assess whether the
      annotators' behavior can be explained by straightforward linguistic
      features. The classifier benefits from a modeling that uses lexical
      information beyond length and overlap measures. However, for our task, we
      argue that expert and not crowdsourcing-based annotation is the best
      compromise between annotation cost and quality.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>martinezalonso-delamaire-sagot:2017:LAW</bibkey>
  </paper>
  <paper id='0806'>
    <title>Annotating Speech, Attitude and Perception Reports</title>
    <author>
      <first>Corien</first>
      <last>Bary</last>
    </author>
    <author>
      <first>Leopold</first>
      <last>Hess</last>
    </author>
    <author>
      <first>Kees</first>
      <last>Thijs</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Berck</last>
    </author>
    <author>
      <first>Iris</first>
      <last>Hendrickx</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–56</pages>
    <url>http://www.aclweb.org/anthology/W17-0806</url>
    <doi>10.18653/v1/W17-0806</doi>
    <abstract>
      We present REPORTS, an annotation scheme for the annotation of speech,
      attitude and perception reports. Such a scheme makes it possible to
      annotate the various text elements involved in such reports (e.g.
      embedding entity, complement, complement head) and their relations in a
      uniform way, which in turn facilitates the automatic extraction of
      information on, for example, complementation and vocabulary distribution.
      We also present the Ancient Greek corpus RAG (Thucydides’ History of the
      Peloponnesian War), to which we have applied this scheme using the
      annotation tool BRAT. We discuss some of the issues, both theoretical and
      practical, that we encountered, show how the corpus helps in answering
      specific questions, and conclude that REPORTS fitted in well with our
      needs.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bary-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0807'>
    <title>
      Consistent Classification of Translation Revisions: A Case Study of
      English-Japanese Student Translations
    </title>
    <author>
      <first>Atsushi</first>
      <last>Fujita</last>
    </author>
    <author>
      <first>Kikuko</first>
      <last>Tanabe</last>
    </author>
    <author>
      <first>Chiho</first>
      <last>Toyoshima</last>
    </author>
    <author>
      <first>Mayuka</first>
      <last>Yamamoto</last>
    </author>
    <author>
      <first>Kyo</first>
      <last>Kageura</last>
    </author>
    <author>
      <first>Anthony</first>
      <last>Hartley</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–66</pages>
    <url>http://www.aclweb.org/anthology/W17-0807</url>
    <doi>10.18653/v1/W17-0807</doi>
    <abstract>
      Consistency is a crucial requirement in text annotation. It is especially
      important in educational applications, as lack of consistency directly
      affects learners' motivation and learning performance. This paper presents
      a quality assessment scheme for English-to-Japanese translations produced
      by learner translators at university. We constructed a revision typology
      and a decision tree manually through an application of the OntoNotes
      method, i.e., an iteration of assessing learners' translations and
      hypothesizing the conditions for consistent decision making, as well as
      re-organizing the typology. Intrinsic evaluation of the created scheme
      confirmed its potential contribution to the consistent classification of
      identified erroneous text spans, achieving visibly higher Cohen's kappa
      values, up to 0.831, than previous work. This paper also describes an
      application of our scheme to an English-to-Japanese translation exercise
      course for undergraduate students at a university in Japan.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fujita-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0808'>
    <title>
      Representation and Interchange of Linguistic Annotation. An In-Depth,
      Side-by-Side Comparison of Three Designs
    </title>
    <author>
      <first>Richard</first>
      <last>Eckart de Castilho</last>
    </author>
    <author>
      <first>Nancy</first>
      <last>Ide</last>
    </author>
    <author>
      <first>Emanuele</first>
      <last>Lapponi</last>
    </author>
    <author>
      <first>Stephan</first>
      <last>Oepen</last>
    </author>
    <author>
      <first>Keith</first>
      <last>Suderman</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <author>
      <first>Marc</first>
      <last>Verhagen</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <url>http://www.aclweb.org/anthology/W17-0808</url>
    <doi>10.18653/v1/W17-0808</doi>
    <abstract>
      For decades, most self-respecting linguistic engineering initiatives have
      designed and implemented custom representations for various layers of, for
      example, morphological, syntactic, and semantic analysis. Despite
      occasional efforts at harmonization or even standardization, our field
      today is blessed with a multitude of ways of encoding and exchanging
      linguistic annotations of these types, both at the levels of ‘abstract
      syntax’, naming choices, and of course file formats. To a large degree, it
      is possible to work within and across design plurality by conversion, and
      often there may be good reasons for divergent design reflecting
      differences in use. However, it is likely that some abstract commonalities
      across choices of representation are obscured by more superficial
      differences, and conversely there is no obvious procedure to tease apart
      what actually constitute contentful vs. mere technical divergences. In
      this study, we seek to conceptually align three representations for common
      types of morpho-syntactic analysis, pinpoint what in our view constitute
      contentful differences, and reflect on the underlying principles and
      specific requirements that led to individual choices. We expect that a
      more in-depth understanding of these choices across designs may led to
      increased harmonization, or at least to more informed design of future
      representations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>eckartdecastilho-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0809'>
    <title>TDB 1.1: Extensions on Turkish Discourse Bank</title>
    <author>
      <first>Deniz</first>
      <last>Zeyrek</last>
    </author>
    <author>
      <first>Murathan</first>
      <last>Kurfalı</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–81</pages>
    <url>http://www.aclweb.org/anthology/W17-0809</url>
    <doi>10.18653/v1/W17-0809</doi>
    <abstract>
      This paper presents the recent developments on Turkish Discourse Bank
      (TDB). First, the resource is summarized and an evaluation is presented.
      Then, TDB 1.1, i.e. enrichments on 10% of the corpus are described
      (namely, senses for explicit discourse connectives, and new annotations
      for three discourse relation types - implicit relations, entity relations
      and alternative lexicalizations). The method of annotation is explained
      and the data are evaluated.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zeyrek-kurfal:2017:LAW</bibkey>
  </paper>
  <paper id='0810'>
    <title>Two Layers of Annotation for Representing Event Mentions in News Stories</title>
    <author>
      <first>Maria Pia</first>
      <last>di Buono</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Tutek</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <author>
      <first>Goran</first>
      <last>Glavaš</last>
    </author>
    <author>
      <first>Bojana</first>
      <last>Dalbelo Bašić</last>
    </author>
    <author>
      <first>Natasa</first>
      <last>Milic-Frayling</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>82–90</pages>
    <url>http://www.aclweb.org/anthology/W17-0810</url>
    <doi>10.18653/v1/W17-0810</doi>
    <abstract>
      In this paper, we describe our preliminary study on annotating event
      mention as a part of our research on high-precision news event extraction
      models. To this end, we propose a two-layer annotation scheme, designed to
      separately capture the functional and conceptual aspects of event
      mentions. We hypothesize that the precision of models can be improved by
      modeling and extracting separately the different aspects of news events,
      and then combining the extracted information by leveraging the
      complementarities of the models. In addition, we carry out a preliminary
      annotation using the proposed scheme and analyze the annotation quality in
      terms of inter-annotator agreement.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dibuono-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0811'>
    <title>Word Similarity Datasets for Indian Languages: Annotation and Baseline Systems</title>
    <author>
      <first>Syed Sarfaraz</first>
      <last>Akhtar</last>
    </author>
    <author>
      <first>Arihant</first>
      <last>Gupta</last>
    </author>
    <author>
      <first>Avijit</first>
      <last>Vajpayee</last>
    </author>
    <author>
      <first>Arjit</first>
      <last>Srivastava</last>
    </author>
    <author>
      <first>Manish</first>
      <last>Shrivastava</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–94</pages>
    <url>http://www.aclweb.org/anthology/W17-0811</url>
    <doi>10.18653/v1/W17-0811</doi>
    <abstract>
      With the advent of word representations, word similarity tasks are
      becoming increasing popular as an evaluation metric for the quality of the
      representations. In this paper, we present manually annotated monolingual
      word similarity datasets of six Indian languages - Urdu, Telugu, Marathi,
      Punjabi, Tamil and Gujarati. These languages are most spoken Indian
      languages worldwide after Hindi and Bengali. For the construction of these
      datasets, our approach relies on translation and re-annotation of word
      similarity datasets of English. We also present baseline scores for word
      representation models using state-of-the-art techniques for Urdu, Telugu
      and Marathi by evaluating them on newly created word similarity datasets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>akhtar-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0812'>
    <title>The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations</title>
    <author>
      <first>Jesse</first>
      <last>Dunietz</last>
    </author>
    <author>
      <first>Lori</first>
      <last>Levin</last>
    </author>
    <author>
      <first>Jaime</first>
      <last>Carbonell</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>95–104</pages>
    <url>http://www.aclweb.org/anthology/W17-0812</url>
    <doi>10.18653/v1/W17-0812</doi>
    <abstract>
      Language of cause and effect captures an essential component of the
      semantics of a text. However, causal language is also intertwined with
      other semantic relations, such as temporal precedence and correlation.
      This makes it difficult to determine when causation is the primary
      intended meaning. This paper presents BECauSE 2.0, a new version of the
      BECauSE corpus with exhaustively annotated expressions of causal language,
      but also seven semantic relations that are frequently co-present with
      causation. The new corpus shows high inter-annotator agreement, and yields
      insights both about the linguistic expressions of causation and about the
      process of annotating co-present semantic relations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dunietz-levin-carbonell:2017:LAW</bibkey>
  </paper>
  <paper id='0813'>
    <title>
      Catching the Common Cause: Extraction and Annotation of Causal Relations
      and their Participants
    </title>
    <author>
      <first>Ines</first>
      <last>Rehbein</last>
    </author>
    <author>
      <first>Josef</first>
      <last>Ruppenhofer</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>105–114</pages>
    <url>http://www.aclweb.org/anthology/W17-0813</url>
    <doi>10.18653/v1/W17-0813</doi>
    <abstract>
      In this paper, we present a simple, yet effective method for the automatic
      identification and extraction of causal relations from text, based on a
      large English-German parallel corpus. The goal of this effort is to create
      a lexical resource for German causal relations. The resource will consist
      of a lexicon that describes constructions that trigger causality as well
      as the participants of the causal event, and will be augmented by a corpus
      with annotated instances for each entry, that can be used as training data
      to develop a system for automatic classification of causal relations.
      Focusing on verbs, our method harvested a set of 100 different lexical
      triggers of causality, including support verb constructions. At the
      moment, our corpus includes over 1,000 annotated instances. The lexicon
      and the annotated data will be made available to the research community.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rehbein-ruppenhofer:2017:LAW</bibkey>
  </paper>
  <paper id='0814'>
    <title>Assessing SRL Frameworks with Automatic Training Data Expansion</title>
    <author>
      <first>Silvana</first>
      <last>Hartmann</last>
    </author>
    <author>
      <first>Éva</first>
      <last>Mújdricza-Maydt</last>
    </author>
    <author>
      <first>Ilia</first>
      <last>Kuznetsov</last>
    </author>
    <author>
      <first>Iryna</first>
      <last>Gurevych</last>
    </author>
    <author>
      <first>Anette</first>
      <last>Frank</last>
    </author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115–121</pages>
    <url>http://www.aclweb.org/anthology/W17-0814</url>
    <doi>10.18653/v1/W17-0814</doi>
    <abstract>
      We present the first experiment-based study that explicitly contrasts the
      three major semantic role labeling frameworks. As a prerequisite, we
      create a dataset labeled with parallel FrameNet-, PropBank-, and
      VerbNet-style labels for German. We train a state-of-the-art SRL tool for
      German for the different annotation styles and provide a comparative
      analysis across frameworks. We further explore the behavior of the
      frameworks with automatic training data generation. VerbNet provides
      larger semantic expressivity than PropBank, and we find that its
      generalization capacity approaches PropBank in SRL training, but it
      benefits less from training data expansion than the sparse-data affected
      FrameNet.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hartmann-EtAl:2017:LAW</bibkey>
  </paper>
  <paper id='0900'>
    <title>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </title>
    <editor>Michael Roth</editor>
    <editor>Nasrin Mostafazadeh</editor>
    <editor>Nathanael Chambers</editor>
    <editor>Annie Louis</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://aclweb.org/anthology/W/W17/W17-09</url>
    <doi>10.18653/v1/W17-09</doi>
    <bibtype>book</bibtype>
    <bibkey>LSDSem:2017</bibkey>
  </paper>
  <paper id='0901'>
    <title>
      Inducing Script Structure from Crowdsourced Event Descriptions via
      Semi-Supervised Clustering
    </title>
    <author>
      <first>Lilian</first>
      <last>Wanzare</last>
    </author>
    <author>
      <first>Alessandra</first>
      <last>Zarcone</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Thater</last>
    </author>
    <author>
      <first>Manfred</first>
      <last>Pinkal</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–11</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0901</url>
    <doi>10.18653/v1/W17-0901</doi>
    <abstract>
      We present a semi-supervised clustering approach to induce script
      structure from crowdsourced descriptions of event sequences by grouping
      event descriptions into paraphrase sets (representing event types) and
      inducing their temporal order. Our approach exploits semantic and
      positional similarity and allows for flexible event order, thus overcoming
      the rigidity of previous approaches. We incorporate crowdsourced
      alignments as prior knowledge and show that exploiting a small number of
      alignments results in a substantial improvement in cluster quality over
      state-of-the-art models and provides an appropriate basis for the
      induction of temporal order. We also show a coverage study to demonstrate
      the scalability of our approach.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wanzare-EtAl:2017:LSDSem</bibkey>
  </paper>
  <paper id='0902'>
    <title>A Consolidated Open Knowledge Representation for Multiple Texts</title>
    <author>
      <first>Rachel</first>
      <last>Wities</last>
    </author>
    <author>
      <first>Vered</first>
      <last>Shwartz</last>
    </author>
    <author>
      <first>Gabriel</first>
      <last>Stanovsky</last>
    </author>
    <author>
      <first>Meni</first>
      <last>Adler</last>
    </author>
    <author>
      <first>Ori</first>
      <last>Shapira</last>
    </author>
    <author>
      <first>Shyam</first>
      <last>Upadhyay</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Roth</last>
    </author>
    <author>
      <first>Eugenio</first>
      <last>Martínez-Cámara</last>
    </author>
    <author>
      <first>Iryna</first>
      <last>Gurevych</last>
    </author>
    <author>
      <first>Ido</first>
      <last>Dagan</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–24</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0902</url>
    <doi>10.18653/v1/W17-0902</doi>
    <abstract>
      We propose to move from Open Information Extraction (OIE) ahead to Open
      Knowledge Representation (OKR), aiming to represent information conveyed
      jointly in a set of texts in an open text- based manner. We do so by
      consolidating OIE extractions using entity and predicate coreference,
      while modeling information containment between coreferring elements via
      lexical entailment. We suggest that generating OKR structures can be a
      useful step in the NLP pipeline, to give semantic applications an easy
      handle on consolidated information across multiple texts.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wities-EtAl:2017:LSDSem</bibkey>
  </paper>
  <paper id='0903'>
    <title>
      Event-Related Features in Feedforward Neural Networks Contribute to
      Identifying Causal Relations in Discourse
    </title>
    <author>
      <first>Edoardo Maria</first>
      <last>Ponti</last>
    </author>
    <author>
      <first>Anna</first>
      <last>Korhonen</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–30</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0903</url>
    <doi>10.18653/v1/W17-0903</doi>
    <abstract>
      Causal relations play a key role in information extraction and reasoning.
      Most of the times, their expression is ambiguous or implicit, i.e. without
      signals in the text. This makes their identification challenging. We aim
      to improve their identification by implementing a Feedforward Neural
      Network with a novel set of features for this task. In particular, these
      are based on the position of event mentions and the semantics of events
      and participants. The resulting classifier outperforms strong baselines on
      two datasets (the Penn Discourse Treebank and the CSTNews corpus)
      annotated with different schemes and containing examples in two languages,
      English and Portuguese. This result demonstrates the importance of events
      for identifying discourse relations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ponti-korhonen:2017:LSDSem</bibkey>
  </paper>
  <paper id='0904'>
    <title>Stance Detection in Facebook Posts of a German Right-wing Party</title>
    <author>
      <first>Manfred</first>
      <last>Klenner</last>
    </author>
    <author>
      <first>Don</first>
      <last>Tuggener</last>
    </author>
    <author>
      <first>Simon</first>
      <last>Clematide</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–40</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0904</url>
    <doi>10.18653/v1/W17-0904</doi>
    <abstract>
      We argue that in order to detect stance, not only the explicit attitudes
      of the stance holder towards the targets are crucial. It is the whole
      narrative the writer drafts that counts, including the way he hypostasizes
      the discourse referents: as benefactors or villains, as victims or
      beneficiaries. We exemplify the ability of our system to identify targets
      and detect the writer's stance towards them on the basis of about 100 000
      Facebook posts of a German right-wing party. A reader and writer model on
      top of our verb-based attitude extraction directly reveal stance
      conflicts.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>klenner-tuggener-clematide:2017:LSDSem</bibkey>
  </paper>
  <paper id='0905'>
    <title>Behind the Scenes of an Evolving Event Cloze Test</title>
    <author>
      <first>Nathanael</first>
      <last>Chambers</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–45</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0905</url>
    <doi>10.18653/v1/W17-0905</doi>
    <abstract>
      This paper analyzes the narrative event cloze test and its recent
      evolution. The test removes one event from a document's chain of events,
      and systems predict the missing event. Originally proposed to evaluate
      learned knowledge of event scenarios (e.g., scripts and frames), most
      recent work now builds ngram-like language models (LM) to beat the test.
      This paper argues that the test has slowly/unknowingly been altered to
      accommodate LMs.5 Most notably, tests are auto-generated rather than by
      hand, and no effort is taken to include core script events. Recent work is
      not clear on evaluation goals and contains contradictory results. We
      implement several models, and show that the test's bias to high-frequency
      events explains the inconsistencies. We conclude with recommendations on
      how to return to the test's original intent, and offer brief suggestions
      on a path forward.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chambers:2017:LSDSem</bibkey>
  </paper>
  <paper id='0906'>
    <title>LSDSem 2017 Shared Task: The Story Cloze Test</title>
    <author>
      <first>Nasrin</first>
      <last>Mostafazadeh</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Roth</last>
    </author>
    <author>
      <first>Annie</first>
      <last>Louis</last>
    </author>
    <author>
      <first>Nathanael</first>
      <last>Chambers</last>
    </author>
    <author>
      <first>James</first>
      <last>Allen</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–51</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0906</url>
    <doi>10.18653/v1/W17-0906</doi>
    <abstract>
      The LSDSem'17 shared task is the Story Cloze Test, a new evaluation for
      story understanding and script learning. This test provides a system with
      a four-sentence story and two possible endings, and the system must choose
      the correct ending to the story. Successful narrative understanding
      (getting closer to human performance of 100%) requires systems to link
      various levels of semantics to commonsense knowledge. A total of eight
      systems participated in the shared task, with a variety of approaches
      including.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mostafazadeh-EtAl:2017:LSDSem</bibkey>
  </paper>
  <paper id='0907'>
    <title>Story Cloze Task: UW NLP System</title>
    <author>
      <first>Roy</first>
      <last>Schwartz</last>
    </author>
    <author>
      <first>Maarten</first>
      <last>Sap</last>
    </author>
    <author>
      <first>Ioannis</first>
      <last>Konstas</last>
    </author>
    <author>
      <first>Leila</first>
      <last>Zilles</last>
    </author>
    <author>
      <first>Yejin</first>
      <last>Choi</last>
    </author>
    <author>
      <first>Noah A.</first>
      <last>Smith</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–55</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0907</url>
    <doi>10.18653/v1/W17-0907</doi>
    <abstract>
      This paper describes University of Washington NLP’s submission for the
      Linking Models of Lexical, Sentential and Discourse-level Semantics
      (LSDSem 2017) shared task—the Story Cloze Task. Our system is a linear
      classifier with a variety of features, including both the scores of a
      neural language model and style features. We report 75.2% accuracy on the
      task. A further discussion of our results can be found in Schwartz et al.
      (2017).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schwartz-EtAl:2017:LSDSem</bibkey>
  </paper>
  <paper id='0908'>
    <title>LSDSem 2017: Exploring Data Generation Methods for the Story Cloze Test</title>
    <author>
      <first>Michael</first>
      <last>Bugert</last>
    </author>
    <author>
      <first>Yevgeniy</first>
      <last>Puzikov</last>
    </author>
    <author>
      <first>Andreas</first>
      <last>Rücklé</last>
    </author>
    <author>
      <first>Judith</first>
      <last>Eckle-Kohler</last>
    </author>
    <author>
      <first>Teresa</first>
      <last>Martin</last>
    </author>
    <author>
      <first>Eugenio</first>
      <last>Martínez-Cámara</last>
    </author>
    <author>
      <first>Daniil</first>
      <last>Sorokin</last>
    </author>
    <author>
      <first>Maxime</first>
      <last>Peyrard</last>
    </author>
    <author>
      <first>Iryna</first>
      <last>Gurevych</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–61</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0908</url>
    <doi>10.18653/v1/W17-0908</doi>
    <abstract>
      The Story Cloze test is a recent effort in providing a common test
      scenario for text understanding systems. As part of the LSDSem 2017 shared
      task, we present a system based on a deep learning architecture combined
      with a rich set of manually-crafted linguistic features. The system
      outperforms all known baselines for the task, suggesting that the chosen
      approach is promising. We additionally present two methods for generating
      further training data based on stories from the ROCStories corpus.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bugert-EtAl:2017:LSDSem</bibkey>
  </paper>
  <paper id='0909'>
    <title>Sentiment Analysis and Lexical Cohesion for the Story Cloze Task</title>
    <author>
      <first>Michael</first>
      <last>Flor</last>
    </author>
    <author>
      <first>Swapna</first>
      <last>Somasundaran</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>62–67</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0909</url>
    <doi>10.18653/v1/W17-0909</doi>
    <abstract>
      We present two NLP components for the Story Cloze Task – dictionary-based
      sentiment analysis and lexical cohesion. While previous research found no
      contribution from sentiment analysis to the accuracy on this task, we
      demonstrate that sentiment is an important aspect. We describe a new
      approach, using a rule that estimates sentiment congruence in a story. Our
      sentiment-based system achieves strong results on this task. Our lexical
      cohesion system achieves accuracy comparable to previously published
      baseline results. A combination of the two systems achieves better
      accuracy than published baselines. We argue that sentiment analysis should
      be considered an integral part of narrative comprehension.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>flor-somasundaran:2017:LSDSem</bibkey>
  </paper>
  <paper id='0910'>
    <title>Resource-Lean Modeling of Coherence in Commonsense Stories</title>
    <author>
      <first>Niko</first>
      <last>Schenk</last>
    </author>
    <author>
      <first>Christian</first>
      <last>Chiarcos</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>68–73</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0910</url>
    <doi>10.18653/v1/W17-0910</doi>
    <abstract>
      We present a resource-lean neural recognizer for modeling coherence in
      commonsense stories. Our lightweight system is inspired by successful
      attempts to modeling discourse relations and stands out due to its
      simplicity and easy optimization compared to prior approaches to narrative
      script learning. We evaluate our approach in the Story Cloze Test
      demonstrating an absolute improvement in accuracy of 4.7% over
      state-of-the-art implementations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schenk-chiarcos:2017:LSDSem</bibkey>
  </paper>
  <paper id='0911'>
    <title>An RNN-based Binary Classifier for the Story Cloze Test</title>
    <author>
      <first>Melissa</first>
      <last>Roemmele</last>
    </author>
    <author>
      <first>Sosuke</first>
      <last>Kobayashi</last>
    </author>
    <author>
      <first>Naoya</first>
      <last>Inoue</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Gordon</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74–80</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0911</url>
    <doi>10.18653/v1/W17-0911</doi>
    <abstract>
      The Story Cloze Test consists of choosing a sentence that best completes a
      story given two choices. In this paper we present a system that performs
      this task using a supervised binary classifier on top of a recurrent
      neural network to predict the probability that a given story ending is
      correct. The classifier is trained to distinguish correct story endings
      given in the training data from incorrect ones that we artificially
      generate. Our experiments evaluate different methods for generating these
      negative examples, as well as different embedding-based representations of
      the stories. Our best result obtains 67.2% accuracy on the test set,
      outperforming the existing top baseline of 58.5%.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>roemmele-EtAl:2017:LSDSem</bibkey>
  </paper>
  <paper id='0912'>
    <title>IIT (BHU): System Description for LSDSem'17 Shared Task</title>
    <author>
      <first>Pranav</first>
      <last>Goel</last>
    </author>
    <author>
      <first>Anil Kumar</first>
      <last>Singh</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>81–86</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0912</url>
    <doi>10.18653/v1/W17-0912</doi>
    <abstract>
      This paper describes an ensemble system submitted as part of the LSDSem
      Shared Task 2017 - the Story Cloze Test. The main conclusion from our
      results is that an approach based on semantic similarity alone may not be
      enough for this task. We test various approaches and compare them with two
      ensemble systems. One is based on voting and the other on logistic
      regression based classifier. Our final system is able to outperform the
      previous state of the art for the Story Cloze test. Another very
      interesting observation is the performance of sentiment based approach
      which works almost as well on its own as our final ensemble system.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>goel-singh:2017:LSDSem</bibkey>
  </paper>
  <paper id='0913'>
    <title>Story Cloze Ending Selection Baselines and Data Examination</title>
    <author>
      <first>Todor</first>
      <last>Mihaylov</last>
    </author>
    <author>
      <first>Anette</first>
      <last>Frank</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential
      and Discourse-level Semantics
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–92</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0913</url>
    <doi>10.18653/v1/W17-0913</doi>
    <abstract>
      This paper describes two supervised baseline systems for the Story Cloze
      Test Shared Task (Mostafazadeh et al., 2016a). We first build a classifier
      using features based on word embeddings and semantic similarity
      computation. We further implement a neural LSTM system with different
      encoding strategies that try to model the relation between the story and
      the provided endings. Our experiments show that a model using
      representation features based on average word embedding vectors over the
      given story words and the candidate ending sentences words, joint with
      similarity features between the story and candidate ending representations
      performed better than the neural models. Our best model based on achieves
      an accuracy of 72.42, ranking 3rd in the official evaluation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mihaylov-frank:2017:LSDSem</bibkey>
  </paper>
  <paper id='1000'>
    <title>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </title>
    <editor>George Giannakopoulos</editor>
    <editor>Elena Lloret</editor>
    <editor>John M. Conroy</editor>
    <editor>Josef Steinberger</editor>
    <editor>Marina Litvak</editor>
    <editor>Peter Rankel</editor>
    <editor>Benoit Favre</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-10</url>
    <doi>10.18653/v1/W17-10</doi>
    <bibtype>book</bibtype>
    <bibkey>MultiLing2017:2017</bibkey>
  </paper>
  <paper id='1001'>
    <title>MultiLing 2017 Overview</title>
    <author>
      <first>George</first>
      <last>Giannakopoulos</last>
    </author>
    <author>
      <first>John</first>
      <last>Conroy</last>
    </author>
    <author>
      <first>Jeff</first>
      <last>Kubina</last>
    </author>
    <author>
      <first>Peter A.</first>
      <last>Rankel</last>
    </author>
    <author>
      <first>Elena</first>
      <last>Lloret</last>
    </author>
    <author>
      <first>Josef</first>
      <last>Steinberger</last>
    </author>
    <author>
      <first>Marina</first>
      <last>Litvak</last>
    </author>
    <author>
      <first>Benoit</first>
      <last>Favre</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–6</pages>
    <url>http://www.aclweb.org/anthology/W17-1001</url>
    <doi>10.18653/v1/W17-1001</doi>
    <abstract>
      In this brief report we present an overview of the MultiLing 2017 effort
      and workshop, as implemented within EACL 2017. MultiLing is a
      community-driven initiative that pushes the state-of-the-art in Automatic
      Summarization by providing data sets and fostering further research and
      development of summarization systems. This year the scope of the workshop
      was widened, bringing together researchers that work on summarization
      across sources, languages and genres. We summarize the main tasks planned
      and implemented this year, the contributions received, and we also provide
      insights on next steps.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>giannakopoulos-EtAl:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1002'>
    <title>Decoupling Encoder and Decoder Networks for Abstractive Document Summarization</title>
    <author>
      <first>Ying</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Jey Han</first>
      <last>Lau</last>
    </author>
    <author>
      <first>Timothy</first>
      <last>Baldwin</last>
    </author>
    <author>
      <first>Trevor</first>
      <last>Cohn</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7–11</pages>
    <url>http://www.aclweb.org/anthology/W17-1002</url>
    <doi>10.18653/v1/W17-1002</doi>
    <abstract>
      Abstractive document summarization seeks to automatically generate a
      summary for a document, based on some abstract ”understanding” of the
      original document. State-of-the-art techniques traditionally use attentive
      encoder–decoder architectures. However, due to the large number of
      parameters in these models, they require large training datasets and long
      training times. In this paper, we propose decoupling the encoder and
      decoder networks, and training them separately. We encode documents using
      an unsupervised document encoder, and then feed the document vector to a
      recurrent neural network decoder. With this decoupled architecture, we
      decrease the number of parameters in the decoder substantially, and
      shorten its training time. Experiments show that the decoupled model
      achieves comparable performance with state-of-the-art models for in-domain
      documents, but less well for out-of-domain documents.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-EtAl:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1003'>
    <title>Centroid-based Text Summarization through Compositionality of Word Embeddings</title>
    <author>
      <first>Gaetano</first>
      <last>Rossiello</last>
    </author>
    <author>
      <first>Pierpaolo</first>
      <last>Basile</last>
    </author>
    <author>
      <first>Giovanni</first>
      <last>Semeraro</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–21</pages>
    <url>http://www.aclweb.org/anthology/W17-1003</url>
    <doi>10.18653/v1/W17-1003</doi>
    <abstract>
      The textual similarity is a crucial aspect for many extractive text
      summarization methods. A bag-of-words representation does not allow to
      grasp the semantic relationships between concepts when comparing strongly
      related sentences with no words in common. To overcome this issue, in this
      paper we propose a centroid-based method for text summarization that
      exploits the compositional capabilities of word embeddings. The
      evaluations on multi-document and multilingual datasets prove the
      effectiveness of the continuous vector representation of words compared to
      the bag-of-words model. Despite its simplicity, our method achieves good
      performance even in comparison to more complex deep learning models. Our
      method is unsupervised and it can be adopted in other summarization tasks.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rossiello-basile-semeraro:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1004'>
    <title>Query-based summarization using MDL principle</title>
    <author>
      <first>Marina</first>
      <last>Litvak</last>
    </author>
    <author>
      <first>Natalia</first>
      <last>Vanetik</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–31</pages>
    <url>http://www.aclweb.org/anthology/W17-1004</url>
    <doi>10.18653/v1/W17-1004</doi>
    <abstract>
      Query-based text summarization is aimed at extracting essential
      information that answers the query from original text. The answer is
      presented in a minimal, often predefined, number of words. In this paper
      we introduce a new unsupervised approach for query-based extractive
      summarization, based on the minimum description length (MDL) principle
      that employs Krimp compression algorithm (Vreeken et al., 2011). The key
      idea of our approach is to select frequent word sets related to a given
      query that compress document sentences better and therefore describe the
      document better. A summary is extracted by selecting sentences that best
      cover query-related frequent word sets. The approach is evaluated based on
      the DUC 2005 and DUC 2006 datasets which are specifically designed for
      query-based summarization (DUC, 2005 2006). It competes with the best
      results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>litvak-vanetik:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1005'>
    <title>
      Word Embedding and Topic Modeling Enhanced Multiple Features for Content
      Linking and Argument / Sentiment Labeling in Online Forums
    </title>
    <author>
      <first>Lei</first>
      <last>Li</last>
    </author>
    <author>
      <first>Liyuan</first>
      <last>Mao</last>
    </author>
    <author>
      <first>Moye</first>
      <last>Chen</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–36</pages>
    <url>http://www.aclweb.org/anthology/W17-1005</url>
    <doi>10.18653/v1/W17-1005</doi>
    <abstract>
      Multiple grammatical and semantic features are adopted in content linking
      and argument/sentiment labeling for online forums in this paper. There are
      mainly two different methods for content linking. First, we utilize the
      deep feature obtained from Word Embedding Model in deep learning and
      compute sentence similarity. Second, we use multiple traditional features
      to locate candidate linking sentences, and then adopt a voting method to
      obtain the final result. LDA topic modeling is used to mine latent
      semantic feature and K-means clustering is implemented for argument
      labeling, while features from sentiment dictionaries and rule-based
      sentiment analysis are integrated for sentiment labeling. Experimental
      results have shown that our methods are valid.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-mao-chen:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1006'>
    <title>
      Ultra-Concise Multi-genre Summarisation of Web2.0: towards Intelligent
      Content Generation
    </title>
    <author>
      <first>Elena</first>
      <last>Lloret</last>
    </author>
    <author>
      <first>Ester</first>
      <last>Boldrini</last>
    </author>
    <author>
      <first>Patricio</first>
      <last>Martinez-Barco</last>
    </author>
    <author>
      <first>Manuel</first>
      <last>Palomar</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–46</pages>
    <url>http://www.aclweb.org/anthology/W17-1006</url>
    <doi>10.18653/v1/W17-1006</doi>
    <abstract>
      The electronic Word of Mouth has become the most powerful communication
      channel thanks to the wide usage of the Social Media. Our research
      proposes an approach towards the production of automatic ultra-concise
      summaries from multiple Web 2.0 sources. We exploit user-generated content
      from reviews and microblogs in dif- ferent domains, and compile and
      analyse four types of ultra-concise summaries: a)positive information, b)
      negative information; c) both or d) objective information. The
      appropriateness and usefulness of our model is demonstrated by its
      successful results and great potential in real-life applications, thus
      meaning a relevant advancement of the state-of-the-art approaches.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lloret-EtAl:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1007'>
    <title>Machine Learning Approach to Evaluate MultiLingual Summaries</title>
    <author>
      <first>Samira</first>
      <last>Ellouze</last>
    </author>
    <author>
      <first>Maher</first>
      <last>Jaoua</last>
    </author>
    <author>
      <first>Lamia</first>
      <last>Hadrich Belguith</last>
    </author>
    <booktitle>
      Proceedings of the MultiLing 2017 Workshop on Summarization and Summary
      Evaluation Across Source Types and Genres
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–54</pages>
    <url>http://www.aclweb.org/anthology/W17-1007</url>
    <doi>10.18653/v1/W17-1007</doi>
    <abstract>
      The present paper introduces a new MultiLing text summary evaluation
      method. This method relies on machine learning approach which operates by
      combining multiple features to build models that predict the human score
      (overall responsiveness) of a new summary. We have tried several single
      and “ensemble learning” classifiers to build the best model. We have
      experimented our method in summary level evaluation where we evaluate each
      text summary separately. The correlation between built models and human
      score is better than the correlation between baselines and manual score.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ellouze-jaoua-hadrichbelguith:2017:MultiLing2017</bibkey>
  </paper>
  <paper id='1100'>
    <title>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </title>
    <editor>Lun-Wei Ku</editor>
    <editor>Cheng-Te Li</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-11</url>
    <doi>10.18653/v1/W17-11</doi>
    <bibtype>book</bibtype>
    <bibkey>SocialNLP2017:2017</bibkey>
  </paper>
  <paper id='1101'>
    <title>A Survey on Hate Speech Detection using Natural Language Processing</title>
    <author>
      <first>Anna</first>
      <last>Schmidt</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Wiegand</last>
    </author>
    <booktitle>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-1101</url>
    <doi>10.18653/v1/W17-1101</doi>
    <abstract>
      This paper presents a survey on hate speech detection. Given the steadily
      growing body of social media content, the amount of online hate speech is
      also increasing. Due to the massive scale of the web, methods that
      automatically detect hate speech are required. Our survey describes key
      areas that have been explored to automatically recognize these types of
      utterances using natural language processing. We also discuss limits of
      those approaches.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schmidt-wiegand:2017:SocialNLP2017</bibkey>
  </paper>
  <paper id='1102'>
    <title>Facebook sentiment: Reactions and Emojis</title>
    <author>
      <first>Ye</first>
      <last>Tian</last>
    </author>
    <author>
      <first>Thiago</first>
      <last>Galery</last>
    </author>
    <author>
      <first>Giulio</first>
      <last>Dulcinati</last>
    </author>
    <author>
      <first>Emilia</first>
      <last>Molimpakis</last>
    </author>
    <author>
      <first>Chao</first>
      <last>Sun</last>
    </author>
    <booktitle>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–16</pages>
    <url>http://www.aclweb.org/anthology/W17-1102</url>
    <doi>10.18653/v1/W17-1102</doi>
    <abstract>
      Emojis are used frequently in social media. A widely assumed view is that
      emojis express the emotional state of the user, which has led to research
      focusing on the expressiveness of emojis independent from the linguistic
      context. We argue that emojis and the linguistic texts can modify the
      meaning of each other. The overall communicated meaning is not a simple
      sum of the two channels. In order to study the meaning interplay, we need
      data indicating the overall sentiment of the entire message as well as the
      sentiment of the emojis stand-alone. We propose that Facebook Reactions
      are a good data source for such a purpose. FB reactions (e.g. “Love” and
      “Angry”) indicate the readers' overall sentiment, against which we can
      investigate the types of emojis used the comments under different reaction
      profiles. We present a data set of 21,000 FB posts (57 million reactions
      and 8 million comments) from public media pages across four countries.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tian-EtAl:2017:SocialNLP2017</bibkey>
  </paper>
  <paper id='1103'>
    <title>Potential and Limitations of Cross-Domain Sentiment Classification</title>
    <author>
      <first>Jan Milan</first>
      <last>Deriu</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Weilenmann</last>
    </author>
    <author>
      <first>Dirk</first>
      <last>Von Gruenigen</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Cieliebak</last>
    </author>
    <booktitle>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–24</pages>
    <url>http://www.aclweb.org/anthology/W17-1103</url>
    <doi>10.18653/v1/W17-1103</doi>
    <abstract>
      In this paper we investigate the cross-domain performance of a current
      state-of-the-art sentiment analysis systems. For this purpose we train a
      convolutional neural network (CNN) on data from different domains and
      evaluate its performance on other domains. Furthermore, we evaluate the
      usefulness of combining a large amount of different smaller annotated
      corpora to a large corpus. Our results show that more sophisticated
      approaches are required to train a system that works equally well on
      various domains.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>deriu-EtAl:2017:SocialNLP2017</bibkey>
  </paper>
  <paper id='1104'>
    <title>Aligning Entity Names with Online Aliases on Twitter</title>
    <author>
      <first>Kevin</first>
      <last>McKelvey</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Goutzounis</last>
    </author>
    <author>
      <first>Stephen</first>
      <last>da Cruz</last>
    </author>
    <author>
      <first>Nathanael</first>
      <last>Chambers</last>
    </author>
    <booktitle>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–35</pages>
    <url>http://www.aclweb.org/anthology/W17-1104</url>
    <doi>10.18653/v1/W17-1104</doi>
    <abstract>
      This paper presents new models that automatically align online aliases
      with their real entity names. Many research applications rely on
      identifying entity names in text, but people often refer to entities with
      unexpected nicknames and aliases. For example, The King and King James are
      aliases for Lebron James, a professional basketball player. Recent work on
      entity linking attempts to resolve mentions to knowledge base entries,
      like a wikipedia page, but linking is unfortunately limited to well-known
      entities with pre-built pages. This paper asks a more basic question: can
      aliases be aligned without background knowledge of the entity? Further,
      can the semantics surrounding alias mentions be used to inform alignments?
      We describe statistical models that make decisions based on the
      lexicographic properties of the aliases with their semantic context in a
      large corpus of tweets. We experiment on a database of Twitter users and
      their usernames, and present the first human evaluation for this task.
      Alignment accuracy approaches human performance at 81%, and we show that
      while lexicographic features are most important, the semantic context of
      an alias further improves classification accuracy.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mckelvey-EtAl:2017:SocialNLP2017</bibkey>
  </paper>
  <paper id='1105'>
    <title>Character-based Neural Embeddings for Tweet Clustering</title>
    <author>
      <first>Svitlana</first>
      <last>Vakulenko</last>
    </author>
    <author>
      <first>Lyndon</first>
      <last>Nixon</last>
    </author>
    <author>
      <first>Mihai</first>
      <last>Lupu</last>
    </author>
    <booktitle>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–44</pages>
    <url>http://www.aclweb.org/anthology/W17-1105</url>
    <doi>10.18653/v1/W17-1105</doi>
    <abstract>
      In this paper we show how the performance of tweet clustering can be
      improved by leveraging character-based neural networks. The proposed
      approach overcomes the limitations related to the vocabulary explosion in
      the word-based models and allows for the seamless processing of the
      multilingual content. Our evaluation results and code are available
      on-line: https://github.com/vendi12/tweet2vec_clustering.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vakulenko-nixon-lupu:2017:SocialNLP2017</bibkey>
  </paper>
  <paper id='1106'>
    <title>A Twitter Corpus and Benchmark Resources for German Sentiment Analysis</title>
    <author>
      <first>Mark</first>
      <last>Cieliebak</last>
    </author>
    <author>
      <first>Jan Milan</first>
      <last>Deriu</last>
    </author>
    <author>
      <first>Dominic</first>
      <last>Egger</last>
    </author>
    <author>
      <first>Fatih</first>
      <last>Uzdilli</last>
    </author>
    <booktitle>
      Proceedings of the Fifth International Workshop on Natural Language
      Processing for Social Media
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45–51</pages>
    <url>http://www.aclweb.org/anthology/W17-1106</url>
    <doi>10.18653/v1/W17-1106</doi>
    <abstract>
      In this paper we present SB10k, a new corpus for sentiment analysis with
      approx. 10,000 German tweets. We use this new corpus and two existing
      corpora to provide state-of-the-art benchmarks for sentiment analysis in
      German: we implemented a CNN (based on the winning system of SemEval-2016)
      and a feature-based SVM and compare their performance on all three
      corpora. For the CNN, we also created German word embeddings trained on
      300M tweets. These word embeddings were then optimized for sentiment
      analysis using distant-supervised learning. The new corpus, the German
      word embeddings (plain and optimized), and source code to re-run the
      benchmarks are publicly available.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cieliebak-EtAl:2017:SocialNLP2017</bibkey>
  </paper>
  <paper id='1200'>
    <title>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </title>
    <editor>Preslav Nakov</editor>
    <editor>Marcos Zampieri</editor>
    <editor>Nikola Ljubešić</editor>
    <editor>Jörg Tiedemann</editor>
    <editor>Shevin Malmasi</editor>
    <editor>Ahmed Ali</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-12</url>
    <doi>10.18653/v1/W17-12</doi>
    <bibtype>book</bibtype>
    <bibkey>VarDial:2017</bibkey>
  </paper>
  <paper id='1201'>
    <title>Findings of the VarDial Evaluation Campaign 2017</title>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <author>
      <first>Nikola</first>
      <last>Ljubešić</last>
    </author>
    <author>
      <first>Preslav</first>
      <last>Nakov</last>
    </author>
    <author>
      <first>Ahmed</first>
      <last>Ali</last>
    </author>
    <author>
      <first>Jörg</first>
      <last>Tiedemann</last>
    </author>
    <author>
      <first>Yves</first>
      <last>Scherrer</last>
    </author>
    <author>
      <first>Noëmi</first>
      <last>Aepli</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–15</pages>
    <url>http://www.aclweb.org/anthology/W17-1201</url>
    <doi>10.18653/v1/W17-1201</doi>
    <abstract>
      We present the results of the VarDial Evaluation Campaign on Natural
      Language Processing (NLP) for Similar Languages, Varieties and Dialects,
      which we organized as part of the fourth edition of the VarDial workshop
      at EACL'2017. This year, we included four shared tasks: Discriminating
      between Similar Languages (DSL), Arabic Dialect Identification (ADI),
      German Dialect Identification (GDI), and Cross-lingual Dependency Parsing
      (CLP). A total of 19 teams submitted runs across the four tasks, and 15 of
      them wrote system description papers.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zampieri-EtAl:2017:VarDial</bibkey>
  </paper>
  <paper id='1202'>
    <title>Dialectometric analysis of language variation in Twitter</title>
    <author>
      <first>Gonzalo</first>
      <last>Donoso</last>
    </author>
    <author>
      <first>David</first>
      <last>Sanchez</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16–25</pages>
    <url>http://www.aclweb.org/anthology/W17-1202</url>
    <doi>10.18653/v1/W17-1202</doi>
    <abstract>
      In the last few years, microblogging platforms such as Twitter have given
      rise to a deluge of textual data that can be used for the analysis of
      informal communication between millions of individuals. In this work, we
      propose an information-theoretic approach to geographic language variation
      using a corpus based on Twitter. We test our models with tens of concepts
      and their associated keywords detected in Spanish tweets geolocated in
      Spain. We employ dialectometric measures (cosine similarity and
      Jensen-Shannon divergence) to quantify the linguistic distance on the
      lexical level between cells created in a uniform grid over the map. This
      can be done for a single concept or in the general case taking into
      account an average of the considered variants. The latter permits an
      analysis of the dialects that naturally emerge from the data.
      Interestingly, our results reveal the existence of two dialect
      macrovarieties. The first group includes a region-specific speech spoken
      in small towns and rural areas whereas the second cluster encompasses
      cities that tend to use a more uniform variety. Since the results obtained
      with the two different metrics qualitatively agree, our work suggests that
      social media corpora can be efficiently used for dialectometric analyses.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>donoso-sanchez:2017:VarDial</bibkey>
  </paper>
  <paper id='1203'>
    <title>Computational analysis of Gondi dialects</title>
    <author>
      <first>Taraka</first>
      <last>Rama</last>
    </author>
    <author>
      <first>Çağrı</first>
      <last>Çöltekin</last>
    </author>
    <author>
      <first>Pavel</first>
      <last>Sofroniev</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–35</pages>
    <url>http://www.aclweb.org/anthology/W17-1203</url>
    <doi>10.18653/v1/W17-1203</doi>
    <abstract>
      This paper presents a computational analysis of Gondi dialects spoken in
      central India. We present a digitized data set of the dialect area, and
      analyze the data using different techniques from dialectometry, deep
      learning, and computational biology. We show that the methods largely
      agree with each other and with the earlier non-computational analyses of
      the language group.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rama-ccoltekin-sofroniev:2017:VarDial</bibkey>
  </paper>
  <paper id='1204'>
    <title>Investigating Diatopic Variation in a Historical Corpus</title>
    <author>
      <first>Stefanie</first>
      <last>Dipper</last>
    </author>
    <author>
      <first>Sandra</first>
      <last>Waldenberger</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–45</pages>
    <url>http://www.aclweb.org/anthology/W17-1204</url>
    <doi>10.18653/v1/W17-1204</doi>
    <abstract>
      This paper investigates diatopic variation in a historical corpus of
      German. Based on equivalent word forms from different language areas,
      replacement rules and mappings are derived which describe the relations
      between these word forms. These rules and mappings are then interpreted as
      reflections of morphological, phonological or graphemic variation. Based
      on sample rules and mappings, we show that our approach can replicate
      results from historical linguistics. While previous studies were
      restricted to predefined word lists, or confined to single authors or
      texts, our approach uses a much wider range of data available in
      historical corpora.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dipper-waldenberger:2017:VarDial</bibkey>
  </paper>
  <paper id='1205'>
    <title>
      Author Profiling at PAN: from Age and Gender Identification to Language
      Variety Identification (invited talk)
    </title>
    <author>
      <first>Paolo</first>
      <last>Rosso</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46</pages>
    <url>http://www.aclweb.org/anthology/W17-1205</url>
    <doi>10.18653/v1/W17-1205</doi>
    <abstract>
      Author profiling is the study of how language is shared by people, a
      problem of growing importance in applications dealing with security, in
      order to understand who could be behind an anonymous threat message, and
      marketing, where companies may be interested in knowing the demographics
      of people that in online reviews liked or disliked their products. In this
      talk we will give an overview of the PAN shared tasks that since 2013 have
      been organised at CLEF and FIRE evaluation forums, mainly on age and
      gender identification in social media, although also personality
      recognition in Twitter as well as in code sources was also addressed. In
      2017 the PAN author profiling shared task addresses jointly gender and
      language variety identification in Twitter where tweets have been
      annotated with authors' gender and their specific variation of their
      native language: English (Australia, Canada, Great Britain, Ireland, New
      Zealand, United States), Spanish (Argentina, Chile, Colombia, Mexico,
      Peru, Spain, Venezuela), Portuguese (Brazil, Portugal), and Arabic (Egypt,
      Gulf, Levantine, Maghrebi).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rosso:2017:VarDial</bibkey>
  </paper>
  <paper id='1206'>
    <title>
      The similarity and Mutual Intelligibility between Amharic and Tigrigna
      Varieties
    </title>
    <author>
      <first>Tekabe Legesse</first>
      <last>Feleke</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–54</pages>
    <url>http://www.aclweb.org/anthology/W17-1206</url>
    <doi>10.18653/v1/W17-1206</doi>
    <abstract>
      The present study has examined the similarity and the mutual
      intelligibility between Amharic and Tigrigna using three tools namely
      Levenshtein distance, intelligibility test and questionnaires. The study
      has shown that both Tigrigna varieties have almost equal phonetic and
      lexical distances from Amharic. The study also indicated that Amharic
      speakers understand less than 50% of the two varieties. Furthermore, the
      study showed that Amharic speakers are more positive about the Ethiopian
      Tigrigna variety than the Eritrean Variety. However, their attitude
      towards the two varieties does not have an impact on their
      intelligibility. The Amharic speakers’ familiarity to the Tigrigna
      varieties is largely dependent on the genealogical relation between
      Amharic and the two Tigrigna varieties.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>feleke:2017:VarDial</bibkey>
  </paper>
  <paper id='1207'>
    <title>
      Why Catalan-Spanish Neural Machine Translation? Analysis, comparison and
      combination with standard Rule and Phrase-based technologies
    </title>
    <author>
      <first>Marta R.</first>
      <last>Costa-jussà</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–62</pages>
    <url>http://www.aclweb.org/anthology/W17-1207</url>
    <doi>10.18653/v1/W17-1207</doi>
    <abstract>
      Catalan and Spanish are two related languages given that both derive from
      Latin. They share similarities in several linguistic levels including
      morphology, syntax and semantics. This makes them particularly interesting
      for the MT task. Given the recent appearance and popularity of neural MT,
      this paper analyzes the performance of this new approach compared to the
      well-established rule-based and phrase-based MT systems. Experiments are
      reported on a large database of 180 million words. Results, in terms of
      standard automatic measures, show that neural MT clearly outperforms the
      rule-based and phrase-based MT system on in-domain test set, but it is
      worst in the out-of-domain test set. A naive system combination specially
      works for the latter. In-domain manual analysis shows that neural MT tends
      to improve both adequacy and fluency, for example, by being able to
      generate more natural translations instead of literal ones, choosing to
      the adequate target word when the source word has several translations and
      improving gender agreement. However, out-of-domain manual analysis shows
      how neural MT is more affected by unknown words or contexts.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>costajussa:2017:VarDial</bibkey>
  </paper>
  <paper id='1208'>
    <title>Kurdish Interdialect Machine Translation</title>
    <author>
      <first>Hossein</first>
      <last>Hassani</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>63–72</pages>
    <url>http://www.aclweb.org/anthology/W17-1208</url>
    <doi>10.18653/v1/W17-1208</doi>
    <abstract>
      This research suggests a method for machine translation among two Kurdish
      dialects. We chose the two widely spoken dialects, Kurmanji and Sorani,
      which are considered to be mutually unintelligible. Also, despite being
      spoken by about 30 million people in different countries, Kurdish is among
      less-resourced languages. The research used bi-dialectal dictionaries and
      showed that the lack of parallel corpora is not a major obstacle in
      machine translation between the two dialects. The experiments showed that
      the machine translated texts are comprehensible to those who do not speak
      the dialect. The research is the first attempt for inter-dialect machine
      translation in Kurdish and particularly could help in making online texts
      in one dialect comprehensible to those who only speak the target dialect.
      The results showed that the translated texts are in 71% and 79% cases
      rated as understandable for Kurmanji and Sorani respectively. They are
      rated as slightly-understandable in 29% cases for Kurmanji and 21% for
      Sorani.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hassani:2017:VarDial</bibkey>
  </paper>
  <paper id='1209'>
    <title>
      Twitter Language Identification Of Similar Languages And Dialects Without
      Ground Truth
    </title>
    <author>
      <first>Jennifer</first>
      <last>Williams</last>
    </author>
    <author>
      <first>Charlie</first>
      <last>Dagli</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73–83</pages>
    <url>http://www.aclweb.org/anthology/W17-1209</url>
    <doi>10.18653/v1/W17-1209</doi>
    <abstract>
      We present a new method to bootstrap filter Twitter language ID labels in
      our dataset for automatic language identification (LID). Our method
      combines geo-location, original Twitter LID labels, and Amazon Mechanical
      Turk to resolve missing and unreliable labels. We are the first to compare
      LID classification performance using the MIRA algorithm and langid.py. We
      show classifier performance on different versions of our dataset with high
      accuracy using only Twitter data, without ground truth, and very few
      training examples. We also show how Platt Scaling can be use to calibrate
      MIRA classifier output values into a probability distribution over
      candidate classes, making the output more intuitive. Our method allows for
      fine-grained distinctions between similar languages and dialects and
      allows us to rediscover the language composition of our Twitter dataset.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>williams-dagli:2017:VarDial</bibkey>
  </paper>
  <paper id='1210'>
    <title>Multi-source morphosyntactic tagging for spoken Rusyn</title>
    <author>
      <first>Yves</first>
      <last>Scherrer</last>
    </author>
    <author>
      <first>Achim</first>
      <last>Rabus</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>84–92</pages>
    <url>http://www.aclweb.org/anthology/W17-1210</url>
    <doi>10.18653/v1/W17-1210</doi>
    <abstract>
      This paper deals with the development of morphosyntactic taggers for
      spoken varieties of the Slavic minority language Rusyn. As neither
      annotated corpora nor parallel corpora are electronically available for
      Rusyn, we propose to combine existing resources from the etymologically
      close Slavic languages Russian, Ukrainian, Slovak, and Polish and adapt
      them to Rusyn. Using MarMoT as tagging toolkit, we show that a tagger
      trained on a balanced set of the four source languages outperforms single
      language taggers by about 9%, and that additional automatically induced
      morphosyntactic lexicons lead to further improvements. The best observed
      accuracies for Rusyn are 82.4% for part-of-speech tagging and 75.5% for
      full morphological tagging.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>scherrer-rabus:2017:VarDial</bibkey>
  </paper>
  <paper id='1211'>
    <title>Identifying dialects with textual and acoustic cues</title>
    <author>
      <first>Abualsoud</first>
      <last>Hanani</last>
    </author>
    <author>
      <first>Aziz</first>
      <last>Qaroush</last>
    </author>
    <author>
      <first>Stephen</first>
      <last>Taylor</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>93–101</pages>
    <url>http://www.aclweb.org/anthology/W17-1211</url>
    <doi>10.18653/v1/W17-1211</doi>
    <abstract>
      We describe several systems for identifying short samples of Arabic or
      Swiss-German dialects, which were prepared for the shared task of the 2017
      DSL Workshop. The Arabic data comprises both text and acoustic files, and
      our best run combined both. The Swiss-German data is text-only.
      Coincidently, our best runs achieved a accuracy of nearly 63% on both the
      Swiss-German and Arabic dialects tasks.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hanani-qaroush-taylor:2017:VarDial</bibkey>
  </paper>
  <paper id='1212'>
    <title>Evaluating HeLI with Non-Linear Mappings</title>
    <author>
      <first>Tommi</first>
      <last>Jauhiainen</last>
    </author>
    <author>
      <first>Krister</first>
      <last>Lindén</last>
    </author>
    <author>
      <first>Heidi</first>
      <last>Jauhiainen</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–108</pages>
    <url>http://www.aclweb.org/anthology/W17-1212</url>
    <doi>10.18653/v1/W17-1212</doi>
    <abstract>
      In this paper we describe the non-linear mappings we used with the
      Helsinki language identification method, HeLI, in the 4th edition of the
      Discriminating between Similar Languages (DSL) shared task, which was
      organized as part of the VarDial 2017 workshop. Our SUKI team participated
      on the closed track together with 10 other teams. Our system reached the
      7th position in the track. We describe the HeLI method and the non-linear
      mappings in mathematical notation. The HeLI method uses a probabilistic
      model with character n-grams and word-based backoff. We also describe our
      trials using the non-linear mappings instead of relative frequencies and
      we present statistics about the back-off function of the HeLI method.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jauhiainen-linden-jauhiainen:2017:VarDial</bibkey>
  </paper>
  <paper id='1213'>
    <title>A Perplexity-Based Method for Similar Languages Discrimination</title>
    <author>
      <first>Pablo</first>
      <last>Gamallo</last>
    </author>
    <author>
      <first>Jose Ramom</first>
      <last>Pichel</last>
    </author>
    <author>
      <first>Iñaki</first>
      <last>Alegria</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>109–114</pages>
    <url>http://www.aclweb.org/anthology/W17-1213</url>
    <doi>10.18653/v1/W17-1213</doi>
    <abstract>
      This article describes the system submitted by the Citius_Ixa_Imaxin team
      to the VarDial 2017 (DSL and GDI tasks). The strategy underlying our
      system is based on a language distance computed by means of model
      perplexity. The best model configuration we have tested is a voting system
      making use of several $n$-grams models of both words and characters, even
      if word unigrams turned out to be a very competitive model with reasonable
      results in the tasks we have participated. An error analysis has been
      performed in which we identified many test examples with no linguistic
      evidences to distinguish among the variants.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gamallo-pichel-alegria:2017:VarDial</bibkey>
  </paper>
  <paper id='1214'>
    <title>
      Improving the Character Ngram Model for the DSL Task with BM25 Weighting
      and Less Frequently Used Feature Sets
    </title>
    <author>
      <first>Yves</first>
      <last>Bestgen</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115–123</pages>
    <url>http://www.aclweb.org/anthology/W17-1214</url>
    <doi>10.18653/v1/W17-1214</doi>
    <abstract>
      This paper describes the system developed by the Centre for English Corpus
      Linguistics (CECL) to discriminating similar languages, language varieties
      and dialects. Based on a SVM with character and POStag n-grams as features
      and the BM25 weighting scheme, it achieved 92.7% accuracy in the
      Discriminating between Similar Languages (DSL) task, ranking first among
      eleven systems but with a lead over the next three teams of only 0.2%. A
      simpler version of the system ranked second in the German Dialect
      Identification (GDI) task thanks to several ad hoc postprocessing steps.
      Complementary analyses carried out by a cross-validation procedure suggest
      that the BM25 weighting scheme could be competitive in this type of tasks,
      at least in comparison with the sublinear TF-IDF. POStag n-grams also
      improved the system performance.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bestgen:2017:VarDial</bibkey>
  </paper>
  <paper id='1215'>
    <title>
      Discriminating between Similar Languages with Word-level Convolutional
      Neural Networks
    </title>
    <author>
      <first>Marcelo</first>
      <last>Criscuolo</last>
    </author>
    <author>
      <first>Sandra Maria</first>
      <last>Aluisio</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>124–130</pages>
    <url>http://www.aclweb.org/anthology/W17-1215</url>
    <doi>10.18653/v1/W17-1215</doi>
    <abstract>
      Discriminating between Similar Languages (DSL) is a challenging task
      addressed at the VarDial Workshop series. We report on our participation
      in the DSL shared task with a two-stage system. In the first stage,
      character n-grams are used to separate language groups, then specialized
      classifiers distinguish similar language varieties. We have conducted
      experiments with three system configurations and submitted one run for
      each. Our main approach is a word-level convolutional neural network (CNN)
      that learns task-specific vectors with minimal text preprocessing. We also
      experiment with multi-layer perceptron (MLP) networks and another hybrid
      configuration. Our best run achieved an accuracy of 90.76%, ranking 8th
      among 11 participants and getting very close to the system that ranked
      first (less than 2 points). Even though the CNN model could not achieve
      the best results, it still makes a viable approach to discriminating
      between similar languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>criscuolo-aluisio:2017:VarDial</bibkey>
  </paper>
  <paper id='1216'>
    <title>
      Cross-lingual dependency parsing for closely related languages -
      Helsinki's submission to VarDial 2017
    </title>
    <author>
      <first>Jörg</first>
      <last>Tiedemann</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>131–136</pages>
    <url>http://www.aclweb.org/anthology/W17-1216</url>
    <doi>10.18653/v1/W17-1216</doi>
    <abstract>
      This paper describes the submission from the University of Helsinki to the
      shared task on cross-lingual dependency parsing at VarDial 2017. We
      present work on annotation projection and treebank translation that gave
      good results for all three target languages in the test set. In
      particular, Slovak seems to work well with information coming from the
      Czech treebank, which is in line with related work. The attachment scores
      for cross-lingual models even surpass the fully supervised models trained
      on the target language treebank. Croatian is the most difficult language
      in the test set and the improvements over the baseline are rather modest.
      Norwegian works best with information coming from Swedish whereas Danish
      contributes surprisingly little.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tiedemann:2017:VarDial</bibkey>
  </paper>
  <paper id='1217'>
    <title>
      Discriminating between Similar Languages Using a Combination of Typed and
      Untyped Character N-grams and Words
    </title>
    <author>
      <first>Helena</first>
      <last>Gomez</last>
    </author>
    <author>
      <first>Ilia</first>
      <last>Markov</last>
    </author>
    <author>
      <first>Jorge</first>
      <last>Baptista</last>
    </author>
    <author>
      <first>Grigori</first>
      <last>Sidorov</last>
    </author>
    <author>
      <first>David</first>
      <last>Pinto</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>137–145</pages>
    <url>http://www.aclweb.org/anthology/W17-1217</url>
    <doi>10.18653/v1/W17-1217</doi>
    <abstract>
      This paper presents the cic_ualg's system that took part in the
      Discriminating between Similar Languages (DSL) shared task, held at the
      VarDial 2017 Workshop. This year's task aims at identifying 14 languages
      across 6 language groups using a corpus of excerpts of journalistic texts.
      Two classification approaches were compared: a single-step (all languages)
      approach and a two-step (language group and then languages within the
      group) approach. Features exploited include lexical features (unigrams of
      words) and character n-grams. Besides traditional (untyped) character
      n-grams, we introduce typed character n-grams in the DSL task. Experiments
      were carried out with different feature representation methods (binary and
      raw term frequency), frequency threshold values, and machine-learning
      algorithms – Support Vector Machines (SVM) and Multinomial Naive Bayes
      (MNB). Our best run in the DSL task achieved 91.46% accuracy.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gomez-EtAl:2017:VarDial</bibkey>
  </paper>
  <paper id='1218'>
    <title>
      Tübingen system in VarDial 2017 shared task: experiments with language
      identification and cross-lingual parsing
    </title>
    <author>
      <first>Çağrı</first>
      <last>Çöltekin</last>
    </author>
    <author>
      <first>Taraka</first>
      <last>Rama</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>146–155</pages>
    <url>http://www.aclweb.org/anthology/W17-1218</url>
    <doi>10.18653/v1/W17-1218</doi>
    <abstract>
      This paper describes our systems and results on VarDial 2017 shared tasks.
      Besides three language/dialect discrimination tasks, we also participated
      in the cross-lingual dependency parsing (CLP) task using a simple
      methodology which we also briefly describe in this paper. For all the
      discrimination tasks, we used linear SVMs with character and word
      features. The system achieves competitive results among other systems in
      the shared task. We also report additional experiments with neural network
      models. The performance of neural network models was close but always
      below the corresponding SVM classifiers in the discrimination tasks. For
      the cross-lingual parsing task, we experimented with an approach based on
      automatically translating the source treebank to the target language, and
      training a parser on the translated treebank. We used off-the-shelf tools
      for both translation and parsing. Despite achieving better-than-baseline
      results, our scores in CLP tasks were substantially lower than the scores
      of the other participants.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ccoltekin-rama:2017:VarDial</bibkey>
  </paper>
  <paper id='1219'>
    <title>
      When Sparse Traditional Models Outperform Dense Neural Networks: the
      Curious Case of Discriminating between Similar Languages
    </title>
    <author>
      <first>Maria</first>
      <last>Medvedeva</last>
    </author>
    <author>
      <first>Martin</first>
      <last>Kroon</last>
    </author>
    <author>
      <first>Barbara</first>
      <last>Plank</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>156–163</pages>
    <url>http://www.aclweb.org/anthology/W17-1219</url>
    <doi>10.18653/v1/W17-1219</doi>
    <abstract>
      We present the results of our participation in the VarDial 4 shared task
      on discriminating closely related languages. Our submission includes
      simple traditional models using linear support vector machines (SVMs) and
      a neural network (NN). The main idea was to leverage language group
      information. We did so with a two-layer approach in the traditional model
      and a multi-task objective in the neural network case. Our results confirm
      earlier findings: simple traditional models outperform neural networks
      consistently for this task, at least given the amount of systems we could
      examine in the available time. Our two-layer linear SVM ranked 2nd in the
      shared task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>medvedeva-kroon-plank:2017:VarDial</bibkey>
  </paper>
  <paper id='1220'>
    <title>German Dialect Identification in Interview Transcriptions</title>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>164–169</pages>
    <url>http://www.aclweb.org/anthology/W17-1220</url>
    <doi>10.18653/v1/W17-1220</doi>
    <abstract>
      This paper presents three systems submitted to the German Dialect
      Identification (GDI) task at the VarDial Evaluation Campaign 2017. The
      task consists of training models to identify the dialect of Swiss- German
      speech transcripts. The dialects included in the GDI dataset are Basel,
      Bern, Lucerne, and Zurich. The three systems we submitted are based on: a
      plurality ensemble, a mean probability ensemble, and a meta-classifier
      trained on character and word n-grams. The best results were obtained by
      the meta-classifier achieving 68.1% accuracy and 66.2% F1-score, ranking
      first among the 10 teams which participated in the GDI shared task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>malmasi-zampieri:2017:VarDial1</bibkey>
  </paper>
  <paper id='1221'>
    <title>
      CLUZH at VarDial GDI 2017: Testing a Variety of Machine Learning Tools for
      the Classification of Swiss German Dialects
    </title>
    <author>
      <first>Simon</first>
      <last>Clematide</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Makarov</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>170–177</pages>
    <url>http://www.aclweb.org/anthology/W17-1221</url>
    <doi>10.18653/v1/W17-1221</doi>
    <abstract>
      Our submissions for the GDI 2017 Shared Task are the results from three
      different types of classifiers: Naı̈ve Bayes, Conditional Random Fields
      (CRF), and Support Vector Machine (SVM). Our CRF-based run achieves a
      weighted F1 score of 65% (third rank) being beaten by the best system by
      0.9%. Measured by classification accuracy, our ensemble run (Naı̈ve Bayes,
      CRF, SVM) reaches 67% (second rank) being 1% lower than the best system.
      We also describe our experiments with Recurrent Neural Network (RNN)
      architectures. Since they performed worse than our non-neural approaches
      we did not include them in the submission.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>clematide-makarov:2017:VarDial</bibkey>
  </paper>
  <paper id='1222'>
    <title>Arabic Dialect Identification Using iVectors and ASR Transcripts</title>
    <author>
      <first>Shervin</first>
      <last>Malmasi</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>Zampieri</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>178–183</pages>
    <url>http://www.aclweb.org/anthology/W17-1222</url>
    <doi>10.18653/v1/W17-1222</doi>
    <abstract>
      This paper presents the systems submitted by the MAZA team to the Arabic
      Dialect Identification (ADI) shared task at the VarDial Evaluation
      Campaign 2017. The goal of the task is to evaluate computational models to
      identify the dialect of Arabic utterances using both audio and text
      transcriptions. The ADI shared task dataset included Modern Standard
      Arabic (MSA) and four Arabic dialects: Egyptian, Gulf, Levantine, and
      North-African. The three systems submitted by MAZA are based on
      combinations of multiple machine learning classifiers arranged as (1)
      voting ensemble; (2) mean probability ensemble; (3) meta-classifier. The
      best results were obtained by the meta-classifier achieving 71.7%
      accuracy, ranking second among the six teams which participated in the ADI
      shared task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>malmasi-zampieri:2017:VarDial2</bibkey>
  </paper>
  <paper id='1223'>
    <title>Discriminating between Similar Languages using Weighted Subword Features</title>
    <author>
      <first>Adrien</first>
      <last>Barbaresi</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184–189</pages>
    <url>http://www.aclweb.org/anthology/W17-1223</url>
    <doi>10.18653/v1/W17-1223</doi>
    <abstract>
      The present contribution revolves around a contrastive subword n-gram
      model which has been tested in the Discriminating between Similar
      Languages shared task. I present and discuss the method used in this
      14-way language identification task comprising varieties of 6 main
      language groups. It features the following characteristics: (1) the
      preprocessing and conversion of a collection of documents to sparse
      features; (2) weighted character n-gram profiles; (3) a multinomial
      Bayesian classifier. Meaningful bag-of-n-grams features can be used as a
      system in a straightforward way, my approach outperforms most of the
      systems used in the DSL shared task (3rd rank).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>barbaresi:2017:VarDial</bibkey>
  </paper>
  <paper id='1224'>
    <title>Exploring Lexical and Syntactic Features for Language Variety Identification</title>
    <author>
      <first>Chris</first>
      <last>van der Lee</last>
    </author>
    <author>
      <first>Antal</first>
      <last>van den Bosch</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>190–199</pages>
    <url>http://www.aclweb.org/anthology/W17-1224</url>
    <doi>10.18653/v1/W17-1224</doi>
    <abstract>
      We present a method to discriminate between texts written in either the
      Netherlandic or the Flemish variant of the Dutch language. The method
      draws on a feature bundle representing text statistics, syntactic
      features, and word $n$-grams. Text statistics include average word length
      and sentence length, while syntactic features include ratios of function
      words and part-of-speech $n$-grams. The effectiveness of the classifier
      was measured by classifying Dutch subtitles developed for either Dutch or
      Flemish television. Several machine learning algorithms were compared as
      well as feature combination methods in order to find the optimal
      generalization performance. A machine-learning meta classifier based on
      AdaBoost attained the best F-score of 0.92.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vanderlee-vandenbosch:2017:VarDial</bibkey>
  </paper>
  <paper id='1225'>
    <title>Learning to Identify Arabic and German Dialects using Multiple Kernels</title>
    <author>
      <first>Radu Tudor</first>
      <last>Ionescu</last>
    </author>
    <author>
      <first>Andrei</first>
      <last>Butnaru</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200–209</pages>
    <url>http://www.aclweb.org/anthology/W17-1225</url>
    <doi>10.18653/v1/W17-1225</doi>
    <abstract>
      We present a machine learning approach for the Arabic Dialect
      Identification (ADI) and the German Dialect Identification (GDI) Closed
      Shared Tasks of the DSL 2017 Challenge. The proposed approach combines
      several kernels using multiple kernel learning. While most of our kernels
      are based on character p-grams (also known as n-grams) extracted from
      speech transcripts, we also use a kernel based on i-vectors, a
      low-dimensional representation of audio recordings, provided only for the
      Arabic data. In the learning stage, we independently employ Kernel
      Discriminant Analysis (KDA) and Kernel Ridge Regression (KRR). Our
      approach is shallow and simple, but the empirical results obtained in the
      shared tasks prove that it achieves very good results. Indeed, we ranked
      on the first place in the ADI Shared Task with a weighted F1 score of
      76.32% (4.62% above the second place) and on the fifth place in the GDI
      Shared Task with a weighted F1 score of 63.67% (2.57% below the first
      place).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ionescu-butnaru:2017:VarDial</bibkey>
  </paper>
  <paper id='1226'>
    <title>Slavic Forest, Norwegian Wood</title>
    <author>
      <first>Rudolf</first>
      <last>Rosa</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Zeman</last>
    </author>
    <author>
      <first>David</first>
      <last>Mareček</last>
    </author>
    <author>
      <first>Zdeněk</first>
      <last>Žabokrtský</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties
      and Dialects (VarDial)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>210–219</pages>
    <url>http://www.aclweb.org/anthology/W17-1226</url>
    <doi>10.18653/v1/W17-1226</doi>
    <abstract>
      We once had a corp, or should we say, it once had us They showed us its
      tags, isn't it great, unified tags They asked us to parse and they told us
      to use everything So we looked around and we noticed there was near
      nothing We took other langs, bitext aligned: words one-to-one We played
      for two weeks, and then they said, here is the test The parser kept
      training till morning, just until deadline So we had to wait and hope what
      we get would be just fine And, when we awoke, the results were done, we
      saw we'd won So, we wrote this paper, isn't it good, Norwegian wood.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rosa-EtAl:2017:VarDial</bibkey>
  </paper>
  <paper id='1300'>
    <title>Proceedings of the Third Arabic Natural Language Processing Workshop</title>
    <editor>Nizar Habash</editor>
    <editor>Mona Diab</editor>
    <editor>Kareem Darwish</editor>
    <editor>Wassim El-Hajj</editor>
    <editor>Hend Al-Khalifa</editor>
    <editor>Houda Bouamor</editor>
    <editor>Nadi Tomeh</editor>
    <editor>Mahmoud El-Haj</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-13</url>
    <doi>10.18653/v1/W17-13</doi>
    <bibtype>book</bibtype>
    <bibkey>W17-13:2017</bibkey>
  </paper>
  <paper id='1301'>
    <title>Identification of Languages in Algerian Arabic Multilingual Documents</title>
    <author>
      <first>Wafia</first>
      <last>Adouane</last>
    </author>
    <author>
      <first>Simon</first>
      <last>Dobnik</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–8</pages>
    <url>http://www.aclweb.org/anthology/W17-1301</url>
    <doi>10.18653/v1/W17-1301</doi>
    <abstract>
      This paper presents a language identification system designed to detect
      the language of each word, in its context, in a multilingual documents as
      generated in social media by bilingual/multilingual communities, in our
      case speakers of Algerian Arabic. We frame the task as a sequence tagging
      problem and use supervised machine learning with standard methods like HMM
      and Ngram classification tagging. We also experiment with a lexicon-based
      method. Combining all the methods in a fall-back mechanism and introducing
      some linguistic rules, to deal with unseen tokens and ambiguous words,
      gives an overall accuracy of 93.14%. Finally, we introduced rules for
      language identification from sequences of recognised words.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>adouane-dobnik:2017:W17-13</bibkey>
  </paper>
  <paper id='1302'>
    <title>Arabic Diacritization: Stats, Rules, and Hacks</title>
    <author>
      <first>Kareem</first>
      <last>Darwish</last>
    </author>
    <author>
      <first>Hamdy</first>
      <last>Mubarak</last>
    </author>
    <author>
      <first>Ahmed</first>
      <last>Abdelali</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>9–17</pages>
    <url>http://www.aclweb.org/anthology/W17-1302</url>
    <doi>10.18653/v1/W17-1302</doi>
    <abstract>
      In this paper, we present a new and fast state-of-the-art Arabic
      diacritizer that guesses the diacritics of words and then their case
      endings. We employ a Viterbi decoder at word-level with back-off to stem,
      morphological patterns, and transliteration and sequence labeling based
      diacritization of named entities. For case endings, we use Support Vector
      Machine (SVM) based ranking coupled with morphological patterns and
      linguistic rules to properly guess case endings. We achieve a low word
      level diacritization error of 3.29% and 12.77% without and with case
      endings respectively on a new multi-genre free of copyright test set. We
      are making the diacritizer available for free for research purposes.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>darwish-mubarak-abdelali:2017:W17-13</bibkey>
  </paper>
  <paper id='1303'>
    <title>Semantic Similarity of Arabic Sentences with Word Embeddings</title>
    <author>
      <first>El Moatez Billah</first>
      <last>Nagoudi</last>
    </author>
    <author>
      <first>Didier</first>
      <last>Schwab</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–24</pages>
    <url>http://www.aclweb.org/anthology/W17-1303</url>
    <doi>10.18653/v1/W17-1303</doi>
    <abstract>
      Semantic textual similarity is the basis of countless applications and
      plays an important role in diverse areas, such as information retrieval,
      plagiarism detection, information extraction and machine translation. This
      article proposes an innovative word embedding-based system devoted to
      calculate the semantic similarity in Arabic sentences. The main idea is to
      exploit vectors as word representations in a multidimensional space in
      order to capture the semantic and syntactic properties of words. IDF
      weighting and Part-of-Speech tagging are applied on the examined sentences
      to support the identification of words that are highly descriptive in each
      sentence. The performance of our proposed system is confirmed through the
      Pearson correlation between our assigned semantic similarity scores and
      human judgments.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nagoudi-schwab:2017:W17-13</bibkey>
  </paper>
  <paper id='1304'>
    <title>
      Morphological Analysis for the Maltese Language: The challenges of a
      hybrid system
    </title>
    <author>
      <first>Claudia</first>
      <last>Borg</last>
    </author>
    <author>
      <first>Albert</first>
      <last>Gatt</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–34</pages>
    <url>http://www.aclweb.org/anthology/W17-1304</url>
    <doi>10.18653/v1/W17-1304</doi>
    <abstract>
      Maltese is a morphologically rich language with a hybrid morphological
      system which features both concatenative and non-concatenative processes.
      This paper analyses the impact of this hybridity on the performance of
      machine learning techniques for morphological labelling and clustering. In
      particular, we analyse a dataset of morphologically related word clusters
      to evaluate the difference in results for concatenative and
      non-concatenative clusters. We also describe research carried out in
      morphological labelling, with a particular focus on the verb category. Two
      evaluations were carried out, one using an unseen dataset, and another one
      using a gold standard dataset which was manually labelled. The gold
      standard dataset was split into concatenative and non-concatenative to
      analyse the difference in results between the two morphological systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>borg-gatt:2017:W17-13</bibkey>
  </paper>
  <paper id='1305'>
    <title>A Morphological Analyzer for Gulf Arabic Verbs</title>
    <author>
      <first>Salam</first>
      <last>Khalifa</last>
    </author>
    <author>
      <first>Sara</first>
      <last>Hassan</last>
    </author>
    <author>
      <first>Nizar</first>
      <last>Habash</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–45</pages>
    <url>http://www.aclweb.org/anthology/W17-1305</url>
    <doi>10.18653/v1/W17-1305</doi>
    <abstract>
      We present CALIMAGLF, a Gulf Arabic morphological analyzer currently
      covering over 2,600 verbal lemmas. We describe in detail the process of
      building the analyzer starting from phonetic dictionary entries to fully
      inflected orthographic paradigms and associated lexicon and orthographic
      variants. We evaluate the coverage of CALIMA-GLF against Modern Standard
      Arabic and Egyptian Arabic analyzers on part of a Gulf Arabic novel.
      CALIMA-GLF verb analysis token recall for identifying correct POS tag
      outperforms both the Modern Standard Arabic and Egyptian Arabic analyzers
      by over 27.4% and 16.9% absolute, respectively.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>khalifa-hassan-habash:2017:W17-13</bibkey>
  </paper>
  <paper id='1306'>
    <title>A Neural Architecture for Dialectal Arabic Segmentation</title>
    <author>
      <first>Younes</first>
      <last>Samih</last>
    </author>
    <author>
      <first>Mohammed</first>
      <last>Attia</last>
    </author>
    <author>
      <first>Mohamed</first>
      <last>Eldesouki</last>
    </author>
    <author>
      <first>Ahmed</first>
      <last>Abdelali</last>
    </author>
    <author>
      <first>Hamdy</first>
      <last>Mubarak</last>
    </author>
    <author>
      <first>Laura</first>
      <last>Kallmeyer</last>
    </author>
    <author>
      <first>Kareem</first>
      <last>Darwish</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–54</pages>
    <url>http://www.aclweb.org/anthology/W17-1306</url>
    <doi>10.18653/v1/W17-1306</doi>
    <abstract>
      The automated processing of Arabic Dialects is challenging due to the lack
      of spelling standards and to the scarcity of annotated data and resources
      in general. Segmentation of words into its constituent parts is an
      important processing building block. In this paper, we show how a
      segmenter can be trained using only 350 annotated tweets using neural
      networks without any normalization or use of lexical features or lexical
      resources. We deal with segmentation as a sequence labeling problem at the
      character level. We show experimentally that our model can rival
      state-of-the-art methods that rely on additional resources.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>samih-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1307'>
    <title>Sentiment Analysis of Tunisian Dialects: Linguistic Ressources and Experiments</title>
    <author>
      <first>Salima</first>
      <last>Medhaffar</last>
    </author>
    <author>
      <first>Fethi</first>
      <last>Bougares</last>
    </author>
    <author>
      <first>Yannick</first>
      <last>Estève</last>
    </author>
    <author>
      <first>Lamia</first>
      <last>Hadrich-Belguith</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–61</pages>
    <url>http://www.aclweb.org/anthology/W17-1307</url>
    <doi>10.18653/v1/W17-1307</doi>
    <abstract>
      Dialectal Arabic (DA) is significantly different from the Arabic language
      taught in schools and used in written communication and formal speech
      (broadcast news, religion, politics, etc.). There are many existing
      researches in the field of Arabic language Sentiment Analysis (SA);
      however, they are generally restricted to Modern Standard Arabic (MSA) or
      some dialects of economic or political interest. In this paper we are
      interested in the SA of the Tunisian Dialect. We utilize Machine Learning
      techniques to determine the polarity of comments written in Tunisian
      Dialect. First, we evaluate the SA systems performances with models
      trained using freely available MSA and Multi-dialectal data sets. We then
      collect and annotate a Tunisian Dialect corpus of 17.000 comments from
      Facebook. This corpus allows us a significant accuracy improvement
      compared to the best model trained on other Arabic dialects or MSA data.
      We believe that this first freely available corpus will be valuable to
      researchers working in the field of Tunisian Sentiment Analysis and
      similar areas.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>medhaffar-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1308'>
    <title>CAT: Credibility Analysis of Arabic Content on Twitter</title>
    <author>
      <first>Rim</first>
      <last>El Ballouli</last>
    </author>
    <author>
      <first>Wassim</first>
      <last>El-Hajj</last>
    </author>
    <author>
      <first>Ahmad</first>
      <last>Ghandour</last>
    </author>
    <author>
      <first>Shady</first>
      <last>Elbassuoni</last>
    </author>
    <author>
      <first>Hazem</first>
      <last>Hajj</last>
    </author>
    <author>
      <first>Khaled</first>
      <last>Shaban</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>62–71</pages>
    <url>http://www.aclweb.org/anthology/W17-1308</url>
    <doi>10.18653/v1/W17-1308</doi>
    <abstract>
      Data generated on Twitter has become a rich source for various data mining
      tasks. Those data analysis tasks that are dependent on the tweet
      semantics, such as sentiment analysis, emotion mining, and rumor detection
      among others, suffer considerably if the tweet is not credible, not real,
      or spam. In this paper, we perform an extensive analysis on credibility of
      Arabic content on Twitter. We also build a classification model (CAT) to
      automatically predict the credibility of a given Arabic tweet. Of
      particular originality is the inclusion of features extracted directly or
      indirectly from the author's profile and timeline. To train and test CAT,
      we annotated for credibility a data set of 9,000 Arabic tweets that are
      topic independent. CAT achieved consistent improvements in predicting the
      credibility of the tweets when compared to several baselines and when
      compared to the state-of-the-art approach with an improvement of 21% in
      weighted average F-measure. We also conducted experiments to highlight the
      importance of the user-based features as opposed to the content-based
      features. We conclude our work with a feature reduction experiment that
      highlights the best indicative features of credibility.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>elballouli-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1309'>
    <title>A New Error Annotation for Dyslexic texts in Arabic</title>
    <author>
      <first>Maha</first>
      <last>Alamri</last>
    </author>
    <author>
      <first>William J.</first>
      <last>Teahan</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72–78</pages>
    <url>http://www.aclweb.org/anthology/W17-1309</url>
    <doi>10.18653/v1/W17-1309</doi>
    <abstract>
      This paper aims to develop a new classification of errors made in Arabic
      by those suffering from dyslexia to be used in the annotation of the
      Arabic dyslexia corpus (BDAC). The dyslexic error classification for
      Arabic texts (DECA) comprises a list of spelling errors extracted from
      previous studies and a collection of texts written by people with dyslexia
      that can provide a framework to help analyse specific errors committed by
      dyslexic writers. The classification comprises 37 types of errors, grouped
      into nine categories. The paper also discusses building a corpus of
      dyslexic Arabic texts that uses the error annotation scheme and provides
      an analysis of the errors that were found in the texts.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alamri-teahan:2017:W17-13</bibkey>
  </paper>
  <paper id='1310'>
    <title>
      An Unsupervised Speaker Clustering Technique based on SOM and I-vectors
      for Speech Recognition Systems
    </title>
    <author>
      <first>Hany</first>
      <last>Ahmed</last>
    </author>
    <author>
      <first>Mohamed</first>
      <last>Elaraby</last>
    </author>
    <author>
      <first>Abdullah</first>
      <last>M. Mousa</last>
    </author>
    <author>
      <first>Mostafa</first>
      <last>Elhosiny</last>
    </author>
    <author>
      <first>Sherif</first>
      <last>Abdou</last>
    </author>
    <author>
      <first>Mohsen</first>
      <last>Rashwan</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–83</pages>
    <url>http://www.aclweb.org/anthology/W17-1310</url>
    <doi>10.18653/v1/W17-1310</doi>
    <abstract>
      In this paper, we introduce an enhancement for speech recognition systems
      using an unsupervised speaker clustering technique. The proposed technique
      is mainly based on I-vectors and Self-Organizing Map Neural
      Network(SOM).The input to the proposed algorithm is a set of speech
      utterances. For each utterance, we extract 100-dimensional I-vector and
      then SOM is used to group the utterances to different speakers. In our
      experiments, we compared our technique with Normalized Cross Likelihood
      ratio Clustering (NCLR). Results show that the proposed technique reduces
      the speaker error rate in comparison with NCLR. Finally, we have
      experimented the effect of speaker clustering on Speaker Adaptive Training
      (SAT) in a speech recognition system implemented to test the performance
      of the proposed technique. It was noted that the proposed technique
      reduced the WER over clustering speakers with NCLR.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ahmed-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1311'>
    <title>SHAKKIL: An Automatic Diacritization System for Modern Standard Arabic Texts</title>
    <author>
      <first>Amany</first>
      <last>Fashwan</last>
    </author>
    <author>
      <first>Sameh</first>
      <last>Alansary</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>84–93</pages>
    <url>http://www.aclweb.org/anthology/W17-1311</url>
    <doi>10.18653/v1/W17-1311</doi>
    <abstract>
      This paper sheds light on a system that would be able to diacritize Arabic
      texts automatically (SHAKKIL). In this system, the diacritization problem
      will be handled through two levels; morphological and syntactic processing
      levels. The adopted morphological disambiguation algorithm depends on four
      layers; Uni-morphological form layer, rule-based morphological
      disambiguation layer, statistical-based disambiguation layer and Out Of
      Vocabulary (OOV) layer. The adopted syntactic disambiguation algorithms is
      concerned with detecting the case ending diacritics depending on a rule
      based approach simulating the shallow parsing technique. This will be
      achieved using an annotated corpus for extracting the Arabic linguistic
      rules, building the language models and testing the system output. This
      system is considered as a good trial of the interaction between rule-based
      approach and statistical approach, where the rules can help the statistics
      in detecting the right diacritization and vice versa. At this point, the
      morphological Word Error Rate (WER) is 4.56% while the morphological
      Diacritic Error Rate (DER) is 1.88% and the syntactic WER is 9.36%. The
      best WER is 14.78% compared to the best-published results, of (Abandah,
      2015); 11.68%, (Rashwan, et al., 2015); 12.90% and (Metwally, Rashwan,
      & Atiya, 2016); 13.70%.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fashwan-alansary:2017:W17-13</bibkey>
  </paper>
  <paper id='1312'>
    <title>Arabic Tweets Treebanking and Parsing: A Bootstrapping Approach</title>
    <author>
      <first>Fahad</first>
      <last>Albogamy</last>
    </author>
    <author>
      <first>Allan</first>
      <last>Ramsay</last>
    </author>
    <author>
      <first>Hanady</first>
      <last>Ahmed</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>94–99</pages>
    <url>http://www.aclweb.org/anthology/W17-1312</url>
    <doi>10.18653/v1/W17-1312</doi>
    <abstract>
      In this paper, we propose using a "bootstrapping" method for constructing
      a dependency treebank of Arabic tweets. This method uses a rule-based
      parser to create a small treebank of one thousand Arabic tweets and a
      data-driven parser to create a larger treebank by using the small treebank
      as a seed training set. We are able to create a dependency treebank from
      unlabelled tweets without any manual intervention. Experiments results
      show that this method can improve the speed of training the parser and the
      accuracy of the resulting parsers.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>albogamy-ramsay-ahmed:2017:W17-13</bibkey>
  </paper>
  <paper id='1313'>
    <title>
      Identifying Effective Translations for Cross-lingual Arabic-to-English
      User-generated Speech Search
    </title>
    <author>
      <first>Ahmad</first>
      <last>Khwileh</last>
    </author>
    <author>
      <first>Haithem</first>
      <last>Afli</last>
    </author>
    <author>
      <first>Gareth</first>
      <last>Jones</last>
    </author>
    <author>
      <first>Andy</first>
      <last>Way</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>100–109</pages>
    <url>http://www.aclweb.org/anthology/W17-1313</url>
    <doi>10.18653/v1/W17-1313</doi>
    <abstract>
      Cross Language Information Retrieval (CLIR) systems are a valuable tool to
      enable speakers of one language to search for content of interest
      expressed in a different language. A group for whom this is of particular
      interest is bilingual Arabic speakers who wish to search for English
      language content using information needs expressed in Arabic queries. A
      key challenge in CLIR is crossing the language barrier between the query
      and the documents. The most common approach to bridging this gap is
      automated query translation, which can be unreliable for vague or short
      queries. In this work, we examine the potential for improving CLIR
      effectiveness by predicting the translation effectiveness using Query
      Performance Prediction (QPP) techniques. We propose a novel QPP method to
      estimate the quality of translation for an Arabic-English Cross-lingual
      User-generated Speech Search (CLUGS) task. We present an empirical
      evaluation that demonstrates the quality of our method on alternative
      translation outputs extracted from an Arabic-to-English Machine
      Translation system developed for this task. Finally, we show how this
      framework can be integrated in CLUGS to find relevant translations for
      improved retrieval performance.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>khwileh-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1314'>
    <title>
      A Characterization Study of Arabic Twitter Data with a Benchmarking for
      State-of-the-Art Opinion Mining Models
    </title>
    <author>
      <first>Ramy</first>
      <last>Baly</last>
    </author>
    <author>
      <first>Gilbert</first>
      <last>Badaro</last>
    </author>
    <author>
      <first>Georges</first>
      <last>El-Khoury</last>
    </author>
    <author>
      <first>Rawan</first>
      <last>Moukalled</last>
    </author>
    <author>
      <first>Rita</first>
      <last>Aoun</last>
    </author>
    <author>
      <first>Hazem</first>
      <last>Hajj</last>
    </author>
    <author>
      <first>Wassim</first>
      <last>El-Hajj</last>
    </author>
    <author>
      <first>Nizar</first>
      <last>Habash</last>
    </author>
    <author>
      <first>Khaled</first>
      <last>Shaban</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110–118</pages>
    <url>http://www.aclweb.org/anthology/W17-1314</url>
    <doi>10.18653/v1/W17-1314</doi>
    <abstract>
      Opinion mining in Arabic is a challenging task given the rich morphology
      of the language. The task becomes more challenging when it is applied to
      Twitter data, which contains additional sources of noise, such as the use
      of unstandardized dialectal variations, the nonconformation to grammatical
      rules, the use of Arabizi and code-switching, and the use of non-text
      objects such as images and URLs to express opinion. In this paper, we
      perform an analytical study to observe how such linguistic phenomena vary
      across different Arab regions. This study of Arabic Twitter
      characterization aims at providing better understanding of Arabic Tweets,
      and fostering advanced research on the topic. Furthermore, we explore the
      performance of the two schools of machine learning on Arabic Twitter,
      namely the feature engineering approach and the deep learning approach. We
      consider models that have achieved state-of-the-art performance for
      opinion mining in English. Results highlight the advantages of using deep
      learning-based models, and confirm the importance of using morphological
      abstractions to address Arabic’s complex morphology.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>baly-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1315'>
    <title>Robust Dictionary Lookup in Multiple Noisy Orthographies</title>
    <author>
      <first>Lingliang</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Nizar</first>
      <last>Habash</last>
    </author>
    <author>
      <first>Godfried</first>
      <last>Toussaint</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–129</pages>
    <url>http://www.aclweb.org/anthology/W17-1315</url>
    <doi>10.18653/v1/W17-1315</doi>
    <abstract>
      We present the MultiScript Phonetic Search algorithm to address the
      problem of language learners looking up unfamiliar words that they heard.
      We apply it to Arabic dictionary lookup with noisy queries done using both
      the Arabic and Roman scripts. Our algorithm is based on a computational
      phonetic distance metric that can be optionally machine learned. To
      benchmark our performance, we created the ArabScribe dataset, containing
      10,000 noisy transcriptions of random Arabic dictionary words. Our
      algorithm outperforms Google Translate's “did you mean" feature, as well
      as the Yamli smart Arabic keyboard.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-habash-toussaint:2017:W17-13</bibkey>
  </paper>
  <paper id='1316'>
    <title>Arabic POS Tagging: Don't Abandon Feature Engineering Just Yet</title>
    <author>
      <first>Kareem</first>
      <last>Darwish</last>
    </author>
    <author>
      <first>Hamdy</first>
      <last>Mubarak</last>
    </author>
    <author>
      <first>Ahmed</first>
      <last>Abdelali</last>
    </author>
    <author>
      <first>Mohamed</first>
      <last>Eldesouki</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>130–137</pages>
    <url>http://www.aclweb.org/anthology/W17-1316</url>
    <doi>10.18653/v1/W17-1316</doi>
    <abstract>
      This paper focuses on comparing between using Support Vector Machine based
      ranking (SVM-Rank) and Bidirectional Long-Short-Term-Memory (bi-LSTM)
      neural-network based sequence labeling in building a state-of-the-art
      Arabic part-of-speech tagging system. Using SVM-Rank leads to
      state-of-the-art results, but with a fair amount of feature engineering.
      Using bi-LSTM, particularly when combined with word embeddings, may lead
      to competitive POS-tagging results by automatically deducing latent
      linguistic features. However, we show that augmenting bi-LSTM sequence
      labeling with some of the features that we used for the SVM-Rank based
      tagger yields to further improvements. We also show that gains that
      realized by using embeddings may not be additive with the gains achieved
      by the features. We are open-sourcing both the SVM-Rank and the bi-LSTM
      based systems for free.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>darwish-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1317'>
    <title>Toward a Web-based Speech Corpus for Algerian Dialectal Arabic Varieties</title>
    <author>
      <first>Soumia</first>
      <last>Bougrine</last>
    </author>
    <author>
      <first>Aicha</first>
      <last>Chorana</last>
    </author>
    <author>
      <first>Abdallah</first>
      <last>Lakhdari</last>
    </author>
    <author>
      <first>Hadda</first>
      <last>Cherroun</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>138–146</pages>
    <url>http://www.aclweb.org/anthology/W17-1317</url>
    <doi>10.18653/v1/W17-1317</doi>
    <abstract>
      The success of machine learning for automatic speech processing has raised
      the need for large scale datasets. However, collecting such data is often
      a challenging task as it implies significant investment involving time and
      money cost. In this paper, we devise a recipe for building largescale
      Speech Corpora by harnessing Web resources namely YouTube, other Social
      Media, Online Radio and TV. We illustrate our methodology by building
      KALAM’DZ, An Arabic Spoken corpus dedicated to Algerian dialectal
      varieties. The preliminary version of our dataset covers all major
      Algerian dialects. In addition, we make sure that this material takes into
      account numerous aspects that foster its richness. In fact, we have
      targeted various speech topics. Some automatic and manual annotations are
      provided. They gather useful information related to the speakers and
      sub-dialect information at the utterance level. Our corpus encompasses the
      8 major Algerian Arabic sub-dialects with 4881 speakers and more than
      104.4 hours segmented in utterances of at least 6 s.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bougrine-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1318'>
    <title>
      Not All Segments are Created Equal: Syntactically Motivated Sentiment
      Analysis in Lexical Space
    </title>
    <author>
      <first>Muhammad</first>
      <last>Abdul-Mageed</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>147–156</pages>
    <url>http://www.aclweb.org/anthology/W17-1318</url>
    <doi>10.18653/v1/W17-1318</doi>
    <abstract>
      Although there is by now a considerable amount of research on subjectivity
      and sentiment analysis on morphologically-rich languages, it is still
      unclear how lexical information can best be modeled in these languages. To
      bridge this gap, we build effective models exploiting exclusively gold-
      and machine-segmented lexical input and successfully employ syntactically
      motivated feature selection to improve classification. Our best models
      achieve significantly above the baselines, with 67.93% and 69.37%
      accuracies for subjectivity and sentiment classification respectively.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>abdulmageed:2017:W17-13</bibkey>
  </paper>
  <paper id='1319'>
    <title>An enhanced automatic speech recognition system for Arabic</title>
    <author>
      <first>Mohamed Amine</first>
      <last>Menacer</last>
    </author>
    <author>
      <first>Odile</first>
      <last>Mella</last>
    </author>
    <author>
      <first>Dominique</first>
      <last>Fohr</last>
    </author>
    <author>
      <first>Denis</first>
      <last>Jouvet</last>
    </author>
    <author>
      <first>David</first>
      <last>Langlois</last>
    </author>
    <author>
      <first>Kamel</first>
      <last>Smaili</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>157–165</pages>
    <url>http://www.aclweb.org/anthology/W17-1319</url>
    <doi>10.18653/v1/W17-1319</doi>
    <abstract>
      Automatic speech recognition for Arabic is a very challenging task.
      Despite all the classical techniques for Automatic Speech Recognition
      (ASR), which can be efficiently applied to Arabic speech recognition, it
      is essential to take into consideration the language specificities to
      improve the system performance. In this article, we focus on Modern
      Standard Arabic (MSA) speech recognition. We introduce the challenges
      related to Arabic language, namely the complex morphology nature of the
      language and the absence of the short vowels in written text, which leads
      to several potential vowelization for each graphemes, which is often
      conflicting. We develop an ASR system for MSA by using Kaldi toolkit.
      Several acoustic and language models are trained. We obtain a Word Error
      Rate (WER) of 14.42 for the baseline system and 12.2 relative improvement
      by rescoring the lattice and by rewriting the output with the right Z
      hamoza above or below Alif.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>menacer-EtAl:2017:W17-13</bibkey>
  </paper>
  <paper id='1320'>
    <title>Universal Dependencies for Arabic</title>
    <author>
      <first>Dima</first>
      <last>Taji</last>
    </author>
    <author>
      <first>Nizar</first>
      <last>Habash</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Zeman</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>166–176</pages>
    <url>http://www.aclweb.org/anthology/W17-1320</url>
    <doi>10.18653/v1/W17-1320</doi>
    <abstract>
      We describe the process of creating NUDAR, a Universal Dependency treebank
      for Arabic. We present the conversion from the Penn Arabic Tree- bank to
      the Universal Dependency syntactic representation through an intermediate
      dependency representation. We discuss the challenges faced in the
      conversion of the trees, the decisions we made to solve them, and the
      validation of our conversion. We also present initial parsing results on
      NUDAR.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>taji-habash-zeman:2017:W17-13</bibkey>
  </paper>
  <paper id='1321'>
    <title>
      A Layered Language Model based Hybrid Approach to Automatic Full
      Diacritization of Arabic
    </title>
    <author>
      <first>Mohamed</first>
      <last>Al-Badrashiny</last>
    </author>
    <author>
      <first>Abdelati</first>
      <last>Hawwari</last>
    </author>
    <author>
      <first>Mona</first>
      <last>Diab</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>177–184</pages>
    <url>http://www.aclweb.org/anthology/W17-1321</url>
    <doi>10.18653/v1/W17-1321</doi>
    <abstract>
      In this paper we present a system for automatic Arabic text diacritization
      using three levels of analysis granularity in a layered back off manner.
      We build and exploit diacritized language models (LM) for each of three
      different levels of granularity: surface form, morphologically segmented
      into prefix/stem/suffix, and character level. For each of the passes, we
      use Viterbi search to pick the most probable diacritization per word in
      the input. We start with the surface form LM, followed by the
      morphological level, then finally we leverage the character level LM. Our
      system outperforms all of the published systems evaluated against the same
      training and test data. It achieves a 10.87% WER for complete full
      diacritization including lexical and syntactic diacritization, and 3.0%
      WER for lexical diacritization, ignoring syntactic diacritization.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>albadrashiny-hawwari-diab:2017:W17-13</bibkey>
  </paper>
  <paper id='1322'>
    <title>Arabic Textual Entailment with Word Embeddings</title>
    <author>
      <first>Nada</first>
      <last>Almarwani</last>
    </author>
    <author>
      <first>Mona</first>
      <last>Diab</last>
    </author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>185–190</pages>
    <url>http://www.aclweb.org/anthology/W17-1322</url>
    <doi>10.18653/v1/W17-1322</doi>
    <abstract>
      Determining the textual entailment be- tween texts is important in many
      NLP tasks, such as summarization, question answering, and information
      extraction and retrieval. Various methods have been suggested based on
      external knowledge sources; however, such resources are not always
      available in all languages and their acquisition is typically laborious
      and very costly. Distributional word representations such as word
      embeddings learned over large corpora have been shown to capture syntactic
      and semantic word relationships. Such models have contributed to improv-
      ing the performance of several NLP tasks. In this paper, we address the
      problem of textual entailment in Arabic. We employ both traditional
      features and distributional representations. Crucially, we do not de- pend
      on any external resources in the pro- cess. Our suggested approach yields
      state of the art performance on a standard data set, ArbTE, achieving an
      accuracy of 76.2 % compared to state of the art of 69.3 %.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>almarwani-diab:2017:W17-13</bibkey>
  </paper>
  <paper id='1400'>
    <title>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</title>
    <editor>Tomaž Erjavec</editor>
    <editor>Jakub Piskorski</editor>
    <editor>Lidia Pivovarova</editor>
    <editor>Jan Šnajder</editor>
    <editor>Josef Steinberger</editor>
    <editor>Roman Yangarber</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-14</url>
    <doi>10.18653/v1/W17-14</doi>
    <bibtype>book</bibtype>
    <bibkey>BSNLP:2017</bibkey>
  </paper>
  <paper id='1401'>
    <title>Toward Pan-Slavic NLP: Some Experiments with Language Adaptation</title>
    <author>
      <first>Serge</first>
      <last>Sharoff</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–2</pages>
    <url>http://www.aclweb.org/anthology/W17-1401</url>
    <doi>10.18653/v1/W17-1401</doi>
    <abstract>
      There is great variation in the amount of NLP resources available for
      Slavonic languages. For example, the Universal Dependency treebank (Nivre
      et al., 2016) has about 2 MW of training resources for Czech, more than 1
      MW for Russian, while only 950 words for Ukrainian and nothing for
      Belorussian, Bosnian or Macedonian. Similarly, the Autodesk Machine
      Translation dataset only covers three Slavonic languages (Czech, Polish
      and Russian). In this talk I will discuss a general approach, which can be
      called Language Adaptation, similarly to Domain Adaptation. In this
      approach, a model for a particular language processing task is built by
      lexical transfer of cognate words and by learning a new feature
      representation for a lesser-resourced (recipient) language starting from a
      better-resourced (donor) language. More specifically, I will demonstrate
      how language adaptation works in such training scenarios as Translation
      Quality Estimation, Part-of-Speech tagging and Named Entity Recognition.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sharoff:2017:BSNLP</bibkey>
  </paper>
  <paper id='1402'>
    <title>Clustering of Russian Adjective-Noun Constructions using Word Embeddings</title>
    <author>
      <first>Andrey</first>
      <last>Kutuzov</last>
    </author>
    <author>
      <first>Elizaveta</first>
      <last>Kuzmenko</last>
    </author>
    <author>
      <first>Lidia</first>
      <last>Pivovarova</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>3–13</pages>
    <url>http://www.aclweb.org/anthology/W17-1402</url>
    <doi>10.18653/v1/W17-1402</doi>
    <abstract>
      This paper presents a method of automatic construction extraction from a
      large corpus of Russian. The term `construction' here means a multi-word
      expression in which a variable can be replaced with another word from the
      same semantic class, for example, `a glass of [water/juice/milk]'. We deal
      with constructions that consist of a noun and its adjective modifier. We
      propose a method of grouping such constructions into semantic classes via
      2-step clustering of word vectors in distributional models. We compare it
      with other clustering techniques and evaluate it against A Russian-English
      Collocational Dictionary of the Human Body that contains manually
      annotated groups of constructions with nouns meaning human body parts. The
      best performing method is used to cluster all adjective-noun bigrams in
      the Russian National Corpus. Results of this procedure are publicly
      available and can be used for building Russian construction dictionary as
      well as to accelerate theoretical studies of constructions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kutuzov-kuzmenko-pivovarova:2017:BSNLP</bibkey>
  </paper>
  <paper id='1403'>
    <title>A Preliminary Study of Croatian Lexical Substitution</title>
    <author>
      <first>Domagoj</first>
      <last>Alagić</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>14–19</pages>
    <url>http://www.aclweb.org/anthology/W17-1403</url>
    <doi>10.18653/v1/W17-1403</doi>
    <abstract>
      Lexical substitution is a task of determining a meaning-preserving
      replacement for a word in context. We report on a preliminary study of
      this task for the Croatian language on a small-scale lexical sample
      dataset, manually annotated using three different annotation schemes. We
      compare the annotations, analyze the inter-annotator agreement, and
      observe a number of interesting language specific details in the obtained
      lexical substitutes. Furthermore, we apply a recently-proposed,
      dependency-based lexical substitution model to our dataset. The model
      achieves a P$@$3 score of 0.35, which indicates the difficulty of the
      task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alagic-vsnajder:2017:BSNLP</bibkey>
  </paper>
  <paper id='1404'>
    <title>Projecting Multiword Expression Resources on a Polish Treebank</title>
    <author>
      <first>Agata</first>
      <last>Savary</last>
    </author>
    <author>
      <first>Jakub</first>
      <last>Waszczuk</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20–26</pages>
    <url>http://www.aclweb.org/anthology/W17-1404</url>
    <doi>10.18653/v1/W17-1404</doi>
    <abstract>
      Multiword expressions (MWEs) are linguistic objects containing two or more
      words and showing idiosyncratic behavior at different levels. Treebanks
      with annotated MWEs enable studies of such properties, as well as training
      and evaluation of MWE-aware parsers. However, few treebanks contain
      full-fledged MWE annotations. We show how this gap can be bridged in
      Polish by projecting 3 MWE resources on a constituency treebank.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>savary-waszczuk:2017:BSNLP</bibkey>
  </paper>
  <paper id='1405'>
    <title>Lexicon Induction for Spoken Rusyn – Challenges and Results</title>
    <author>
      <first>Achim</first>
      <last>Rabus</last>
    </author>
    <author>
      <first>Yves</first>
      <last>Scherrer</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–32</pages>
    <url>http://www.aclweb.org/anthology/W17-1405</url>
    <doi>10.18653/v1/W17-1405</doi>
    <abstract>
      This paper reports on challenges and results in developing NLP resources
      for spoken Rusyn. Being a Slavic minority language, Rusyn does not have
      any resources to make use of. We propose to build a morphosyntactic
      dictionary for Rusyn, combining existing resources from the etymologically
      close Slavic languages Russian, Ukrainian, Slovak, and Polish. We adapt
      these resources to Rusyn by using vowel-sensitive Levenshtein distance,
      hand-written language-specific transformation rules, and combinations of
      the two. Compared to an exact match baseline, we increase the coverage of
      the resulting morphological dictionary by up to 77.4% relative (42.9%
      absolute), which results in a tagging recall increased by 11.6% relative
      (9.1% absolute). Our research confirms and expands the results of previous
      studies showing the efficiency of using NLP resources from neighboring
      languages for low-resourced languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rabus-scherrer:2017:BSNLP</bibkey>
  </paper>
  <paper id='1406'>
    <title>The Universal Dependencies Treebank for Slovenian</title>
    <author>
      <first>Kaja</first>
      <last>Dobrovoljc</last>
    </author>
    <author>
      <first>Tomaž</first>
      <last>Erjavec</last>
    </author>
    <author>
      <first>Simon</first>
      <last>Krek</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33–38</pages>
    <url>http://www.aclweb.org/anthology/W17-1406</url>
    <doi>10.18653/v1/W17-1406</doi>
    <abstract>
      This paper introduces the Universal Dependencies Treebank for Slovenian.
      We overview the existing dependency treebanks for Slovenian and then
      detail the conversion of the ssj200k treebank to the framework of
      Universal Dependencies version 2. We explain the mapping of part-of-speech
      categories, morphosyntactic features, and the dependency relations,
      focusing on the more problematic language-specific issues. We conclude
      with a quantitative overview of the treebank and directions for further
      work.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dobrovoljc-erjavec-krek:2017:BSNLP</bibkey>
  </paper>
  <paper id='1407'>
    <title>
      Universal Dependencies for Serbian in Comparison with Croatian and Other
      Slavic Languages
    </title>
    <author>
      <first>Tanja</first>
      <last>Samardžić</last>
    </author>
    <author>
      <first>Mirjana</first>
      <last>Starović</last>
    </author>
    <author>
      <first>Željko</first>
      <last>Agić</last>
    </author>
    <author>
      <first>Nikola</first>
      <last>Ljubešić</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–44</pages>
    <url>http://www.aclweb.org/anthology/W17-1407</url>
    <doi>10.18653/v1/W17-1407</doi>
    <abstract>
      The paper documents the procedure of building a new Universal Dependencies
      (UDv2) treebank for Serbian starting from an existing Croatian UDv1
      treebank and taking into account the other Slavic UD annotation
      guidelines. We describe the automatic and manual annotation procedures,
      discuss the annotation of Slavic-specific categories (case governing
      quantifiers, reflexive pronouns, question particles) and propose an
      approach to handling deverbal nouns in Slavic languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>samardvzic-EtAl:2017:BSNLP</bibkey>
  </paper>
  <paper id='1408'>
    <title>Spelling Correction for Morphologically Rich Language: a Case Study of Russian</title>
    <author>
      <first>Alexey</first>
      <last>Sorokin</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45–53</pages>
    <url>http://www.aclweb.org/anthology/W17-1408</url>
    <doi>10.18653/v1/W17-1408</doi>
    <abstract>
      We present an algorithm for automatic correction of spelling errors on the
      sentence level, which uses noisy channel model and feature-based reranking
      of hypotheses. Our system is designed for Russian and clearly outperforms
      the winner of SpellRuEval-2016 competition. We show that language model
      size has the greatest influence on spelling correction quality. We also
      experiment with different types of features and show that morphological
      and semantic information also improves the accuracy of spellchecking.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sorokin:2017:BSNLP</bibkey>
  </paper>
  <paper id='1409'>
    <title>
      Debunking Sentiment Lexicons: A Case of Domain-Specific Sentiment
      Classification for Croatian
    </title>
    <author>
      <first>Paula</first>
      <last>Gombar</last>
    </author>
    <author>
      <first>Zoran</first>
      <last>Medić</last>
    </author>
    <author>
      <first>Domagoj</first>
      <last>Alagić</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54–59</pages>
    <url>http://www.aclweb.org/anthology/W17-1409</url>
    <doi>10.18653/v1/W17-1409</doi>
    <abstract>
      Sentiment lexicons are widely used as an intuitive and inexpensive way of
      tackling sentiment classification, often within a simple lexicon
      word-counting approach or as part of a supervised model. However, it is an
      open question whether these approaches can compete with supervised models
      that use only word-representation features. We address this question in
      the context of domain-specific sentiment classification for Croatian. We
      experiment with the graph-based acquisition of sentiment lexicons, analyze
      their quality, and investigate how effectively they can be used in
      sentiment classification. Our results indicate that, even with as few as
      500 labeled instances, a supervised model substantially outperforms a
      word-counting model. We also observe that adding lexicon-based features
      does not significantly improve supervised sentiment classification.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gombar-EtAl:2017:BSNLP</bibkey>
  </paper>
  <paper id='1410'>
    <title>
      Adapting a State-of-the-Art Tagger for South Slavic Languages to
      Non-Standard Text
    </title>
    <author>
      <first>Nikola</first>
      <last>Ljubešić</last>
    </author>
    <author>
      <first>Tomaž</first>
      <last>Erjavec</last>
    </author>
    <author>
      <first>Darja</first>
      <last>Fišer</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60–68</pages>
    <url>http://www.aclweb.org/anthology/W17-1410</url>
    <doi>10.18653/v1/W17-1410</doi>
    <abstract>
      In this paper we present the adaptations of a state-of-the-art tagger for
      South Slavic languages to non-standard texts on the example of the Slovene
      language. We investigate the impact of introducing in-domain training data
      as well as additional supervision through external resources or tools like
      word clusters and word normalization. We remove more than half of the
      error of the standard tagger when applied to non-standard texts by
      training it on a combination of standard and non-standard training data,
      while enriching the data representation with external resources removes
      additional 11 percent of the error. The final configuration achieves
      tagging accuracy of 87.41% on the full morphosyntactic description, which
      is, nevertheless, still quite far from the accuracy of 94.27% achieved on
      standard text.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ljubevsic-erjavec-fivser:2017:BSNLP</bibkey>
  </paper>
  <paper id='1411'>
    <title>Comparison of Short-Text Sentiment Analysis Methods for Croatian</title>
    <author>
      <first>Leon</first>
      <last>Rotim</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69–75</pages>
    <url>http://www.aclweb.org/anthology/W17-1411</url>
    <doi>10.18653/v1/W17-1411</doi>
    <abstract>
      We focus on the task of supervised sentiment classification of short and
      informal texts in Croatian, using two simple yet effective methods: word
      embeddings and string kernels. We investigate whether word embeddings
      offer any advantage over corpus- and preprocessing-free string kernels,
      and how these compare to bag-of-words baselines. We conduct a comparison
      on three different datasets, using different preprocessing methods and
      kernel functions. Results show that, on two out of three datasets, word
      embeddings outperform string kernels, which in turn outperform word and
      n-gram bag-of-words baselines.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rotim-vsnajder:2017:BSNLP</bibkey>
  </paper>
  <paper id='1412'>
    <title>
      The First Cross-Lingual Challenge on Recognition, Normalization, and
      Matching of Named Entities in Slavic Languages
    </title>
    <author>
      <first>Jakub</first>
      <last>Piskorski</last>
    </author>
    <author>
      <first>Lidia</first>
      <last>Pivovarova</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <author>
      <first>Josef</first>
      <last>Steinberger</last>
    </author>
    <author>
      <first>Roman</first>
      <last>Yangarber</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–85</pages>
    <url>http://www.aclweb.org/anthology/W17-1412</url>
    <doi>10.18653/v1/W17-1412</doi>
    <abstract>
      This paper describes the outcomes of the first challenge on multilingual
      named entity recognition that aimed at recognizing mentions of named
      entities in web documents in Slavic languages, their
      normalization/lemmatization, and cross-language matching. It was organised
      in the context of the 6th Balto-Slavic Natural Language Processing
      Workshop, co-located with the EACL 2017 conference. Although eleven teams
      signed up for the evaluation, due to the complexity of the task(s) and
      short time available for elaborating a solution, only two teams submitted
      results on time. The reported evaluation figures reflect the relatively
      higher level of complexity of named entity-related tasks in the context of
      processing texts in Slavic languages. Since the duration of the challenge
      goes beyond the date of the publication of this paper and updated picture
      of the participating systems and their corresponding performance can be
      found on the web page of the challenge.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>piskorski-EtAl:2017:BSNLP</bibkey>
  </paper>
  <paper id='1413'>
    <title>Liner2 — a Generic Framework for Named Entity Recognition</title>
    <author>
      <first>Michał</first>
      <last>Marcińczuk</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Kocoń</last>
    </author>
    <author>
      <first>Marcin</first>
      <last>Oleksy</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>86–91</pages>
    <url>http://www.aclweb.org/anthology/W17-1413</url>
    <doi>10.18653/v1/W17-1413</doi>
    <abstract>
      In the paper we present an adaptation of Liner2 framework to solve the
      BSNLP 2017 shared task on multilingual named entity recognition. The tool
      is tuned to recognize and lemmatize named entities for Polish.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marcinczuk-kocon-oleksy:2017:BSNLP</bibkey>
  </paper>
  <paper id='1414'>
    <title>
      Language-Independent Named Entity Analysis Using Parallel Projection and
      Rule-Based Disambiguation
    </title>
    <author>
      <first>James</first>
      <last>Mayfield</last>
    </author>
    <author>
      <first>Paul</first>
      <last>McNamee</last>
    </author>
    <author>
      <first>Cash</first>
      <last>Costello</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>92–96</pages>
    <url>http://www.aclweb.org/anthology/W17-1414</url>
    <doi>10.18653/v1/W17-1414</doi>
    <abstract>
      The 2017 shared task at the Balto-Slavic NLP workshop requires identifying
      coarse-grained named entities in seven languages, identifying each
      entity’s base form, and clustering name mentions across the multilingual
      set of documents. The fact that no training data is provided to systems
      for building supervised classifiers further adds to the complexity. To
      complete the task we first use publicly available parallel texts to
      project named entity recognition capability from English to each
      evaluation language. We ignore entirely the subtask of identifying
      non-inflected forms of names. Finally, we create cross-document entity
      identifiers by clustering named mentions using a procedure-based approach.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mayfield-mcnamee-costello:2017:BSNLP</bibkey>
  </paper>
  <paper id='1415'>
    <title>Comparison of String Similarity Measures for Obscenity Filtering</title>
    <author>
      <first>Ekaterina</first>
      <last>Chernyak</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>97–101</pages>
    <url>http://www.aclweb.org/anthology/W17-1415</url>
    <doi>10.18653/v1/W17-1415</doi>
    <abstract>
      In this paper we address the problem of filtering obscene lexis in Russian
      texts. We use string similarity measures to find words similar or
      identical to words from a stop list and establish both a test collection
      and a baseline for the task. Our experiments show that a novel string
      similarity measure based on the notion of an annotated suffix tree
      outperforms some of the other well known measures.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chernyak:2017:BSNLP</bibkey>
  </paper>
  <paper id='1416'>
    <title>Stylometric Analysis of Parliamentary Speeches: Gender Dimension</title>
    <author>
      <first>Justina</first>
      <last>Mandravickaite</last>
    </author>
    <author>
      <first>Tomas</first>
      <last>Krilavičius</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–107</pages>
    <url>http://www.aclweb.org/anthology/W17-1416</url>
    <doi>10.18653/v1/W17-1416</doi>
    <abstract>
      Relation between gender and language has been studied by many authors,
      however, there is still some uncertainty left regarding gender influence
      on language usage in the professional environment. Often, the studied data
      sets are too small or texts of individual authors are too short in order
      to capture differences of language usage wrt gender successfully. This
      study draws from a larger corpus of speeches transcripts of the Lithuanian
      Parliament (1990-2013) to explore language differences of political
      debates by gender via stylometric analysis. Experimental set up consists
      of stylistic features that indicate lexical style and do not require
      external linguistic tools, namely the most frequent words, in combination
      with unsupervised machine learning algorithms. Results show that gender
      differences in the language use remain in professional environment not
      only in usage of function words, preferred linguistic constructions, but
      in the presented topics as well.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mandravickaite-krilavivcius:2017:BSNLP</bibkey>
  </paper>
  <paper id='1417'>
    <title>Towards Never Ending Language Learning for Morphologically Rich Languages</title>
    <author>
      <first>Kseniya</first>
      <last>Buraya</last>
    </author>
    <author>
      <first>Lidia</first>
      <last>Pivovarova</last>
    </author>
    <author>
      <first>Sergey</first>
      <last>Budkov</last>
    </author>
    <author>
      <first>Andrey</first>
      <last>Filchenkov</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>108–118</pages>
    <url>http://www.aclweb.org/anthology/W17-1417</url>
    <doi>10.18653/v1/W17-1417</doi>
    <abstract>
      This work deals with ontology learning from unstructured Russian text. We
      implement one of components Never Ending Language Learner and introduce
      the algorithm extensions aimed to gather specificity of morphologicaly
      rich free-word-order language. We demonstrate that this method may be
      successfully applied to Russian data. In addition we perform several
      additional experiments comparing different settings of the training
      process. We demonstrate that utilizing of morphological features
      significantly improves the system precision while using of seed patterns
      helps to improve the coverage.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>buraya-EtAl:2017:BSNLP</bibkey>
  </paper>
  <paper id='1418'>
    <title>
      Gender Profiling for Slovene Twitter communication: the Influence of
      Gender Marking, Content and Style
    </title>
    <author>
      <first>Ben</first>
      <last>Verhoeven</last>
    </author>
    <author>
      <first>Iza</first>
      <last>Škrjanec</last>
    </author>
    <author>
      <first>Senja</first>
      <last>Pollak</last>
    </author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119–125</pages>
    <url>http://www.aclweb.org/anthology/W17-1418</url>
    <doi>10.18653/v1/W17-1418</doi>
    <abstract>
      We present results of the first gender classification experiments on
      Slovene text to our knowledge. Inspired by the TwiSty corpus and
      experiments (Verhoeven et al., 2016), we employed the Janes corpus
      (Erjavec et al., 2016) and its gender annotations to perform gender
      classification experiments on Twitter text comparing a token-based and a
      lemma-based approach. We find that the token-based approach (92.6%
      accuracy), containing gender markings related to the author, outperforms
      the lemma-based approach by about 5%. Especially in the lemmatized
      version, we also observe stylistic and content-based differences in
      writing between men (e.g. more profane language, numerals and beer
      mentions) and women (e.g. more pronouns, emoticons and character
      flooding). Many of our findings corroborate previous research on other
      languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>verhoeven-vskrjanec-pollak:2017:BSNLP</bibkey>
  </paper>
  <paper id='1500'>
    <title>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </title>
    <editor>Maciej Ogrodniczuk</editor>
    <editor>Vincent Ng</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-15</url>
    <doi>10.18653/v1/W17-15</doi>
    <bibtype>book</bibtype>
    <bibkey>CORBON:2017</bibkey>
  </paper>
  <paper id='1501'>
    <title>Use Generalized Representations, But Do Not Forget Surface Features</title>
    <author>
      <first>Nafise Sadat</first>
      <last>Moosavi</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Strube</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–7</pages>
    <url>http://www.aclweb.org/anthology/W17-1501</url>
    <doi>10.18653/v1/W17-1501</doi>
    <abstract>
      Only a year ago, all state-of-the-art coreference resolvers were using an
      extensive amount of surface features. Recently, there was a paradigm shift
      towards using word embeddings and deep neural networks, where the use of
      surface features is very limited. In this paper, we show that a simple SVM
      model with surface features outperforms more complex neural models for
      detecting anaphoric mentions. Our analysis suggests that using generalized
      representations and surface features have different strength that should
      be both taken into account for improving coreference resolution.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>moosavi-strube:2017:CORBON</bibkey>
  </paper>
  <paper id='1502'>
    <title>Enriching Basque Coreference Resolution System using Semantic Knowledge sources</title>
    <author>
      <first>Ander</first>
      <last>Soraluze</last>
    </author>
    <author>
      <first>Olatz</first>
      <last>Arregi</last>
    </author>
    <author>
      <first>Xabier</first>
      <last>Arregi</last>
    </author>
    <author>
      <first>Arantza</first>
      <last>Díaz de Ilarraza</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–16</pages>
    <url>http://www.aclweb.org/anthology/W17-1502</url>
    <doi>10.18653/v1/W17-1502</doi>
    <abstract>
      In this paper we present a Basque coreference resolution system enriched
      with semantic knowledge. An error analysis carried out revealed the
      deficiencies that the system had in resolving coreference cases in which
      semantic or world knowledge is needed. We attempt to improve the
      deficiencies using two semantic knowledge sources, specifically Wikipedia
      and WordNet.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>soraluze-EtAl:2017:CORBON</bibkey>
  </paper>
  <paper id='1503'>
    <title>Improving Polish Mention Detection with Valency Dictionary</title>
    <author>
      <first>Maciej</first>
      <last>Ogrodniczuk</last>
    </author>
    <author>
      <first>Bartłomiej</first>
      <last>Nitoń</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–23</pages>
    <url>http://www.aclweb.org/anthology/W17-1503</url>
    <doi>10.18653/v1/W17-1503</doi>
    <abstract>
      This paper presents results of an experiment integrating information from
      valency dictionary of Polish into a mention detection system. Two types of
      information is acquired: positions of syntactic schemata for nominal and
      verbal constructs and secondary prepositions present in schemata. The
      syntactic schemata are used to prevent (for verbal realizations) or
      encourage (for nominal groups) constructing mentions from phrases filling
      multiple schema positions, the secondary prepositions – to filter out
      artificial mentions created from their nominal components. Mention
      detection is evaluated against the manual annotation of the Polish
      Coreference Corpus in two settings: taking into account only mention heads
      or exact borders.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ogrodniczuk-niton:2017:CORBON</bibkey>
  </paper>
  <paper id='1504'>
    <title>A Google-Proof Collection of French Winograd Schemas</title>
    <author>
      <first>Pascal</first>
      <last>Amsili</last>
    </author>
    <author>
      <first>Olga</first>
      <last>Seminck</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–29</pages>
    <url>http://www.aclweb.org/anthology/W17-1504</url>
    <doi>10.18653/v1/W17-1504</doi>
    <abstract>
      This article presents the first collection of French Winograd Schemas.
      Winograd Schemas form anaphora resolution problems that can only be
      resolved with extensive world knowledge. For this reason the Winograd
      Schema Challenge has been proposed as an alternative to the Turing Test. A
      very important feature of Winograd Schemas is that it should be impossible
      to resolve them with statistical information about word co-occurrences:
      they should be Google-proof. We propose a measure of Google-proofness
      based on Mutual Information, and demonstrate the method on our collection
      of French Winograd Schemas.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>amsili-seminck:2017:CORBON</bibkey>
  </paper>
  <paper id='1505'>
    <title>Using Coreference Links to Improve Spanish-to-English Machine Translation</title>
    <author>
      <first>Lesly</first>
      <last>Miculicich Werlen</last>
    </author>
    <author>
      <first>Andrei</first>
      <last>Popescu-Belis</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>30–40</pages>
    <url>http://www.aclweb.org/anthology/W17-1505</url>
    <doi>10.18653/v1/W17-1505</doi>
    <abstract>
      In this paper, we present a proof-of-concept implementation of a
      coreference-aware decoder for document-level machine translation. We
      consider that better translations should have coreference links that are
      closer to those in the source text, and implement this criterion in two
      ways. First, we define a similarity measure between source and target
      coreference structures, by projecting the target ones onto the source and
      reusing existing coreference metrics. Based on this similarity measure, we
      re-rank the translation hypotheses of a baseline system for each sentence.
      Alternatively, to address the lack of diversity of mentions in the MT
      hypotheses, we focus on mention pairs and integrate their coreference
      scores with MT ones, resulting in post-editing decisions for mentions. The
      experimental results for Spanish to English MT on the AnCora-ES corpus
      show that the second approach yields a substantial increase in the
      accuracy of pronoun translation, with BLEU scores remaining constant.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>miculicichwerlen-popescubelis:2017:CORBON</bibkey>
  </paper>
  <paper id='1506'>
    <title>
      Multi-source annotation projection of coreference chains: assessing
      strategies and testing opportunities
    </title>
    <author>
      <first>Yulia</first>
      <last>Grishina</last>
    </author>
    <author>
      <first>Manfred</first>
      <last>Stede</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–50</pages>
    <url>http://www.aclweb.org/anthology/W17-1506</url>
    <doi>10.18653/v1/W17-1506</doi>
    <abstract>
      In this paper, we examine the possibility of using annotation projection
      from multiple sources for automatically obtaining coreference annotations
      in the target language. We implement a multi-source annotation projection
      algorithm and apply it on an English-German-Russian parallel corpus in
      order to transfer coreference chains from two sources to the target side.
      Operating in two settings – a low-resource and a more
      linguistically-informed one – we show that automatic coreference transfer
      could benefit from combining information from multiple languages, and
      assess the quality of both the extraction and the linking of target
      coreference mentions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grishina-stede:2017:CORBON</bibkey>
  </paper>
  <paper id='1507'>
    <title>CORBON 2017 Shared Task: Projection-Based Coreference Resolution</title>
    <author>
      <first>Yulia</first>
      <last>Grishina</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>51–55</pages>
    <url>http://www.aclweb.org/anthology/W17-1507</url>
    <doi>10.18653/v1/W17-1507</doi>
    <abstract>
      The CORBON 2017 Shared Task, organised as part of the Coreference
      Resolution Beyond OntoNotes workshop at EACL 2017, presented a new
      challenge for multilingual coreference resolution: we offer a
      projection-based setting in which one is supposed to build a coreference
      resolver for a new language exploiting little or even no knowledge of it,
      with our languages of interest being German and Russian. We additionally
      offer a more traditional setting, targeting the development of a
      multilingual coreference resolver without any restrictions on the
      resources and methods used. In this paper, we describe the task setting
      and provide the results of one participant who successfully completed the
      task, comparing their results to the closely related previous research.
      Analysing the task setting and the results, we discuss the major
      challenges and make suggestions on the future directions of coreference
      evaluation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grishina:2017:CORBON</bibkey>
  </paper>
  <paper id='1508'>
    <title>Projection-based Coreference Resolution Using Deep Syntax</title>
    <author>
      <first>Michal</first>
      <last>Novák</last>
    </author>
    <author>
      <first>Anna</first>
      <last>Nedoluzhko</last>
    </author>
    <author>
      <first>Zdeněk</first>
      <last>Žabokrtský</last>
    </author>
    <booktitle>
      Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes
      (CORBON 2017)
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–64</pages>
    <url>http://www.aclweb.org/anthology/W17-1508</url>
    <doi>10.18653/v1/W17-1508</doi>
    <abstract>
      The paper describes the system for coreference resolution in German and
      Russian, trained exclusively on coreference relations project ed through a
      parallel corpus from English. The resolver operates on the level of deep
      syntax and makes use of multiple specialized models. It achieves 32 and 22
      points in terms of CoNLL score for Russian and German, respectively.
      Analysis of the evaluation results show that the resolver for Russian is
      able to preserve 66% of the English resolver's quality in terms of CoNLL
      score. The system was submitted to the Closed track of the CORBON 2017
      Shared task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>novak-nedoluzhko-vzabokrtsky:2017:CORBON</bibkey>
  </paper>
  <paper id='1600'>
    <title>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
    <editor>Dirk Hovy</editor>
    <editor>Shannon Spruit</editor>
    <editor>Margaret Mitchell</editor>
    <editor>Emily M. Bender</editor>
    <editor>Michael Strube</editor>
    <editor>Hanna Wallach</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-16</url>
    <doi>10.18653/v1/W17-16</doi>
    <bibtype>book</bibtype>
    <bibkey>EthNLP:2017</bibkey>
  </paper>
  <paper id='1601'>
    <title>Gender as a Variable in Natural-Language Processing: Ethical Considerations</title>
    <author>
      <first>Brian</first>
      <last>Larson</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–11</pages>
    <url>http://www.aclweb.org/anthology/W17-1601</url>
    <doi>10.18653/v1/W17-1601</doi>
    <abstract>
      Researchers and practitioners in natural-language processing (NLP) and
      related fields should attend to ethical principles in study design,
      ascription of categories/variables to study participants, and reporting of
      findings or results. This paper discusses theoretical and ethical
      frameworks for using gender as a variable in NLP studies and proposes four
      guidelines for researchers and practitioners. The principles outlined here
      should guide practitioners, researchers, and peer reviewers, and they may
      be applicable to other social categories, such as race, applied to human
      beings connected to NLP research.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>larson:2017:EthNLP</bibkey>
  </paper>
  <paper id='1602'>
    <title>
      These are not the Stereotypes You are Looking For: Bias and Fairness in
      Authorial Gender Attribution
    </title>
    <author>
      <first>Corina</first>
      <last>Koolen</last>
    </author>
    <author>
      <first>Andreas</first>
      <last>van Cranenburgh</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–22</pages>
    <url>http://www.aclweb.org/anthology/W17-1602</url>
    <doi>10.18653/v1/W17-1602</doi>
    <abstract>
      Stylometric and text categorization results show that author gender can be
      discerned in texts with relatively high accuracy. However, it is difficult
      to explain what gives rise to these results and there are many possible
      confounding factors, such as the domain, genre, and target audience of a
      text. More fundamentally, such classification efforts risk invoking
      stereotyping and essentialism. We explore this issue in two datasets of
      Dutch literary novels, using commonly used descriptive (LIWC, topic
      modeling) and predictive (machine learning) methods. Our results show the
      importance of controlling for variables in the corpus and we argue for
      taking care not to overgeneralize from the results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koolen-vancranenburgh:2017:EthNLP</bibkey>
  </paper>
  <paper id='1603'>
    <title>A Quantitative Study of Data in the NLP community</title>
    <author>
      <first>Margot</first>
      <last>Mieskes</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>23–29</pages>
    <url>http://www.aclweb.org/anthology/W17-1603</url>
    <doi>10.18653/v1/W17-1603</doi>
    <abstract>
      We present results on a quantitative analysis of publications in the NLP
      domain on collecting, publishing and availability of research data. We
      find that a wide range of publications rely on data crawled from the web,
      but few give details on how potentially sensitive data was treated.
      Additionally, we find that while links to repositories of data are given,
      they often do not work even a short time after publication. We put
      together several suggestions on how to improve this situation based on
      publications from the NLP domain, but also other research areas.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mieskes:2017:EthNLP</bibkey>
  </paper>
  <paper id='1604'>
    <title>Ethical by Design: Ethics Best Practices for Natural Language Processing</title>
    <author>
      <first>Jochen L.</first>
      <last>Leidner</last>
    </author>
    <author>
      <first>Vassilis</first>
      <last>Plachouras</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>30–40</pages>
    <url>http://www.aclweb.org/anthology/W17-1604</url>
    <doi>10.18653/v1/W17-1604</doi>
    <abstract>
      Natural language processing (NLP) systems analyze and/or generate human
      language, typically on users’ behalf. One natural and necessary question
      that needs to be addressed in this context, both in research projects and
      in production settings, is the question how ethical the work is, both
      regarding the process and its outcome. Towards this end, we articulate a
      set of issues, propose a set of best practices, notably a process
      featuring an ethics review board, and sketch and how they could be
      meaningfully applied. Our main argument is that ethical outcomes ought to
      be achieved by design, i.e. by following a process aligned by ethical
      values. We also offer some response options for those facing ethics
      issues. While a number of previous works exist that discuss ethical
      issues, in particular around big data and machine learning, to the
      authors’ knowledge this is the first account of NLP and ethics from the
      perspective of a principled process.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>leidner-plachouras:2017:EthNLP</bibkey>
  </paper>
  <paper id='1605'>
    <title>Building Better Open-Source Tools to Support Fairness in Automated Scoring</title>
    <author>
      <first>Nitin</first>
      <last>Madnani</last>
    </author>
    <author>
      <first>Anastassia</first>
      <last>Loukina</last>
    </author>
    <author>
      <first>Alina</first>
      <last>von Davier</last>
    </author>
    <author>
      <first>Jill</first>
      <last>Burstein</last>
    </author>
    <author>
      <first>Aoife</first>
      <last>Cahill</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–52</pages>
    <url>http://www.aclweb.org/anthology/W17-1605</url>
    <doi>10.18653/v1/W17-1605</doi>
    <abstract>
      Automated scoring of written and spoken responses is an NLP application
      that can significantly impact lives especially when deployed as part of
      high-stakes tests such as the GREėxtregistered~ and the
      TOEFLėxtregistered~. Ethical considerations require that automated scoring
      algorithms treat all test- takers fairly. The educational measurement
      community has done significant research on fairness in assessments and
      automated scoring systems must incorporate their recommendations. The best
      way to do that is by making available automated, non-proprietary tools to
      NLP researchers that directly incorporate these recommendations and
      generate the analyses needed to help identify and resolve biases in their
      scoring systems. In this paper, we attempt to provide such a solution.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>madnani-EtAl:2017:EthNLP</bibkey>
  </paper>
  <paper id='1606'>
    <title>Gender and Dialect Bias in YouTube's Automatic Captions</title>
    <author>
      <first>Rachael</first>
      <last>Tatman</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53–59</pages>
    <url>http://www.aclweb.org/anthology/W17-1606</url>
    <doi>10.18653/v1/W17-1606</doi>
    <abstract>
      This project evaluates the accuracy of YouTube's automatically-generated
      captions across two genders and five dialect groups. Speakers' dialect and
      gender was controlled for by using videos uploaded as part of the “accent
      tag challenge", where speakers explicitly identify their language
      background. The results show robust differences in accuracy across both
      gender and dialect, with lower accuracy for 1) women and 2) speakers from
      Scotland. This finding builds on earlier research finding that speaker's
      sociolinguistic identity may negatively impact their ability to use
      automatic speech recognition, and demonstrates the need for
      sociolinguistically-stratified validation of systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tatman:2017:EthNLP</bibkey>
  </paper>
  <paper id='1607'>
    <title>
      Integrating the Management of Personal Data Protection and Open Science
      with Research Ethics
    </title>
    <author>
      <first>Dave</first>
      <last>Lewis</last>
    </author>
    <author>
      <first>Joss</first>
      <last>Moorkens</last>
    </author>
    <author>
      <first>Kaniz</first>
      <last>Fatema</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60–65</pages>
    <url>http://www.aclweb.org/anthology/W17-1607</url>
    <doi>10.18653/v1/W17-1607</doi>
    <abstract>
      We examine the impact of the EU General Data Protection Regulation and the
      push from research funders to provide open access research data on the
      current practices in Language Technology Research. We analyse the
      challenges that arise and the opportunities to address many of them
      through the use of existing open data practices. We discuss the impact of
      this also on current practice in research ethics.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lewis-moorkens-fatema:2017:EthNLP</bibkey>
  </paper>
  <paper id='1608'>
    <title>Ethical Considerations in NLP Shared Tasks</title>
    <author>
      <first>Carla</first>
      <last>Parra Escartín</last>
    </author>
    <author>
      <first>Wessel</first>
      <last>Reijers</last>
    </author>
    <author>
      <first>Teresa</first>
      <last>Lynn</last>
    </author>
    <author>
      <first>Joss</first>
      <last>Moorkens</last>
    </author>
    <author>
      <first>Andy</first>
      <last>Way</last>
    </author>
    <author>
      <first>Chao-Hong</first>
      <last>Liu</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–73</pages>
    <url>http://www.aclweb.org/anthology/W17-1608</url>
    <doi>10.18653/v1/W17-1608</doi>
    <abstract>
      Shared tasks are increasingly common in our field, and new challenges are
      suggested at almost every conference and workshop. However, as this has
      become an established way of pushing research forward, it is important to
      discuss how we researchers organise and participate in shared tasks, and
      make that information available to the community to allow further research
      improvements. In this paper, we present a number of ethical issues along
      with other areas of concern that are related to the competitive nature of
      shared tasks. As such issues could potentially impact on research ethics
      in the Natural Language Processing community, we also propose the
      development of a framework for the organisation of and participation in
      shared tasks that can help mitigate against these issues arising.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>parraescartin-EtAl:2017:EthNLP</bibkey>
  </paper>
  <paper id='1609'>
    <title>Social Bias in Elicited Natural Language Inferences</title>
    <author>
      <first>Rachel</first>
      <last>Rudinger</last>
    </author>
    <author>
      <first>Chandler</first>
      <last>May</last>
    </author>
    <author>
      <first>Benjamin</first>
      <last>Van Durme</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74–79</pages>
    <url>http://www.aclweb.org/anthology/W17-1609</url>
    <doi>10.18653/v1/W17-1609</doi>
    <abstract>
      We analyze the Stanford Natural Language Inference (SNLI) corpus in an
      investigation of bias and stereotyping in NLP data. The SNLI
      human-elicitation protocol makes it prone to amplifying bias and
      stereotypical associations, which we demonstrate statistically (using
      pointwise mutual information) and with qualitative examples.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rudinger-may-vandurme:2017:EthNLP</bibkey>
  </paper>
  <paper id='1610'>
    <title>A Short Review of Ethical Challenges in Clinical Natural Language Processing</title>
    <author>
      <first>Simon</first>
      <last>Suster</last>
    </author>
    <author>
      <first>Stephan</first>
      <last>Tulkens</last>
    </author>
    <author>
      <first>Walter</first>
      <last>Daelemans</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>80–87</pages>
    <url>http://www.aclweb.org/anthology/W17-1610</url>
    <doi>10.18653/v1/W17-1610</doi>
    <abstract>
      Clinical NLP has an immense potential in contributing to how clinical
      practice will be revolutionized by the advent of large scale processing of
      clinical records. However, this potential has remained largely untapped
      due to slow progress primarily caused by strict data access policies for
      researchers. In this paper, we discuss the concern for privacy and the
      measures it entails. We also suggest sources of less sensitive data.
      Finally, we draw attention to biases that can compromise the validity of
      empirical research and lead to socially harmful applications.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>suster-tulkens-daelemans:2017:EthNLP</bibkey>
  </paper>
  <paper id='1611'>
    <title>Goal-Oriented Design for Ethical Machine Learning and NLP</title>
    <author>
      <first>Tyler</first>
      <last>Schnoebelen</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>88–93</pages>
    <url>http://www.aclweb.org/anthology/W17-1611</url>
    <doi>10.18653/v1/W17-1611</doi>
    <abstract>
      The argument made in this paper is that to act ethically in machine
      learning and NLP requires focusing on goals. NLP projects are often
      classificatory systems that deal with human subjects, which means that
      goals from people affected by the systems should be included. The paper
      takes as its core example a model that detects criminality, showing the
      problems of training data, categories, and outcomes. The paper is oriented
      to the kinds of critiques on power and the reproduction of inequality that
      are found in social theory, but it also includes concrete suggestions on
      how to put goal-oriented design into practice.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schnoebelen:2017:EthNLP</bibkey>
  </paper>
  <paper id='1612'>
    <title>Ethical Research Protocols for Social Media Health Research</title>
    <author>
      <first>Adrian</first>
      <last>Benton</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Coppersmith</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Dredze</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>94–102</pages>
    <url>http://www.aclweb.org/anthology/W17-1612</url>
    <doi>10.18653/v1/W17-1612</doi>
    <abstract>
      Social media have transformed data-driven research in political science,
      the social sciences, health, and medicine. Since health research often
      touches on sensitive topics that relate to ethics of treatment and patient
      privacy, similar ethical considerations should be acknowledged when using
      social media data in health research. While much has been said regarding
      the ethical considerations of social media research, health research leads
      to an additional set of concerns. We provide practical suggestions in the
      form of guidelines for researchers working with social media data in
      health research. These guidelines can inform an IRB proposal for
      researchers new to social media health research.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>benton-coppersmith-dredze:2017:EthNLP</bibkey>
  </paper>
  <paper id='1613'>
    <title>Say the Right Thing Right: Ethics Issues in Natural Language Generation Systems</title>
    <author>
      <first>Charese</first>
      <last>Smiley</last>
    </author>
    <author>
      <first>Frank</first>
      <last>Schilder</last>
    </author>
    <author>
      <first>Vassilis</first>
      <last>Plachouras</last>
    </author>
    <author>
      <first>Jochen L.</first>
      <last>Leidner</last>
    </author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>103–108</pages>
    <url>http://www.aclweb.org/anthology/W17-1613</url>
    <doi>10.18653/v1/W17-1613</doi>
    <abstract>
      We discuss the ethical implications of Natural Language Generation
      systems. We use one particular system as a case study to identify and
      classify issues, and we provide an ethics checklist, in the hope that
      future system designers may benefit from conducting their own ethics
      reviews based on our checklist.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>smiley-EtAl:2017:EthNLP</bibkey>
  </paper>
  <paper id='1700'>
    <title>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</title>
    <editor>Stella Markantonatou</editor>
    <editor>Carlos Ramisch</editor>
    <editor>Agata Savary</editor>
    <editor>Veronika Vincze</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-17</url>
    <doi>10.18653/v1/W17-17</doi>
    <bibtype>book</bibtype>
    <bibkey>MWE2017:2017</bibkey>
  </paper>
  <paper id='1701'>
    <title>ParaDi: Dictionary of Paraphrases of Czech Complex Predicates with Light Verbs</title>
    <author>
      <first>Petra</first>
      <last>Barancikova</last>
    </author>
    <author>
      <first>Václava</first>
      <last>Kettnerová</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-1701</url>
    <doi>10.18653/v1/W17-1701</doi>
    <abstract>
      We present a new freely available dictionary of paraphrases of Czech
      complex predicates with light verbs, ParaDi. Candidates for single
      predicative paraphrases of selected complex predicates have been extracted
      automatically from large monolingual data using word2vec. They have been
      manually verified and further refined. We demonstrate one of many possible
      applications of ParaDi in an experiment with improving machine translation
      quality.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>barancikova-kettnerova:2017:MWE2017</bibkey>
  </paper>
  <paper id='1702'>
    <title>Multi-word Entity Classification in a Highly Multilingual Environment</title>
    <author>
      <first>Sophie</first>
      <last>Chesney</last>
    </author>
    <author>
      <first>Guillaume</first>
      <last>Jacquet</last>
    </author>
    <author>
      <first>Ralf</first>
      <last>Steinberger</last>
    </author>
    <author>
      <first>Jakub</first>
      <last>Piskorski</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–20</pages>
    <url>http://www.aclweb.org/anthology/W17-1702</url>
    <doi>10.18653/v1/W17-1702</doi>
    <abstract>
      This paper describes an approach for the classification of millions of
      existing multi-word entities (MWEntities), such as organisation or event
      names, into thirteen category types, based only on the tokens they
      contain. In order to classify our very large in-house collection of
      multilingual MWEntities into an application-oriented set of entity
      categories, we trained and tested distantly-supervised classifiers in 43
      languages based on MWEntities extracted from BabelNet. The best-performing
      classifier was the multi-class SVM using a TF.IDF-weighted data
      representation. Interestingly, one unique classifier trained on a mix of
      all languages consistently performed better than classifiers trained for
      individual languages, reaching an averaged F1-value of 88.8%. In this
      paper, we present the training and test data, including a human evaluation
      of its accuracy, describe the methods used to train the classifiers, and
      discuss the results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chesney-EtAl:2017:MWE2017</bibkey>
  </paper>
  <paper id='1703'>
    <title>Using bilingual word-embeddings for multilingual collocation extraction</title>
    <author>
      <first>Marcos</first>
      <last>Garcia</last>
    </author>
    <author>
      <first>Marcos</first>
      <last>García-Salido</last>
    </author>
    <author>
      <first>Margarita</first>
      <last>Alonso-Ramos</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–30</pages>
    <url>http://www.aclweb.org/anthology/W17-1703</url>
    <doi>10.18653/v1/W17-1703</doi>
    <abstract>
      This paper presents a new strategy for multilingual collocation extraction
      which takes advantage of parallel corpora to learn bilingual
      word-embeddings. Monolingual collocation candidates are retrieved using
      Universal Dependencies, while the distributional models are then applied
      to search for equivalents of the elements of each collocation in the
      target languages. The proposed method extracts not only collocation
      equivalents with direct translation between languages, but also other
      cases where the collocations in the two languages are not literal
      translations of each other. Several experiments -evaluating collocations
      with three syntactic patterns- in English, Spanish, and Portuguese show
      that our approach can effectively extract large pairs of bilingual
      equivalents with an average precision of about 90%. Moreover, preliminary
      results on comparable corpora suggest that the distributional models can
      be applied for identifying new bilingual collocations in different
      domains.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>garcia-garciasalido-alonsoramos:2017:MWE2017</bibkey>
  </paper>
  <paper id='1704'>
    <title>
      The PARSEME Shared Task on Automatic Identification of Verbal Multiword
      Expressions
    </title>
    <author>
      <first>Agata</first>
      <last>Savary</last>
    </author>
    <author>
      <first>Carlos</first>
      <last>Ramisch</last>
    </author>
    <author>
      <first>Silvio</first>
      <last>Cordeiro</last>
    </author>
    <author>
      <first>Federico</first>
      <last>Sangati</last>
    </author>
    <author>
      <first>Veronika</first>
      <last>Vincze</last>
    </author>
    <author>
      <first>Behrang</first>
      <last>QasemiZadeh</last>
    </author>
    <author>
      <first>Marie</first>
      <last>Candito</last>
    </author>
    <author>
      <first>Fabienne</first>
      <last>Cap</last>
    </author>
    <author>
      <first>Voula</first>
      <last>Giouli</last>
    </author>
    <author>
      <first>Ivelina</first>
      <last>Stoyanova</last>
    </author>
    <author>
      <first>Antoine</first>
      <last>Doucet</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–47</pages>
    <url>http://www.aclweb.org/anthology/W17-1704</url>
    <doi>10.18653/v1/W17-1704</doi>
    <abstract>
      Multiword expressions (MWEs) are known as a "pain in the neck" for NLP due
      to their idiosyncratic behaviour. While some categories of MWEs have been
      addressed by many studies, verbal MWEs (VMWEs), such as to take a
      decision, to break one’s heart or to turn off, have been rarely modelled.
      This is notably due to their syntactic variability, which hinders treating
      them as "words with spaces". We describe an initiative meant to bring
      about substantial progress in understanding, modelling and process- ing
      VMWEs. It is a joint effort, carried out within a European research
      network, to elaborate universal terminologies and annotation guidelines
      for 18 languages. Its main outcome is a multilingual 5-million- word
      annotated corpus which underlies a shared task on automatic identification
      of VMWEs. This paper presents the corpus annotation methodology and
      outcome, the shared task organisation and the results of the participating
      systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>savary-EtAl:2017:MWE2017</bibkey>
  </paper>
  <paper id='1705'>
    <title>
      USzeged: Identifying Verbal Multiword Expressions with POS Tagging and
      Parsing Techniques
    </title>
    <author>
      <first>Katalin Ilona</first>
      <last>Simkó</last>
    </author>
    <author>
      <first>Viktória</first>
      <last>Kovács</last>
    </author>
    <author>
      <first>Veronika</first>
      <last>Vincze</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–53</pages>
    <url>http://www.aclweb.org/anthology/W17-1705</url>
    <doi>10.18653/v1/W17-1705</doi>
    <abstract>
      The paper describes our system submitted for the Workshop on Multiword
      Expressions’ shared task on automatic identification of verbal multiword
      expressions. It uses POS tagging and dependency parsing to identify
      single- and multi-token verbal MWEs in text. Our system is language
      independent and competed on nine of the eighteen languages. Our paper
      describes how our system works and gives its error analysis for the
      languages it was submitted for.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>simko-kovacs-vincze:2017:MWE2017</bibkey>
  </paper>
  <paper id='1706'>
    <title>Parsing and MWE Detection: Fips at the PARSEME Shared Task</title>
    <author>
      <first>Luka</first>
      <last>Nerima</last>
    </author>
    <author>
      <first>Vasiliki</first>
      <last>Foufi</last>
    </author>
    <author>
      <first>Eric</first>
      <last>Wehrli</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54–59</pages>
    <url>http://www.aclweb.org/anthology/W17-1706</url>
    <doi>10.18653/v1/W17-1706</doi>
    <abstract>
      Identifying multiword expressions (MWEs) in a sentence in order to ensure
      their proper processing in subsequent applications, like machine
      translation, and performing the syntactic analysis of the sentence are
      interrelated processes. In our approach, priority is given to parsing
      alternatives involving collocations, and hence collocational information
      helps the parser through the maze of alternatives, with the aim to lead to
      substantial improvements in the performance of both tasks (collocation
      identification and parsing), and in that of a subsequent task (machine
      translation). In this paper, we are going to present our system and the
      procedure that we have followed in order to participate to the open track
      of the PARSEME shared task on automatic identification of verbal multiword
      expressions (VMWEs) in running texts.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nerima-foufi-wehrli:2017:MWE2017</bibkey>
  </paper>
  <paper id='1707'>
    <title>Neural Networks for Multi-Word Expression Detection</title>
    <author>
      <first>Natalia</first>
      <last>Klyueva</last>
    </author>
    <author>
      <first>Antoine</first>
      <last>Doucet</last>
    </author>
    <author>
      <first>Milan</first>
      <last>Straka</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60–65</pages>
    <url>http://www.aclweb.org/anthology/W17-1707</url>
    <doi>10.18653/v1/W17-1707</doi>
    <abstract>
      In this paper we describe the MUMULS system that participated to the 2017
      shared task on automatic identification of verbal multiword expressions
      (VMWEs). The MUMULS system was implemented using a supervised approach
      based on recurrent neural networks using the open source library
      TensorFlow. The model was trained on a data set containing annotated VMWEs
      as well as morphological and syntactic information. The MUMULS system
      performed the identification of VMWEs in 15 languages, it was one of few
      systems that could categorize VMWEs type in nearly all languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>klyueva-doucet-straka:2017:MWE2017</bibkey>
  </paper>
  <paper id='1708'>
    <title>
      Factoring Ambiguity out of the Prediction of Compositionality for German
      Multi-Word Expressions
    </title>
    <author>
      <first>Stefan</first>
      <last>Bott</last>
    </author>
    <author>
      <first>Sabine</first>
      <last>Schulte im Walde</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–72</pages>
    <url>http://www.aclweb.org/anthology/W17-1708</url>
    <doi>10.18653/v1/W17-1708</doi>
    <abstract>
      Ambiguity represents an obstacle for distributional semantic models(DSMs),
      which typically subsume the contexts of all word senses within one vector.
      While individual vector space approaches have been concerned with sense
      discrimination (e.g., Schütze 1998, Erk 2009, Erk and Pado 2010), such
      discrimination has rarely been integrated into DSMs across semantic tasks.
      This paper presents a soft-clustering approach to sense discrimination
      that filters sense-irrelevant features when predicting the degrees of
      compositionality for German noun-noun compounds and German particle verbs.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bott-schulteimwalde:2017:MWE2017</bibkey>
  </paper>
  <paper id='1709'>
    <title>Multiword expressions and lexicalism: the view from LFG</title>
    <author>
      <first>Jamie Y.</first>
      <last>Findlay</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73–79</pages>
    <url>http://www.aclweb.org/anthology/W17-1709</url>
    <doi>10.18653/v1/W17-1709</doi>
    <abstract>
      Multiword expressions (MWEs) pose a problem for lexicalist theories like
      Lexical Functional Grammar (LFG), since they are prima facie
      counterexamples to a strong form of the lexical integrity principle, which
      entails that a lexical item can only be realised as a single,
      syntactically atomic word. In this paper, I demonstrate some of the
      problems facing any strongly lexicalist account of MWEs, and argue that
      the lexical integrity principle must be weakened. I conclude by sketching
      a formalism which integrates a Tree Adjoining Grammar into the LFG
      architecture, taking advantage of this relaxation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>findlay:2017:MWE2017</bibkey>
  </paper>
  <paper id='1710'>
    <title>Understanding Idiomatic Variation</title>
    <author>
      <first>Kristina</first>
      <last>Geeraert</last>
    </author>
    <author>
      <first>R. Harald</first>
      <last>Baayen</last>
    </author>
    <author>
      <first>John</first>
      <last>Newman</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>80–90</pages>
    <url>http://www.aclweb.org/anthology/W17-1710</url>
    <doi>10.18653/v1/W17-1710</doi>
    <abstract>
      This study investigates the processing of idiomatic variants through an
      eye-tracking experiment. Four types of idiom variants were included, in
      addition to the canonical form and the literal meaning. Results suggest
      that modifications to idioms, modulo obvious effects of length
      differences, are not more difficult to process than the canonical forms
      themselves. This fits with recent corpus findings.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>geeraert-baayen-newman:2017:MWE2017</bibkey>
  </paper>
  <paper id='1711'>
    <title>
      Discovering Light Verb Constructions and their Translations from Parallel
      Corpora without Word Alignment
    </title>
    <author>
      <first>Natalie</first>
      <last>Vargas</last>
    </author>
    <author>
      <first>Carlos</first>
      <last>Ramisch</last>
    </author>
    <author>
      <first>Helena</first>
      <last>Caseli</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–96</pages>
    <url>http://www.aclweb.org/anthology/W17-1711</url>
    <doi>10.18653/v1/W17-1711</doi>
    <abstract>
      We propose a method for joint unsupervised discovery of multiword
      expressions (MWEs) and their translations from parallel corpora. First, we
      apply independent monolingual MWE extraction in source and target
      languages simultaneously. Then, we calculate translation probability,
      association score and distributional similarity of co-occurring pairs.
      Finally, we rank all translations of a given MWE using a linear
      combination of these features. Preliminary experiments on light verb
      constructions show promising results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vargas-ramisch-caseli:2017:MWE2017</bibkey>
  </paper>
  <paper id='1712'>
    <title>
      Identification of Multiword Expressions for Latvian and Lithuanian: Hybrid
      Approach
    </title>
    <author>
      <first>Justina</first>
      <last>Mandravickaite</last>
    </author>
    <author>
      <first>Tomas</first>
      <last>Krilavičius</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>97–101</pages>
    <url>http://www.aclweb.org/anthology/W17-1712</url>
    <doi>10.18653/v1/W17-1712</doi>
    <abstract>
      We discuss an experiment on automatic identification of bi-gram multi-word
      expressions in parallel Latvian and Lithuanian corpora. Raw corpora,
      lexical association measures (LAMs) and supervised machine learning (ML)
      are used due to deficit and quality of lexical resources (e.g.,
      POS-tagger, parser) and tools. While combining LAMs with ML is rather
      effective for other languages, it has shown some nice results for
      Lithuanian and Latvian as well. Combining LAMs with ML we have achieved
      92,4% precision and 52,2% recall for Latvian and 95,1% precision and 77,8%
      recall for Lithuanian.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mandravickaite-krilavivcius:2017:MWE2017</bibkey>
  </paper>
  <paper id='1713'>
    <title>
      Show Me Your Variance and I Tell You Who You Are - Deriving Compound
      Compositionality from Word Alignments
    </title>
    <author>
      <first>Fabienne</first>
      <last>Cap</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–107</pages>
    <url>http://www.aclweb.org/anthology/W17-1713</url>
    <doi>10.18653/v1/W17-1713</doi>
    <abstract>
      We use word alignment variance as an indicator for the
      non-compositionality of German and English noun compounds. Our
      work-in-progress results are on their own not competitive with
      state-of-the art approaches, but they show that alignment variance is
      correlated with compositionality and thus worth a closer look in the
      future.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cap:2017:MWE2017</bibkey>
  </paper>
  <paper id='1714'>
    <title>
      Semantic annotation to characterize contextual variation in terminological
      noun compounds: a pilot study
    </title>
    <author>
      <first>Melania</first>
      <last>Cabezas-García</last>
    </author>
    <author>
      <first>Antonio</first>
      <last>San Martín</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>108–113</pages>
    <url>http://www.aclweb.org/anthology/W17-1714</url>
    <doi>10.18653/v1/W17-1714</doi>
    <abstract>
      Noun compounds (NCs) are semantically complex and not fully compositional,
      as is often assumed. This paper presents a pilot study regarding the
      semantic annotation of environmental NCs with a view to accessing their
      semantics and exploring their domain-based contextual variation. Our
      results showed that the semantic annotation of NCs afforded important
      insights into how context impacts their conceptualization.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cabezasgarcia-sanmartin:2017:MWE2017</bibkey>
  </paper>
  <paper id='1715'>
    <title>
      Detection of Verbal Multi-Word Expressions via Conditional Random Fields
      with Syntactic Dependency Features and Semantic Re-Ranking
    </title>
    <author>
      <first>Alfredo</first>
      <last>Maldonado</last>
    </author>
    <author>
      <first>Lifeng</first>
      <last>Han</last>
    </author>
    <author>
      <first>Erwan</first>
      <last>Moreau</last>
    </author>
    <author>
      <first>Ashjan</first>
      <last>Alsulaimani</last>
    </author>
    <author>
      <first>Koel Dutta</first>
      <last>Chowdhury</last>
    </author>
    <author>
      <first>Carl</first>
      <last>Vogel</last>
    </author>
    <author>
      <first>Qun</first>
      <last>Liu</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>114–120</pages>
    <url>http://www.aclweb.org/anthology/W17-1715</url>
    <doi>10.18653/v1/W17-1715</doi>
    <abstract>
      A description of a system for identifying Verbal Multi-Word Expressions
      (VMWEs) in running text is presented. The system mainly exploits universal
      syntactic dependency features through a Conditional Random Fields (CRF)
      sequence model. The system competed in the Closed Track at the PARSEME
      VMWE Shared Task 2017, ranking 2nd place in most languages on full
      VMWE-based evaluation and 1st in three languages on token-based
      evaluation. In addition, this paper presents an option to re-rank the 10
      best CRF-predicted sequences via semantic vectors, boosting its scores
      above other systems in the competition. We also show that all systems in
      the competition would struggle to beat a simple lookup baseline system and
      argue for a more purpose-specific evaluation scheme.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>maldonado-EtAl:2017:MWE2017</bibkey>
  </paper>
  <paper id='1716'>
    <title>
      A data-driven approach to verbal multiword expression detection. PARSEME
      Shared Task system description paper
    </title>
    <author>
      <first>Tiberiu</first>
      <last>Boroş</last>
    </author>
    <author>
      <first>Sonia</first>
      <last>Pipa</last>
    </author>
    <author>
      <first>Verginica</first>
      <last>Barbu Mititelu</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Tufiş</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>121–126</pages>
    <url>http://www.aclweb.org/anthology/W17-1716</url>
    <doi>10.18653/v1/W17-1716</doi>
    <abstract>
      "Multiword expressions" are groups of words acting as a morphologic,
      syntactic and semantic unit in linguistic analysis. Verbal multiword
      expressions represent the subgroup of multiword expressions, namely that
      in which a verb is the syntactic head of the group considered in its
      canonical (or dictionary) form. All multiword expressions are a great
      challenge for natural language processing, but the verbal ones are
      particularly interesting for tasks such as parsing, as the verb is the
      central element in the syntactic organization of a sentence. In this paper
      we introduce our data-driven approach to verbal multiword expressions
      which was objectively validated during the PARSEME shared task on verbal
      multiword expressions identification. We tested our approach on 12
      languages, and we provide detailed information about corpora composition,
      feature selection process, validation procedure and performance on all
      languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>borocs-EtAl:2017:MWE2017</bibkey>
  </paper>
  <paper id='1717'>
    <title>
      The ATILF-LLF System for Parseme Shared Task: a Transition-based Verbal
      Multiword Expression Tagger
    </title>
    <author>
      <first>Hazem</first>
      <last>Al Saied</last>
    </author>
    <author>
      <first>Matthieu</first>
      <last>Constant</last>
    </author>
    <author>
      <first>Marie</first>
      <last>Candito</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>127–132</pages>
    <url>http://www.aclweb.org/anthology/W17-1717</url>
    <doi>10.18653/v1/W17-1717</doi>
    <abstract>
      We describe the ATILF-LLF system built for the MWE 2017 Shared Task on
      automatic identification of verbal multiword expressions. We participated
      in the closed track only, for all the 18 available languages. Our system
      is a robust greedy transition-based system, in which MWE are identified
      through a MERGE transition. The system was meant to accommodate the
      variety of linguistic resources provided for each language, in terms of
      accompanying morphological and syntactic information. Using per-MWE
      Fscore, the system was ranked first for all but two languages (Hungarian
      and Romanian).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alsaied-constant-candito:2017:MWE2017</bibkey>
  </paper>
  <paper id='1718'>
    <title>Investigating the Opacity of Verb-Noun Multiword Expression Usages in Context</title>
    <author>
      <first>Shiva</first>
      <last>Taslimipoor</last>
    </author>
    <author>
      <first>Omid</first>
      <last>Rohanian</last>
    </author>
    <author>
      <first>Ruslan</first>
      <last>Mitkov</last>
    </author>
    <author>
      <first>Afsaneh</first>
      <last>Fazly</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>133–138</pages>
    <url>http://www.aclweb.org/anthology/W17-1718</url>
    <doi>10.18653/v1/W17-1718</doi>
    <abstract>
      This study investigates the supervised token-based identification of
      Multiword Expressions (MWEs). This is an ongoing research to exploit the
      information contained in the contexts in which different instances of an
      expression could occur. This information is used to investigate the
      question of whether an expression is literal or MWE. Lexical and syntactic
      context features derived from vector representations are shown to be more
      effective over traditional statistical measures to identify tokens of
      MWEs.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>taslimipoor-EtAl:2017:MWE2017</bibkey>
  </paper>
  <paper id='1719'>
    <title>Compositionality in Verb-Particle Constructions</title>
    <author>
      <first>Archna</first>
      <last>Bhatia</last>
    </author>
    <author>
      <first>Choh Man</first>
      <last>Teng</last>
    </author>
    <author>
      <first>James</first>
      <last>Allen</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>139–148</pages>
    <url>http://www.aclweb.org/anthology/W17-1719</url>
    <doi>10.18653/v1/W17-1719</doi>
    <abstract>
      We are developing a broad-coverage deep semantic lexicon for a system that
      parses sentences into a logical form expressed in a rich ontology that
      supports reasoning. In this paper we look at verb-particle constructions
      (VPCs), and the extent to which they can be treated compositionally vs
      idiomatically. First we distinguish between the different types of VPCs
      based on their compositionality and then present a set of heuristics for
      classifying specific instances as compositional or not. We then identify a
      small set of general sense classes for particles when used compositionally
      and discuss the resulting lexical representations that are being added to
      the lexicon. By treating VPCs as compositional whenever possible, we
      attain broad coverage in a compact way, and also enable interpretations of
      novel VPC usages not explicitly present in the lexicon.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bhatia-teng-allen:2017:MWE2017</bibkey>
  </paper>
  <paper id='1720'>
    <title>Rule-Based Translation of Spanish Verb-Noun Combinations into Basque</title>
    <author>
      <first>Uxoa</first>
      <last>Iñurrieta</last>
    </author>
    <author>
      <first>Itziar</first>
      <last>Aduriz</last>
    </author>
    <author>
      <first>Arantza</first>
      <last>Diaz de Ilarraza</last>
    </author>
    <author>
      <first>Gorka</first>
      <last>Labaka</last>
    </author>
    <author>
      <first>Kepa</first>
      <last>Sarasola</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>149–154</pages>
    <url>http://www.aclweb.org/anthology/W17-1720</url>
    <doi>10.18653/v1/W17-1720</doi>
    <abstract>
      This paper presents a method to improve the translation of Verb-Noun
      Combinations (VNCs) in a rule-based Machine Translation (MT) system for
      Spanish-Basque. Linguistic information about a set of VNCs is gathered
      from the public database Konbitzul, and it is integrated into the MT
      system, leading to an improvement in BLEU, NIST and TER scores, as well as
      the results being evidently better according to human evaluators.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>inurrieta-EtAl:2017:MWE2017</bibkey>
  </paper>
  <paper id='1721'>
    <title>Verb-Particle Constructions in Questions</title>
    <author>
      <first>Veronika</first>
      <last>Vincze</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>155–160</pages>
    <url>http://www.aclweb.org/anthology/W17-1721</url>
    <doi>10.18653/v1/W17-1721</doi>
    <abstract>
      In this paper, we investigate the behavior of verb-particle constructions
      in English questions. We present a small dataset that contains questions
      and verb-particle construction candidates. We demonstrate that there are
      significant differences in the distribution of WH-words, verbs and
      prepositions/particles in sentences that contain VPCs and sentences that
      contain only verb + prepositional phrase combinations both by statistical
      means and in machine learning experiments. Hence, VPCs and non-VPCs can be
      effectively separated from each other by using a rich feature set,
      containing several novel features.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vincze:2017:MWE2017</bibkey>
  </paper>
  <paper id='1722'>
    <title>Simple Compound Splitting for German</title>
    <author>
      <first>Marion</first>
      <last>Weller-Di Marco</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>161–166</pages>
    <url>http://www.aclweb.org/anthology/W17-1722</url>
    <doi>10.18653/v1/W17-1722</doi>
    <abstract>
      This paper presents a simple method for German compound splitting that
      combines a basic frequency-based approach with a form-to-lemma mapping to
      approximate morphological operations. With the exception of a small set of
      hand-crafted rules for modeling transitional elements, this approach is
      resource-poor. In our evaluation, the simple splitter outperforms a
      splitter relying on rich morphological resources.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wellerdimarco:2017:MWE2017</bibkey>
  </paper>
  <paper id='1723'>
    <title>
      Identification of Ambiguous Multiword Expressions Using Sequence Models
      and Lexical Resources
    </title>
    <author>
      <first>Manon</first>
      <last>Scholivet</last>
    </author>
    <author>
      <first>Carlos</first>
      <last>Ramisch</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>167–175</pages>
    <url>http://www.aclweb.org/anthology/W17-1723</url>
    <doi>10.18653/v1/W17-1723</doi>
    <abstract>
      We present a simple and efficient tagger capable of identifying highly
      ambiguous multiword expressions (MWEs) in French texts. It is based on
      conditional random fields (CRF), using local context information as
      features. We show that this approach can obtain results that, in some
      cases, approach more sophisticated parser-based MWE identification methods
      without requiring syntactic trees from a treebank. Moreover, we study how
      well the CRF can take into account external information coming from a
      lexicon.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>scholivet-ramisch:2017:MWE2017</bibkey>
  </paper>
  <paper id='1724'>
    <title>
      Comparing Recurring Lexico-Syntactic Trees (RLTs) and Ngram Techniques for
      Extended Phraseology Extraction
    </title>
    <author>
      <first>Agnès</first>
      <last>Tutin</last>
    </author>
    <author>
      <first>Olivier</first>
      <last>Kraif</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>176–180</pages>
    <url>http://www.aclweb.org/anthology/W17-1724</url>
    <doi>10.18653/v1/W17-1724</doi>
    <abstract>
      This paper aims at assessing to what extent a syntax-based method
      (Recurring Lexico-syntactic Trees (RLT) extraction) allows us to extract
      large phraseological units such as prefabricated routines, e.g. "as
      previously said" or "as far as we/I know" in scientific writing. In order
      to evaluate this method, we compare it to the classical ngram extraction
      technique, on a subset of recurring segments including speech verbs in a
      French corpus of scientific writing. Results show that the LRT extraction
      technique is far more efficient for extended MWEs such as routines or
      collocations but performs more poorly for surface phenomena such as
      syntactic constructions or fully frozen expressions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tutin-kraif:2017:MWE2017</bibkey>
  </paper>
  <paper id='1725'>
    <title>Benchmarking Joint Lexical and Syntactic Analysis on Multiword-Rich Data</title>
    <author>
      <first>Matthieu</first>
      <last>Constant</last>
    </author>
    <author>
      <first>Héctor</first>
      <last>Martínez Alonso</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>181–186</pages>
    <url>http://www.aclweb.org/anthology/W17-1725</url>
    <doi>10.18653/v1/W17-1725</doi>
    <abstract>
      This article evaluates the extension of a dependency parser that performs
      joint syntactic analysis and multiword expression identification. We show
      that, given sufficient training data, the parser benefits from explicit
      multiword information and improves overall labeled accuracy score in eight
      of the ten evaluation cases.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>constant-martinezalonso:2017:MWE2017</bibkey>
  </paper>
  <paper id='1726'>
    <title>
      Semi-Automated Resolution of Inconsistency for a Harmonized Multiword
      Expression and Dependency Parse Annotation
    </title>
    <author>
      <first>King</first>
      <last>Chan</last>
    </author>
    <author>
      <first>Julian</first>
      <last>Brooke</last>
    </author>
    <author>
      <first>Timothy</first>
      <last>Baldwin</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>187–193</pages>
    <url>http://www.aclweb.org/anthology/W17-1726</url>
    <doi>10.18653/v1/W17-1726</doi>
    <abstract>
      This paper presents a methodology for identifying and resolving various
      kinds of inconsistency in the context of merging dependency and multiword
      expression (MWE) annotations, to generate a dependency treebank with
      comprehensive MWE annotations. Candidates for correction are identified
      using a variety of heuristics, including an entirely novel one which
      identifies violations of MWE constituency in the dependency tree, and
      resolved by arbitration with minimal human intervention. Using this
      technique, we identified and corrected several hundred errors across both
      parse and MWE annotations, representing changes to a significant
      percentage (well over 10%) of the MWE instances in the joint corpus.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chan-brooke-baldwin:2017:MWE2017</bibkey>
  </paper>
  <paper id='1727'>
    <title>
      Combining Linguistic Features for the Detection of Croatian Multiword
      Expressions
    </title>
    <author>
      <first>Maja</first>
      <last>Buljan</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Šnajder</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>194–199</pages>
    <url>http://www.aclweb.org/anthology/W17-1727</url>
    <doi>10.18653/v1/W17-1727</doi>
    <abstract>
      As multiword expressions (MWEs) exhibit a range of idiosyncrasies, their
      automatic detection warrants the use of many different features. Tsvetkov
      and Wintner (2014) proposed a Bayesian network model that combines
      linguistically motivated features and also models their interactions. In
      this paper, we extend their model with new features and apply it to
      Croatian, a morphologically complex and a relatively free word order
      language, achieving a satisfactory performance of 0.823 F1-score.
      Furthermore, by comparing against (semi)naive Bayes models, we demonstrate
      that manually modeling feature interactions is indeed important. We make
      our annotated dataset of Croatian MWEs freely available.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>buljan-vsnajder:2017:MWE2017</bibkey>
  </paper>
  <paper id='1728'>
    <title>
      Complex Verbs are Different: Exploring the Visual Modality in Multi-Modal
      Models to Predict Compositionality
    </title>
    <author>
      <first>Maximilian</first>
      <last>Köper</last>
    </author>
    <author>
      <first>Sabine</first>
      <last>Schulte im Walde</last>
    </author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200–206</pages>
    <url>http://www.aclweb.org/anthology/W17-1728</url>
    <doi>10.18653/v1/W17-1728</doi>
    <abstract>
      This paper compares a neural network DSM relying on textual co-occurrences
      with a multi-modal model integrating visual information. We focus on
      nominal vs. verbal compounds, and zoom into lexical, empirical and
      perceptual target properties to explore the contribution of the visual
      modality. Our experiments show that (i) visual features contribute
      differently for verbs than for nouns, and (ii) images complement textual
      information, if (a) the textual modality by itself is poor and appropriate
      image subsets are used, or (b) the textual modality by itself is rich and
      large (potentially noisy) images are added.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koper-schulteimwalde:2017:MWE2017</bibkey>
  </paper>
  <paper id='1800'>
    <title>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</title>
    <editor>Eduardo Blanco</editor>
    <editor>Roser Morante</editor>
    <editor>Roser Saurí</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-18</url>
    <doi>10.18653/v1/W17-18</doi>
    <bibtype>book</bibtype>
    <bibkey>SemBEaR:2017</bibkey>
  </paper>
  <paper id='1801'>
    <title>
      Understanding the Semantics of Narratives of Interpersonal Violence
      through Reader Annotations and Physiological Reactions
    </title>
    <author>
      <first>Alexander</first>
      <last>Calderwood</last>
    </author>
    <author>
      <first>Elizabeth A.</first>
      <last>Pruett</last>
    </author>
    <author>
      <first>Raymond</first>
      <last>Ptucha</last>
    </author>
    <author>
      <first>Christopher</first>
      <last>Homan</last>
    </author>
    <author>
      <first>Cecilia</first>
      <last>Ovesdotter Alm</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W17-1801</url>
    <doi>10.18653/v1/W17-1801</doi>
    <abstract>
      Interpersonal violence (IPV) is a prominent sociological problem that
      affects people of all demographic backgrounds. By analyzing how readers
      interpret, perceive, and react to experiences narrated in social media
      posts, we explore an understudied source for discourse about abuse. We
      asked readers to annotate Reddit posts about relationships with vs.
      without IPV for stakeholder roles and emotion, while measuring their
      galvanic skin response (GSR), pulse, and facial expression. We map
      annotations to coreference resolution output to obtain a labeled
      coreference chain for stakeholders in texts, and apply automated semantic
      role labeling for analyzing IPV discourse. Findings provide insights into
      how readers process roles and emotion in narratives. For example, abusers
      tend to be linked with violent actions and certain affect states. We train
      classifiers to predict stakeholder categories of coreference chains. We
      also find that subjects' GSR noticeably changed for IPV texts, suggesting
      that co-collected measurement-based data about annotators can be used to
      support text annotation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>calderwood-EtAl:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1802'>
    <title>
      Intension, Attitude, and Tense Annotation in a High-Fidelity Semantic
      Representation
    </title>
    <author>
      <first>Gene</first>
      <last>Kim</last>
    </author>
    <author>
      <first>Lenhart</first>
      <last>Schubert</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–15</pages>
    <url>http://www.aclweb.org/anthology/W17-1802</url>
    <doi>10.18653/v1/W17-1802</doi>
    <abstract>
      This paper describes current efforts in developing an annotation schema
      and guidelines for sentences in Episodic Logic (EL). We focus on important
      distinctions for representing modality, attitudes, and tense and present
      an annotation schema that makes these distinctions. EL has proved
      competitive with other logical formulations in speed and
      inference-enablement, while expressing a wider array of natural language
      phenomena including intensional modification of predicates and sentences,
      propositional attitudes, and tense and aspect.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kim-schubert:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1803'>
    <title>Towards a lexicon of event-selecting predicates for a French FactBank</title>
    <author>
      <first>Ingrid</first>
      <last>Falk</last>
    </author>
    <author>
      <first>Fabienne</first>
      <last>Martin</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16–21</pages>
    <url>http://www.aclweb.org/anthology/W17-1803</url>
    <doi>10.18653/v1/W17-1803</doi>
    <abstract>
      This paper presents ongoing work for the construction of a French FactBank
      and a lexicon of French event-selecting predi- cates (ESPs), by applying
      the factuality detection algorithm introduced in (Saurí and Pustejovsky,
      2012). This algorithm relies on a lexicon of ESPs, specifying how these
      predicates influence the polar- ity of their embedded events. For this
      pilot study, we focused on French factive and implicative verbs, and
      capitalised on a lex- ical resource for the English counterparts of these
      verbs provided by the CLSI Group (Nairn et al., 2006; Karttunen, 2012).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>falk-martin:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1804'>
    <title>Universal Dependencies to Logical Form with Negation Scope</title>
    <author>
      <first>Federico</first>
      <last>Fancellu</last>
    </author>
    <author>
      <first>Siva</first>
      <last>Reddy</last>
    </author>
    <author>
      <first>Adam</first>
      <last>Lopez</last>
    </author>
    <author>
      <first>Bonnie</first>
      <last>Webber</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–32</pages>
    <url>http://www.aclweb.org/anthology/W17-1804</url>
    <doi>10.18653/v1/W17-1804</doi>
    <abstract>
      Many language technology applications would benefit from the ability to
      represent negation and its scope on top of widely-used linguistic
      resources. In this paper, we investigate the possibility of obtaining a
      first-order logic representation with negation scope marked using
      Universal Dependencies. To do so, we enhance UDepLambda, a framework that
      converts dependency graphs to logical forms. The resulting UDepLambdalnot
      is able to handle phenomena related to scope by means of an higher-order
      type theory, relevant not only to negation but also to universal
      quantification and other complex semantic phenomena. The initial
      conversion we did for English is promising, in that one can represent the
      scope of negation also in the presence of more complex phenomena such as
      universal quantifiers.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fancellu-EtAl:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1805'>
    <title>Meaning Banking beyond Events and Roles</title>
    <author>
      <first>Johan</first>
      <last>Bos</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33</pages>
    <url>http://www.aclweb.org/anthology/W17-1805</url>
    <doi>10.18653/v1/W17-1805</doi>
    <abstract>
      In this talk I will discuss the analysis of several semantic phenomena
      that need meaning representations that can describe attributes of
      propositional contexts. I will do this in a version of Discourse
      Representation Theory, using a universal semantic tagset developed as part
      of a project that aims to produce a large meaning bank (a
      semantically-annotated corpus) for four languages (English, Dutch, German
      and Italian).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bos:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1806'>
    <title>The Scope and Focus of Negation: A Complete Annotation Framework for Italian</title>
    <author>
      <first>Begoña</first>
      <last>Altuna</last>
    </author>
    <author>
      <first>Anne-Lyse</first>
      <last>Minard</last>
    </author>
    <author>
      <first>Manuela</first>
      <last>Speranza</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34–42</pages>
    <url>http://www.aclweb.org/anthology/W17-1806</url>
    <doi>10.18653/v1/W17-1806</doi>
    <abstract>
      In this paper we present a complete framework for the annotation of
      negation in Italian, which accounts for both negation scope and negation
      focus, and also for language-specific phenomena such as negative concord.
      In our view, the annotation of negation complements more comprehensive
      Natural Language Processing tasks, such as temporal information processing
      and sentiment analysis. We applied the proposed framework and the
      guidelines built on top of it to the annotation of written texts, namely
      news articles and tweets, thus producing annotated data for a total of
      over 36,000 tokens.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>altuna-minard-speranza:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1807'>
    <title>Annotation of negation in the IULA Spanish Clinical Record Corpus</title>
    <author>
      <first>Montserrat</first>
      <last>Marimon</last>
    </author>
    <author>
      <first>Jorge</first>
      <last>Vivaldi</last>
    </author>
    <author>
      <first>Núria</first>
      <last>Bel</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–52</pages>
    <url>http://www.aclweb.org/anthology/W17-1807</url>
    <doi>10.18653/v1/W17-1807</doi>
    <abstract>
      This paper presents the IULA Spanish Clinical Record Corpus, a corpus of
      3,194 sentences extracted from anonymized clinical records and manually
      annotated with negation markers and their scope. The corpus was conceived
      as a resource to support clinical text-mining systems, but it is also a
      useful resource for other Natural Language Processing systems handling
      clinical texts: automatic encoding of clinical records, diagnosis support,
      term extraction, among others, as well as for the study of clinical texts.
      The corpus is publicly available with a CC-BY-SA 3.0 license.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marimon-vivaldi-bel:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1808'>
    <title>Annotating Negation in Spanish Clinical Texts</title>
    <author>
      <first>Noa</first>
      <last>Cruz</last>
    </author>
    <author>
      <first>Roser</first>
      <last>Morante</last>
    </author>
    <author>
      <first>Manuel J.</first>
      <last>Maña López</last>
    </author>
    <author>
      <first>Jacinto</first>
      <last>Mata Vázquez</last>
    </author>
    <author>
      <first>Carlos L.</first>
      <last>Parra Calderón</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53–58</pages>
    <url>http://www.aclweb.org/anthology/W17-1808</url>
    <doi>10.18653/v1/W17-1808</doi>
    <abstract>
      In this paper we present on-going work on annotating negation in Spanish
      clinical documents. A corpus of anamnesis and radiology reports has been
      annotated by two domain expert annotators with negation markers and
      negated events. The Dice coefficient for inter-annotator agreement is
      higher than 0.94 for negation markers and higher than 0.72 for negated
      events. The corpus will be publicly released when the annotation process
      is finished, constituting the first corpus annotated with negation for
      Spanish clinical reports available for the NLP community.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cruz-EtAl:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1809'>
    <title>Neural Networks for Negation Cue Detection in Chinese</title>
    <author>
      <first>Hangfeng</first>
      <last>He</last>
    </author>
    <author>
      <first>Federico</first>
      <last>Fancellu</last>
    </author>
    <author>
      <first>Bonnie</first>
      <last>Webber</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–63</pages>
    <url>http://www.aclweb.org/anthology/W17-1809</url>
    <doi>10.18653/v1/W17-1809</doi>
    <abstract>
      Negation cue detection involves identifying the span inherently expressing
      negation in a negative sentence. In Chinese, negative cue detection is
      complicated by morphological proprieties of the language. Previous work
      has shown that negative cue detection in Chinese can benefit from specific
      lexical and morphemic features, as well as cross-lingual information. We
      show here that they are not necessary: A bi-directional LSTM can perform
      equally well, with minimal feature engineering. In particular, the use of
      a character-based model allows us to capture characteristics of negation
      cues in Chinese using word-embedding information only. Not only does our
      model performs on par with previous work, further error analysis clarifies
      what problems remain to be addressed.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>he-fancellu-webber:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1810'>
    <title>An open-source tool for negation detection: a maximum-margin approach</title>
    <author>
      <first>Martine</first>
      <last>Enger</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <author>
      <first>Lilja</first>
      <last>Øvrelid</last>
    </author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>64–69</pages>
    <url>http://www.aclweb.org/anthology/W17-1810</url>
    <doi>10.18653/v1/W17-1810</doi>
    <abstract>
      This paper presents an open-source toolkit for negation detection. It
      identifies negation cues and their corresponding scope in either raw or
      parsed text using maximum-margin classification. The system design draws
      on best practice from the existing literature on negation detection,
      aiming for a simple and portable system that still achieves competitive
      performance. Pre-trained models and experimental results are provided for
      English.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>enger-velldal-ovrelid:2017:SemBEaR</bibkey>
  </paper>
  <paper id='1900'>
    <title>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </title>
    <editor>Jose Camacho-Collados</editor>
    <editor>Mohammad Taher Pilehvar</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-19</url>
    <doi>10.18653/v1/W17-19</doi>
    <bibtype>book</bibtype>
    <bibkey>SENSE2017:2017</bibkey>
  </paper>
  <paper id='1901'>
    <title>Compositional Semantics using Feature-Based Models from WordNet</title>
    <author>
      <first>Pablo</first>
      <last>Gamallo</last>
    </author>
    <author>
      <first>Martín</first>
      <last>Pereira-Fariña</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–11</pages>
    <url>http://www.aclweb.org/anthology/W17-1901</url>
    <doi>10.18653/v1/W17-1901</doi>
    <abstract>
      This article describes a method to build semantic representations of
      composite expressions in a compositional way by using WordNet relations to
      represent the meaning of words. The meaning of a target word is modelled
      as a vector in which its semantically related words are assigned weights
      according to both the type of the relationship and the distance to the
      target word. Word vectors are compositionally combined by syntactic
      dependencies. Each syntactic dependency triggers two complementary
      compositional functions: the named head function and dependent function.
      The experiments show that the proposed compositional method outperforms
      the state-of-the-art for both intransitive subject-verb and transitive
      subject-verb-object constructions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gamallo-pereirafarina:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1902'>
    <title>Automated WordNet Construction Using Word Embeddings</title>
    <author>
      <first>Mikhail</first>
      <last>Khodak</last>
    </author>
    <author>
      <first>Andrej</first>
      <last>Risteski</last>
    </author>
    <author>
      <first>Christiane</first>
      <last>Fellbaum</last>
    </author>
    <author>
      <first>Sanjeev</first>
      <last>Arora</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12–23</pages>
    <url>http://www.aclweb.org/anthology/W17-1902</url>
    <doi>10.18653/v1/W17-1902</doi>
    <abstract>
      We present a fully unsupervised method for automated construction of
      WordNets based upon recent advances in distributional representations of
      sentences and word-senses combined with readily available machine
      translation tools. The approach requires very few linguistic resources and
      is thus extensible to multiple target languages. To evaluate our method we
      construct two 600-word testsets for word-to-synset matching in French and
      Russian using native speakers and evaluate the performance of our method
      along with several other recent approaches. Our method exceeds the best
      language-specific and multi-lingual automated WordNets in F-score for both
      languages. The databases we construct for French and Russian, both
      languages without large publicly available manually constructed WordNets,
      will be publicly released along with the testsets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>khodak-EtAl:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1903'>
    <title>
      Improving Verb Metaphor Detection by Propagating Abstractness to Words,
      Phrases and Individual Senses
    </title>
    <author>
      <first>Maximilian</first>
      <last>Köper</last>
    </author>
    <author>
      <first>Sabine</first>
      <last>Schulte im Walde</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–30</pages>
    <url>http://www.aclweb.org/anthology/W17-1903</url>
    <doi>10.18653/v1/W17-1903</doi>
    <abstract>
      Abstract words refer to things that can not be seen, heard, felt, smelled,
      or tasted as opposed to concrete words. Among other applications, the
      degree of abstractness has been shown to be a useful information for
      metaphor detection. Our contribution to this topic are as follows: i) we
      compare supervised techniques to learn and extend abstractness ratings for
      huge vocabularies ii) we learn and investigate norms for larger units by
      propagating abstractness to verb-noun pairs which lead to better metaphor
      detection iii) we overcome the limitation of learning a single rating per
      word and show that multi-sense abstractness ratings are potentially useful
      for metaphor detection. Finally, with this paper we publish automatically
      created abstractness norms for 3million English words and multi-words as
      well as automatically created sense specific abstractness ratings
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koper-schulteimwalde:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1904'>
    <title>
      Improving Clinical Diagnosis Inference through Integration of Structured
      and Unstructured Knowledge
    </title>
    <author>
      <first>Yuan</first>
      <last>Ling</last>
    </author>
    <author>
      <first>Yuan</first>
      <last>An</last>
    </author>
    <author>
      <first>Sadid</first>
      <last>Hasan</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–36</pages>
    <url>http://www.aclweb.org/anthology/W17-1904</url>
    <doi>10.18653/v1/W17-1904</doi>
    <abstract>
      This paper presents a novel approach to the task of automatically
      inferring the most probable diagnosis from a given clinical narrative.
      Structured Knowledge Bases (KBs) can be useful for such complex tasks but
      not sufficient. Hence, we leverage a vast amount of unstructured free text
      to integrate with structured KBs. The key innovative ideas include
      building a concept graph from both structured and unstructured knowledge
      sources and ranking the diagnosis concepts using the enhanced word
      embedding vectors learned from integrated sources. Experiments on the TREC
      CDS and HumanDx datasets showed that our methods improved the results of
      clinical diagnosis inference.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ling-an-hasan:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1905'>
    <title>
      Classifying Lexical-semantic Relationships by Exploiting Sense/Concept
      Representations
    </title>
    <author>
      <first>Kentaro</first>
      <last>Kanada</last>
    </author>
    <author>
      <first>Tetsunori</first>
      <last>Kobayashi</last>
    </author>
    <author>
      <first>Yoshihiko</first>
      <last>Hayashi</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–46</pages>
    <url>http://www.aclweb.org/anthology/W17-1905</url>
    <doi>10.18653/v1/W17-1905</doi>
    <abstract>
      This paper proposes a method for classifying the type of lexical-semantic
      relation between a given pair of words. Given an inventory of target
      relationships, this task can be seen as a multi-class classification
      problem. We train a supervised classifier by assuming: (1) a specific type
      of lexical-semantic relation between a pair of words would be indicated by
      a carefully designed set of relation-specific similarities associated with
      the words; and (2) the similarities could be effectively computed by
      “sense representations” (sense/concept embeddings). The experimental
      results show that the proposed method clearly outperforms an existing
      state-of-the-art method that does not utilize sense/concept embeddings,
      thereby demonstrating the effectiveness of the sense representations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kanada-kobayashi-hayashi:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1906'>
    <title>Supervised and unsupervised approaches to measuring usage similarity</title>
    <author>
      <first>Milton</first>
      <last>King</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Cook</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–52</pages>
    <url>http://www.aclweb.org/anthology/W17-1906</url>
    <doi>10.18653/v1/W17-1906</doi>
    <abstract>
      Usage similarity (USim) is an approach to determining word meaning in
      context that does not rely on a sense inventory. Instead, pairs of usages
      of a target lemma are rated on a scale. In this paper we propose
      unsupervised approaches to USim based on embeddings for words, contexts,
      and sentences, and achieve state-of-the-art results over two USim
      datasets. We further consider supervised approaches to USim, and find that
      although they outperform unsupervised approaches, they are unable to
      generalize to lemmas that are unseen in the training data.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>king-cook:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1907'>
    <title>Lexical Disambiguation of Igbo using Diacritic Restoration</title>
    <author>
      <first>Ignatius</first>
      <last>Ezeani</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Hepple</last>
    </author>
    <author>
      <first>Ikechukwu</first>
      <last>Onyenwe</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53–60</pages>
    <url>http://www.aclweb.org/anthology/W17-1907</url>
    <doi>10.18653/v1/W17-1907</doi>
    <abstract>
      Properly written texts in Igbo, a low-resource African language, are rich
      in both orthographic and tonal diacritics. Diacritics are essential in
      capturing the distinctions in pronunciation and meaning of words, as well
      as in lexical disambiguation. Unfortunately, most electronic texts in
      diacritic languages are written without diacritics. This makes diacritic
      restoration a necessary step in corpus building and language processing
      tasks for languages with diacritics. In our previous work, we built some
      n-gram models with simple smoothing techniques based on a closed-world
      assumption. However, as a classification task, diacritic restoration is
      well suited for and will be more generalisable with machine learning. This
      paper, therefore, presents a more standard approach to dealing with the
      task which involves the application of machine learning algorithms.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ezeani-hepple-onyenwe:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1908'>
    <title>
      Creating and Validating Multilingual Semantic Representations for Six
      Languages: Expert versus Non-Expert Crowds
    </title>
    <author>
      <first>Mahmoud</first>
      <last>El-Haj</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Rayson</last>
    </author>
    <author>
      <first>Scott</first>
      <last>Piao</last>
    </author>
    <author>
      <first>Stephen</first>
      <last>Wattam</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61–71</pages>
    <url>http://www.aclweb.org/anthology/W17-1908</url>
    <doi>10.18653/v1/W17-1908</doi>
    <abstract>
      Creating high-quality wide-coverage multilingual semantic lexicons to
      support knowledge-based approaches is a challenging time-consuming manual
      task. This has traditionally been performed by linguistic experts: a slow
      and expensive process. We present an experiment in which we adapt and
      evaluate crowdsourcing methods employing native speakers to generate a
      list of coarse-grained senses under a common multilingual semantic
      taxonomy for sets of words in six languages. 451 non-experts (including
      427 Mechanical Turk workers) and 15 expert participants semantically
      annotated 250 words manually for Arabic, Chinese, English, Italian,
      Portuguese and Urdu lexicons. In order to avoid erroneous (spam)
      crowdsourced results, we used a novel task-specific two-phase filtering
      process where users were asked to identify synonyms in the target
      language, and remove erroneous senses.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>elhaj-EtAl:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1909'>
    <title>
      Using Linked Disambiguated Distributional Networks for Word Sense
      Disambiguation
    </title>
    <author>
      <first>Alexander</first>
      <last>Panchenko</last>
    </author>
    <author>
      <first>Stefano</first>
      <last>Faralli</last>
    </author>
    <author>
      <first>Simone Paolo</first>
      <last>Ponzetto</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Biemann</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72–78</pages>
    <url>http://www.aclweb.org/anthology/W17-1909</url>
    <doi>10.18653/v1/W17-1909</doi>
    <abstract>
      We introduce a new method for unsupervised knowledge-based word sense
      disambiguation (WSD) based on a resource that links two types of
      sense-aware lexical networks: one is induced from a corpus using
      distributional semantics, the other is manually constructed. The
      combination of two networks reduces the sparsity of sense representations
      used for WSD. We evaluate these enriched representations within two
      lexical sample sense disambiguation benchmarks. Our results indicate that
      (1) features extracted from the corpus-based resource help to
      significantly outperform a model based solely on the lexical resource; (2)
      our method achieves results comparable or better to four state-of-the-art
      unsupervised knowledge-based WSD systems including three hybrid systems
      that also rely on text corpora. In contrast to these hybrid methods, our
      approach does not require access to web search engines, texts mapped to a
      sense inventory, or machine translation systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>panchenko-EtAl:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1910'>
    <title>One Representation per Word - Does it make Sense for Composition?</title>
    <author>
      <first>Thomas</first>
      <last>Kober</last>
    </author>
    <author>
      <first>Julie</first>
      <last>Weeds</last>
    </author>
    <author>
      <first>John</first>
      <last>Wilkie</last>
    </author>
    <author>
      <first>Jeremy</first>
      <last>Reffin</last>
    </author>
    <author>
      <first>David</first>
      <last>Weir</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–90</pages>
    <url>http://www.aclweb.org/anthology/W17-1910</url>
    <doi>10.18653/v1/W17-1910</doi>
    <abstract>
      In this paper, we investigate whether an a priori disambiguation of word
      senses is strictly necessary or whether the meaning of a word in context
      can be disambiguated through composition alone. We evaluate the
      performance of off-the-shelf single-vector and multi-sense vector models
      on a benchmark phrase similarity task and a novel task for word-sense
      discrimination. We find that single-sense vector models perform as well or
      better than multi-sense vector models despite arguably less clean
      elementary representations. Our findings furthermore show that simple
      composition functions such as pointwise addition are able to recover sense
      specific information from a single-sense vector model remarkably well.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kober-EtAl:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1911'>
    <title>Elucidating Conceptual Properties from Word Embeddings</title>
    <author>
      <first>Kyoung-Rok</first>
      <last>Jang</last>
    </author>
    <author>
      <first>Sung-Hyon</first>
      <last>Myaeng</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–95</pages>
    <url>http://www.aclweb.org/anthology/W17-1911</url>
    <doi>10.18653/v1/W17-1911</doi>
    <abstract>
      In this paper, we introduce a method of identifying the components (i.e.
      dimensions) of word embeddings that strongly signifies properties of a
      word. By elucidating such properties hidden in word embeddings, we could
      make word embeddings more interpretable, and also could perform
      property-based meaning comparison. With the capability, we can answer
      questions like "To what degree a given word has the property cuteness?" or
      "In what perspective two words are similar?". We verify our method by
      examining how the strength of property-signifying components correlates
      with the degree of prototypicality of a target word.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jang-myaeng:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1912'>
    <title>TTCSe: a Vectorial Resource for Computing Conceptual Similarity</title>
    <author>
      <first>Enrico</first>
      <last>Mensa</last>
    </author>
    <author>
      <first>Daniele P.</first>
      <last>Radicioni</last>
    </author>
    <author>
      <first>Antonio</first>
      <last>Lieto</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>96–101</pages>
    <url>http://www.aclweb.org/anthology/W17-1912</url>
    <doi>10.18653/v1/W17-1912</doi>
    <abstract>
      In this paper we introduce the TTCSe, a linguistic resource that relies on
      BabelNet, NASARI and ConceptNet, that has now been used to compute the
      conceptual similarity between concept pairs. The conceptual representation
      herein provides uniform access to concepts based on BabelNet synset IDs,
      and consists of a vector-based semantic representation which is compliant
      with the Conceptual Spaces, a geometric framework for common-sense
      knowledge representation and reasoning. The TTCSe has been evaluated in a
      preliminary experimentation on a conceptual similarity task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mensa-radicioni-lieto:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1913'>
    <title>
      Measuring the Italian-English lexical gap for action verbs and its impact
      on translation
    </title>
    <author>
      <first>Lorenzo</first>
      <last>Gregori</last>
    </author>
    <author>
      <first>Alessandro</first>
      <last>Panunzi</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102–109</pages>
    <url>http://www.aclweb.org/anthology/W17-1913</url>
    <doi>10.18653/v1/W17-1913</doi>
    <abstract>
      This paper describes a method to measure the lexical gap of action verbs
      in Italian and English by using the IMAGACT ontology of action. The
      fine-grained categorization of action concepts of the data source allowed
      to have wide overview of the relation between concepts in the two
      languages. The calculated lexical gap for both English and Italian is
      about 30% of the action concepts, much higher than previous results.
      Beyond this general numbers a deeper analysis has been performed in order
      to evaluate the impact that lexical gaps can have on translation. In
      particular a distinction has been made between the cases in which the
      presence of a lexical gap affects translation correctness and completeness
      at a semantic level. The results highlight a high percentage of concepts
      that can be considered hard to translate (about 18% from English to
      Italian and 20% from Italian to English) and confirms that action verbs
      are a critical lexical class for translation tasks.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gregori-panunzi:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1914'>
    <title>Word Sense Filtering Improves Embedding-Based Lexical Substitution</title>
    <author>
      <first>Anne</first>
      <last>Cocos</last>
    </author>
    <author>
      <first>Marianna</first>
      <last>Apidianaki</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Callison-Burch</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110–119</pages>
    <url>http://www.aclweb.org/anthology/W17-1914</url>
    <doi>10.18653/v1/W17-1914</doi>
    <abstract>
      The role of word sense disambiguation in lexical substitution has been
      questioned due to the high performance of vector space models which
      propose good substitutes without explicitly accounting for sense. We show
      that a filtering mechanism based on a sense inventory optimized for
      substitutability can improve the results of these models. Our sense
      inventory is constructed using a clustering method which generates
      paraphrase clusters that are congruent with lexical substitution
      annotations in a development set. The results show that lexical
      substitution can still benefit from senses which can improve the output of
      vector space paraphrase ranking models.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cocos-apidianaki-callisonburch:2017:SENSE2017</bibkey>
  </paper>
  <paper id='1915'>
    <title>
      Supervised and Unsupervised Word Sense Disambiguation on Word Embedding
      Vectors of Unambigous Synonyms
    </title>
    <author>
      <first>Aleksander</first>
      <last>Wawer</last>
    </author>
    <author>
      <first>Agnieszka</first>
      <last>Mykowiecka</last>
    </author>
    <booktitle>
      Proceedings of the 1st Workshop on Sense, Concept and Entity
      Representations and their Applications
    </booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>120–125</pages>
    <url>http://www.aclweb.org/anthology/W17-1915</url>
    <doi>10.18653/v1/W17-1915</doi>
    <abstract>
      This paper compares two approaches to word sense disambiguation using word
      embeddings trained on unambiguous synonyms. The first is unsupervised
      method based on computing log probability from sequences of word embedding
      vectors, taking into account ambiguous word senses and guessing correct
      sense from context. The second method is supervised. We use a multilayer
      neural network model to learn a context-sensitive transformation that maps
      an input vector of ambiguous word into an output vector representing its
      sense. We evaluate both methods on corpora with manual annotations of word
      senses from the Polish wordnet (plWordnet).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wawer-mykowiecka:2017:SENSE2017</bibkey>
  </paper>
  <paper id='2000'>
    <title>Proceedings of the Sixth Workshop on Vision and Language</title>
    <editor>Anya Belz</editor>
    <editor>Erkut Erdem</editor>
    <editor>Katerina Pastra</editor>
    <editor>Krystian Mikolajczyk</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-20</url>
    <doi>10.18653/v1/W17-20</doi>
    <bibtype>book</bibtype>
    <bibkey>VL17:2017</bibkey>
  </paper>
  <paper id='2001'>
    <title>
      The BURCHAK corpus: a Challenge Data Set for Interactive Learning of
      Visually Grounded Word Meanings
    </title>
    <author>
      <first>Yanchao</first>
      <last>Yu</last>
    </author>
    <author>
      <first>Arash</first>
      <last>Eshghi</last>
    </author>
    <author>
      <first>Gregory</first>
      <last>Mills</last>
    </author>
    <author>
      <first>Oliver</first>
      <last>Lemon</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-2001</url>
    <doi>10.18653/v1/W17-2001</doi>
    <abstract>
      We motivate and describe a new freely available human-human dialogue data
      set for interactive learning of visually grounded word meanings through
      ostensive definition by a tutor to a learner. The data has been collected
      using a novel, character-by-character variant of the DiET chat tool
      (Healey et al., 2003; anon.) with a novel task, where a Learner needs to
      learn invented visual attribute words (such as "burchak" for square) from
      a tutor. As such, the text-based interactions closely resemble
      face-to-face conversation and thus contain many of the linguistic
      phenomena encountered in natural, spontaneous dialogue. These include
      self- and other-correction, mid-sentence continuations, interruptions,
      turn overlaps, fillers, hedges and many kinds of ellipsis. We also present
      a generic n-gram framework for building user (i.e. tutor) simulations from
      this type of incremental dialogue data, which is freely available to
      researchers. We show that the simulations produce outputs that are similar
      to the original data (e.g. 78% turn match similarity). Finally, we train
      and evaluate a Reinforcement Learning dialogue control agent for learning
      visually grounded word meanings, trained from the BURCHAK corpus. The
      learned policy shows comparable performance to a rule-based system built
      previously.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yu-EtAl:2017:VL17</bibkey>
  </paper>
  <paper id='2002'>
    <title>
      The Use of Object Labels and Spatial Prepositions as Keywords in a
      Web-Retrieval-Based Image Caption Generation System
    </title>
    <author>
      <first>Brandon</first>
      <last>Birmingham</last>
    </author>
    <author>
      <first>Adrian</first>
      <last>Muscat</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–20</pages>
    <url>http://www.aclweb.org/anthology/W17-2002</url>
    <doi>10.18653/v1/W17-2002</doi>
    <abstract>
      In this paper, a retrieval-based caption generation system that searches
      the web for suitable image descriptions is studied. Google's reverse image
      search is used to find potentially relevant web multimedia content for
      query images. Sentences are extracted from web pages and the likelihood of
      the descriptions is computed to select one sentence from the retrieved
      text documents. The search mechanism is modified to replace the caption
      generated by Google with a caption composed of labels and spatial
      prepositions as part of the query's text alongside the image. The object
      labels are obtained using an off-the-shelf R-CNN and a machine learning
      model is developed to predict the prepositions. The effect on the caption
      generation system performance when using the generated text is
      investigated. Both human evaluations and automatic metrics are used to
      evaluate the retrieved descriptions. Results show that the
      web-retrieval-based approach performed better when describing
      single-object images with sentences extracted from stock photography
      websites. On the other hand, images with two image objects were better
      described with template-generated sentences composed of object labels and
      prepositions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>birmingham-muscat:2017:VL17</bibkey>
  </paper>
  <paper id='2003'>
    <title>
      Learning to Recognize Animals by Watching Documentaries: Using Subtitles
      as Weak Supervision
    </title>
    <author>
      <first>Aparna</first>
      <last>Nurani Venkitasubramanian</last>
    </author>
    <author>
      <first>Tinne</first>
      <last>Tuytelaars</last>
    </author>
    <author>
      <first>Marie-Francine</first>
      <last>Moens</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–30</pages>
    <url>http://www.aclweb.org/anthology/W17-2003</url>
    <doi>10.18653/v1/W17-2003</doi>
    <abstract>
      We investigate animal recognition models learned from wildlife video
      documentaries by using the weak supervision of the textual subtitles. This
      is a particularly challenging setting, since i) the animals occur in their
      natural habitat and are often largely occluded and ii) subtitles are to a
      large degree complementary to the visual content, providing a very weak
      supervisory signal. This is in contrast to most work on integrated vision
      and language in the literature, where textual descriptions are tightly
      linked to the image content, and often generated in a curated fashion for
      the task at hand. In particular, we investigate different image
      representations and models, including a support vector machine on top of
      activations of a pretrained convolutional neural network, as well as a
      Naive Bayes framework on a 'bag-of-activations' image representation,
      where each element of the bag is considered separately. This
      representation allows key components in the image to be isolated, in spite
      of largely varying backgrounds and image clutter, without an object
      detection or image segmentation step. The methods are evaluated based on
      how well they transfer to unseen camera-trap images captured across
      diverse topographical regions under different environmental conditions and
      illumination settings, involving a large domain shift.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nuranivenkitasubramanian-tuytelaars-moens:2017:VL17</bibkey>
  </paper>
  <paper id='2004'>
    <title>
      Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study
      on E-Commerce Listing Titles
    </title>
    <author>
      <first>Iacer</first>
      <last>Calixto</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Stein</last>
    </author>
    <author>
      <first>Evgeny</first>
      <last>Matusov</last>
    </author>
    <author>
      <first>Sheila</first>
      <last>Castilho</last>
    </author>
    <author>
      <first>Andy</first>
      <last>Way</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–37</pages>
    <url>http://www.aclweb.org/anthology/W17-2004</url>
    <doi>10.18653/v1/W17-2004</doi>
    <abstract>
      In this paper, we study how humans perceive the use of images as an
      additional knowledge source to machine-translate user-generated product
      listings in an e-commerce company. We conduct a human evaluation where we
      assess how a multi-modal neural machine translation (NMT) model compares
      to two text-only approaches: a conventional state-of-the-art
      attention-based NMT and a phrase-based statistical machine translation
      (PBSMT) model. We evaluate translations obtained with different systems
      and also discuss the data set of user-generated product listings, which in
      our case comprises both product listings and associated images. We found
      that humans preferred translations obtained with a PBSMT system to both
      text-only and multi-modal NMT over 56% of the time. Nonetheless, human
      evaluators ranked translations from a multi-modal NMT model as better than
      those of a text-only NMT over 88% of the time, which suggests that images
      do help NMT in this use-case.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>calixto-EtAl:2017:VL17</bibkey>
  </paper>
  <paper id='2005'>
    <title>The BreakingNews Dataset</title>
    <author>
      <first>Arnau</first>
      <last>Ramisa</last>
    </author>
    <author>
      <first>Fei</first>
      <last>Yan</last>
    </author>
    <author>
      <first>Francesc</first>
      <last>Moreno-Noguer</last>
    </author>
    <author>
      <first>Krystian</first>
      <last>Mikolajczyk</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–39</pages>
    <url>http://www.aclweb.org/anthology/W17-2005</url>
    <doi>10.18653/v1/W17-2005</doi>
    <abstract>
      We present BreakingNews, a novel dataset with approximately 100K news
      articles including images, text and captions, and enriched with
      heterogeneous meta-data (e.g. GPS coordinates and popularity metrics). The
      tenuous connection between the images and text in news data is appropriate
      to take work at the intersection of Computer Vision and Natural Language
      Processing to the next step, hence we hope this dataset will help spur
      progress in the field.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ramisa-EtAl:2017:VL17</bibkey>
  </paper>
  <paper id='2006'>
    <title>
      Automatic identification of head movements in video-recorded
      conversations: can words help?
    </title>
    <author>
      <first>Patrizia</first>
      <last>Paggio</last>
    </author>
    <author>
      <first>Costanza</first>
      <last>Navarretta</last>
    </author>
    <author>
      <first>Bart</first>
      <last>Jongejan</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>40–42</pages>
    <url>http://www.aclweb.org/anthology/W17-2006</url>
    <doi>10.18653/v1/W17-2006</doi>
    <abstract>
      We present an approach where an SVM classifier learns to classify head
      movements based on measurements of velocity, acceleration, and the third
      derivative of position with respect to time, jerk. Consequently,
      annotations of head movements are added to new video data. The results of
      the automatic annotation are evaluated against manual annotations in the
      same data and show an accuracy of 68% with respect to these. The results
      also show that using jerk improves accuracy. We then conduct an
      investigation of the overlap between temporal sequences classified as
      either movement or non-movement and the speech stream of the person
      performing the gesture. The statistics derived from this analysis show
      that using word features may help increase the accuracy of the model.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>paggio-navarretta-jongejan:2017:VL17</bibkey>
  </paper>
  <paper id='2007'>
    <title>Multi-Modal Fashion Product Retrieval</title>
    <author>
      <first>Antonio</first>
      <last>Rubio Romano</last>
    </author>
    <author>
      <first>LongLong</first>
      <last>Yu</last>
    </author>
    <author>
      <first>Edgar</first>
      <last>Simo-Serra</last>
    </author>
    <author>
      <first>Francesc</first>
      <last>Moreno-Noguer</last>
    </author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–45</pages>
    <url>http://www.aclweb.org/anthology/W17-2007</url>
    <doi>10.18653/v1/W17-2007</doi>
    <abstract>
      Finding a product in the fashion world can be a daunting task. Everyday,
      e-commerce sites are updating with thousands of images and their
      associated metadata (textual information), deepening the problem. In this
      paper, we leverage both the images and textual metadata and propose a
      joint multi-modal embedding that maps both the text and images into a
      common latent space. Distances in the latent space correspond to
      similarity between products, allowing us to effectively perform retrieval
      in this latent space. We compare against existing approaches and show
      significant improvements in retrieval tasks on a large-scale e-commerce
      dataset.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rubioromano-EtAl:2017:VL17</bibkey>
  </paper>
  <paper id='2200'>
    <title>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </title>
    <editor>Beatrice Alex</editor>
    <editor>Stefania Degaetano-Ortlieb</editor>
    <editor>Anna Feldman</editor>
    <editor>Anna Kazantseva</editor>
    <editor>Nils Reiter</editor>
    <editor>Stan Szpakowicz</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-22</url>
    <doi>10.18653/v1/W17-22</doi>
    <bibtype>book</bibtype>
    <bibkey>LaTeCH-CLfL:2017</bibkey>
  </paper>
  <paper id='2201'>
    <title>Metaphor Detection in a Poetry Corpus</title>
    <author>
      <first>Vaibhav</first>
      <last>Kesarwani</last>
    </author>
    <author>
      <first>Diana</first>
      <last>Inkpen</last>
    </author>
    <author>
      <first>Stan</first>
      <last>Szpakowicz</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Tanasescu</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W17-2201</url>
    <doi>10.18653/v1/W17-2201</doi>
    <abstract>
      Metaphor is indispensable in poetry. It showcases the poet's creativity,
      and contributes to the overall emotional pertinence of the poem while
      honing its specific rhetorical impact. Previous work on metaphor detection
      relies on either rule-based or statistical models, none of them applied to
      poetry. Our method focuses on metaphor detection in a poetry corpus. It
      combines rule-based and statistical models (word embeddings) to develop a
      new classification system. Our system has achieved a precision of 0.759
      and a recall of 0.804 in identifying one type of metaphor in poetry.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kesarwani-EtAl:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2202'>
    <title>Machine Translation and Automated Analysis of the Sumerian Language</title>
    <author>
      <first>Émilie</first>
      <last>Pagé-Perron</last>
    </author>
    <author>
      <first>Maria</first>
      <last>Sukhareva</last>
    </author>
    <author>
      <first>Ilya</first>
      <last>Khait</last>
    </author>
    <author>
      <first>Christian</first>
      <last>Chiarcos</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–16</pages>
    <url>http://www.aclweb.org/anthology/W17-2202</url>
    <doi>10.18653/v1/W17-2202</doi>
    <abstract>
      This paper presents a newly funded international project for machine
      translation and automated analysis of ancient cuneiform languages where
      NLP specialists and Assyriologists collaborate to create an information
      retrieval system for Sumerian. This research is conceived in response to
      the need to translate large numbers of administrative texts that are only
      available in transcription, in order to make them accessible to a wider
      audience. The methodology includes creation of a specialized NLP pipeline
      and also the use of linguistic linked open data to increase access to the
      results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pageperron-EtAl:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2203'>
    <title>
      Investigating the Relationship between Literary Genres and Emotional Plot
      Development
    </title>
    <author>
      <first>Evgeny</first>
      <last>Kim</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Padó</last>
    </author>
    <author>
      <first>Roman</first>
      <last>Klinger</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–26</pages>
    <url>http://www.aclweb.org/anthology/W17-2203</url>
    <doi>10.18653/v1/W17-2203</doi>
    <abstract>
      Literary genres are commonly viewed as being defined in terms of content
      and stylistic features. In this paper, we focus on one particular class of
      lexical features, namely emotion information, and investigate the
      hypothesis that emotion-related information correlates with particular
      genres. Us- ing genre classification as a testbed, we compare a model that
      computes lexicon- based emotion scores globally for complete stories with
      a model that tracks emotion arcs through stories on a subset of Project
      Gutenberg with five genres. Our main findings are: (a), the global emotion
      model is competitive with a large-vocabulary bag-of-words genre classifier
      (80%F1); (b), the emotion arc model shows a lower performance (59 % F1)
      but shows complementary behavior to the global model, as indicated by a
      very good performance of an oracle model (94 % F1) and an improved
      performance of an ensemble model (84 % F1); (c), genres differ in the
      extent to which stories follow the same emotional arcs, with particularly
      uniform behavior for anger (mystery) and fear (ad- ventures, romance,
      humor, science fiction).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kim-pado-klinger:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2204'>
    <title>Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets</title>
    <author>
      <first>Pablo</first>
      <last>Ruiz</last>
    </author>
    <author>
      <first>Clara</first>
      <last>Martínez Cantón</last>
    </author>
    <author>
      <first>Thierry</first>
      <last>Poibeau</last>
    </author>
    <author>
      <first>Elena</first>
      <last>González-Blanco</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–32</pages>
    <url>http://www.aclweb.org/anthology/W17-2204</url>
    <doi>10.18653/v1/W17-2204</doi>
    <abstract>
      Enjambment takes place when a syntactic unit is broken up across two lines
      of poetry, giving rise to different stylistic effects. In Spanish literary
      studies, there are unclear points about the types of stylistic effects
      that can arise, and under which linguistic conditions. To systematically
      gather evidence about this, we developed a system to automatically
      identify enjambment (and its type) in Spanish. For evaluation, we manually
      annotated a reference corpus covering different periods. As a scholarly
      corpus to apply the tool, from public HTML sources we created a diachronic
      corpus covering four centuries of sonnets (3750 poems), and we analyzed
      the occurrence of enjambment across stanzaic boundaries in different
      periods. Besides, we found examples that highlight limitations in current
      definitions of enjambment.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ruiz-EtAl:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2205'>
    <title>Plotting Markson's "Mistress"</title>
    <author>
      <first>Conor</first>
      <last>Kelleher</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Keane</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33–39</pages>
    <url>http://www.aclweb.org/anthology/W17-2205</url>
    <doi>10.18653/v1/W17-2205</doi>
    <abstract>
      The post-modern novel "Wittgenstein’s Mistress" by David Markson (1988)
      presents the reader with a very challenging non-linear narrative, that
      itself appears to one of the novel’s themes. We present a distant reading
      of this work designed to complement a close reading of it by David Foster
      Wallace (1990). Using a combination of text analysis, entity recognition
      and networks, we plot repetitive structures in the novel’s narrative
      relating them to its critical analysis.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kelleher-keane:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2206'>
    <title>
      Annotation Challenges for Reconstructing the Structural Elaboration of
      Middle Low German
    </title>
    <author>
      <first>Nina</first>
      <last>Seemann</last>
    </author>
    <author>
      <first>Marie-Luis</first>
      <last>Merten</last>
    </author>
    <author>
      <first>Michaela</first>
      <last>Geierhos</last>
    </author>
    <author>
      <first>Doris</first>
      <last>Tophinke</last>
    </author>
    <author>
      <first>Eyke</first>
      <last>Hüllermeier</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>40–45</pages>
    <url>http://www.aclweb.org/anthology/W17-2206</url>
    <doi>10.18653/v1/W17-2206</doi>
    <abstract>
      In this paper, we present the annotation challenges we have encountered
      when working on a historical language that was undergoing elaboration
      processes. We especially focus on syntactic ambiguity and gradience in
      Middle Low German, which causes uncertainty to some extent. Since current
      annotation tools consider construction contexts and the dynamics of the
      grammaticalization only partially, we plan to extend CorA - a web-based
      annotation tool for historical and other non-standard language data - to
      capture elaboration phenomena and annotator unsureness. Moreover, we seek
      to interactively learn morphological as well as syntactic annotations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>seemann-EtAl:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2207'>
    <title>Phonological Soundscapes in Medieval Poetry</title>
    <author>
      <first>Christopher</first>
      <last>Hench</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–56</pages>
    <url>http://www.aclweb.org/anthology/W17-2207</url>
    <doi>10.18653/v1/W17-2207</doi>
    <abstract>
      The oral component of medieval poetry was integral to its performance and
      reception. Yet many believe that the medieval voice has been forever lost,
      and any attempts at rediscovering it are doomed to failure due to scribal
      practices, manuscript mouvance, and linguistic normalization in editing
      practices. This paper offers a method to abstract from this noise and
      better understand relative differences in phonological soundscapes by
      considering syllable qualities. The presented syllabification method and
      soundscape analysis offer themselves as cross-disciplinary tools for
      low-resource languages. As a case study, we examine medieval German lyric
      and argue that the heavily debated lyrical ‘I’ follows a unique trajectory
      through soundscapes, shedding light on the performance and practice of
      these poets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hench:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2208'>
    <title>
      An End-to-end Environment for Research Question-Driven Entity Extraction
      and Network Analysis
    </title>
    <author>
      <first>Andre</first>
      <last>Blessing</last>
    </author>
    <author>
      <first>Nora</first>
      <last>Echelmeyer</last>
    </author>
    <author>
      <first>Markus</first>
      <last>John</last>
    </author>
    <author>
      <first>Nils</first>
      <last>Reiter</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–67</pages>
    <url>http://www.aclweb.org/anthology/W17-2208</url>
    <doi>10.18653/v1/W17-2208</doi>
    <abstract>
      This paper presents an approach to extract co-occurrence networks from
      literary texts. It is a deliberate decision not to aim for a fully
      automatic pipeline, as the literary research questions need to guide both
      the definition of the nature of the things that co-occur as well as how to
      decide co-occurrence. We showcase the approach on a Middle High German
      romance, parz. Manual inspection and discussion shows the huge impact
      various choices have.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>blessing-EtAl:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2209'>
    <title>
      Modeling intra-textual variation with entropy and surprisal: topical vs.
      stylistic patterns
    </title>
    <author>
      <first>Stefania</first>
      <last>Degaetano-Ortlieb</last>
    </author>
    <author>
      <first>Elke</first>
      <last>Teich</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>68–77</pages>
    <url>http://www.aclweb.org/anthology/W17-2209</url>
    <doi>10.18653/v1/W17-2209</doi>
    <abstract>
      We present a data-driven approach to investigate intra-textual variation
      by combining entropy and surprisal. With this approach we detect
      linguistic variation based on phrasal lexico-grammatical patterns across
      sections of research articles. Entropy is used to detect patterns typical
      of specific sections. Surprisal is used to differentiate between more and
      less informationally-loaded patterns as well as type of information
      (topical vs. stylistic). While we here focus on research articles in
      biology/genetics, the methodology is especially interesting for digital
      humanities scholars, as it can be applied to any text type or domain and
      combined with additional variables (e.g. time, author or social group).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>degaetanoortlieb-teich:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2210'>
    <title>Finding a Character’s Voice: Stylome Classification on Literary Characters</title>
    <author>
      <first>Liviu P.</first>
      <last>Dinu</last>
    </author>
    <author>
      <first>Ana Sabina</first>
      <last>Uban</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>78–82</pages>
    <url>http://www.aclweb.org/anthology/W17-2210</url>
    <doi>10.18653/v1/W17-2210</doi>
    <abstract>
      We investigate in this paper the problem of classifying the stylome of
      characters in a literary work. Previous research in the field of
      authorship attribution has shown that the writing style of an author can
      be characterized and distinguished from that of other authors
      automatically. In this paper we take a look at the less approached problem
      of how the styles of different characters can be distinguished, trying to
      verify if an author managed to create believable characters with
      individual styles. We present the results of some initial experiments
      developed on the novel "Liaisons Dangereuses", showing that a simple bag
      of words model can be used to classify the characters.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dinu-uban:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2211'>
    <title>
      An Ontology-Based Method for Extracting and Classifying Domain-Specific
      Compositional Nominal Compounds
    </title>
    <author>
      <first>Maria Pia</first>
      <last>di Buono</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>83–88</pages>
    <url>http://www.aclweb.org/anthology/W17-2211</url>
    <doi>10.18653/v1/W17-2211</doi>
    <abstract>
      In this paper, we present our preliminary study on an ontology-based
      method to extract and classify compositional nominal compounds in specific
      domains of knowledge. This method is based on the assumption that,
      applying a conceptual model to represent knowledge domain, it is possible
      to improve the extraction and classification of lexicon occurrences for
      that domain in a semi-automatic way. We explore the possibility of
      extracting and classifying a specific construction type (nominal
      compounds) spanning a specific domain (Cultural Heritage) and a specific
      language (Italian).
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dibuono:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2212'>
    <title>
      Speeding up corpus development for linguistic research: language
      documentation and acquisition in Romansh Tuatschin
    </title>
    <author>
      <first>Géraldine</first>
      <last>Walther</last>
    </author>
    <author>
      <first>Benoît</first>
      <last>Sagot</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>89–94</pages>
    <url>http://www.aclweb.org/anthology/W17-2212</url>
    <doi>10.18653/v1/W17-2212</doi>
    <abstract>
      In this paper, we present ongoing work for developing language resources
      and basic NLP tools for an undocumented variety of Romansh, in the context
      of a language documentation and language acquisition project. Our tools
      are meant to improve the speed and reliability of corpus annotations for
      noisy data involving large amounts of code-switching, occurrences of
      child-speech and orthographic noise. Being able to increase the efficiency
      of language resource development for language documentation and
      acquisition research also constitutes a step towards solving the data
      sparsity issues with which researchers have been struggling.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>walther-sagot:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2213'>
    <title>
      Distantly Supervised POS Tagging of Low-Resource Languages under Extreme
      Data Sparsity: The Case of Hittite
    </title>
    <author>
      <first>Maria</first>
      <last>Sukhareva</last>
    </author>
    <author>
      <first>Francesco</first>
      <last>Fuscagni</last>
    </author>
    <author>
      <first>Johannes</first>
      <last>Daxenberger</last>
    </author>
    <author>
      <first>Susanne</first>
      <last>Görke</last>
    </author>
    <author>
      <first>Doris</first>
      <last>Prechel</last>
    </author>
    <author>
      <first>Iryna</first>
      <last>Gurevych</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>95–104</pages>
    <url>http://www.aclweb.org/anthology/W17-2213</url>
    <doi>10.18653/v1/W17-2213</doi>
    <abstract>
      This paper presents a statistical approach to automatic morphosyntactic
      annotation of Hittite transcripts. Hittite is an extinct Indo-European
      language using the cuneiform script. There are currently no
      morphosyntactic annotations available for Hittite, so we explored methods
      of distant supervision. The annotations were projected from parallel
      German translations of the Hittite texts. In order to reduce data
      sparsity, we applied stemming of German and Hittite texts. As there is no
      off-the-shelf Hittite stemmer, a stemmer for Hittite was developed for
      this purpose. The resulting annotation projections were used to train a
      POS tagger, achieving an accuracy of 69% on a test sample. To our
      knowledge, this is the first attempt of statistical POS tagging of a
      cuneiform language.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sukhareva-EtAl:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2214'>
    <title>A Dataset for Sanskrit Word Segmentation</title>
    <author>
      <first>Amrith</first>
      <last>Krishna</last>
    </author>
    <author>
      <first>Pavan Kumar</first>
      <last>Satuluri</last>
    </author>
    <author>
      <first>Pawan</first>
      <last>Goyal</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>105–114</pages>
    <url>http://www.aclweb.org/anthology/W17-2214</url>
    <doi>10.18653/v1/W17-2214</doi>
    <abstract>
      The last decade saw a surge in digitisation efforts for ancient
      manuscripts in Sanskrit. Due to various linguistic peculiarities inherent
      to the language, even the preliminary tasks such as word segmentation are
      non-trivial in Sanskrit. Elegant models for Word Segmentation in Sanskrit
      are indispensable for further syntactic and semantic processing of the
      manuscripts. Current works in word segmentation for Sanskrit, though
      commendable in their novelty, often have variations in their objective and
      evaluation criteria. In this work, we set the record straight. We formally
      define the objectives and the requirements for the word segmentation task.
      In order to encourage research in the field and to alleviate the time and
      effort required in pre-processing, we release a dataset of 115,000
      sentences for word segmentation. For each sentence in the dataset we
      include the input character sequence, ground truth segmentation, and
      additionally lexical and morphological information about all the
      phonetically possible segments for the given sentence. In this work, we
      also discuss the linguistic considerations made while generating the
      candidate space of the possible segments.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>krishna-satuluri-goyal:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2215'>
    <title>Lexical Correction of Polish Twitter Political Data</title>
    <author>
      <first>Maciej</first>
      <last>Ogrodniczuk</last>
    </author>
    <author>
      <first>Mateusz</first>
      <last>Kopeć</last>
    </author>
    <booktitle>
      Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for
      Cultural Heritage, Social Sciences, Humanities and Literature
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115–125</pages>
    <url>http://www.aclweb.org/anthology/W17-2215</url>
    <doi>10.18653/v1/W17-2215</doi>
    <abstract>
      Language processing architectures are often evaluated in near-to-perfect
      conditions with respect to processed content. The tools which perform
      sufficiently well on electronic press, books and other type of
      non-interactive content may poorly handle littered, colloquial and
      multilingual textual data which make the majority of communication today.
      This paper aims at investigating how Polish Twitter data (in a slightly
      controlled `political' flavour) differs from expectation of linguistic
      tools and how they could be corrected to be ready for processing by
      standard language processing chains available for Polish. The setting
      includes specialised components for spelling correction of tweets as well
      as hashtag and username decoding.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ogrodniczuk-kopec:2017:LaTeCH-CLfL</bibkey>
  </paper>
  <paper id='2300'>
    <title>BioNLP 2017</title>
    <editor>Kevin Bretonnel Cohen</editor>
    <editor>Dina Demner-Fushman</editor>
    <editor>Sophia Ananiadou</editor>
    <editor>Junichi Tsujii</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-23</url>
    <doi>10.18653/v1/W17-23</doi>
    <bibtype>book</bibtype>
    <bibkey>BioNLP17:2017</bibkey>
  </paper>
  <paper id='2301'>
    <title>Target word prediction and paraphasia classification in spoken discourse</title>
    <author>
      <first>Joel</first>
      <last>Adams</last>
    </author>
    <author>
      <first>Steven</first>
      <last>Bedrick</last>
    </author>
    <author>
      <first>Gerasimos</first>
      <last>Fergadiotis</last>
    </author>
    <author>
      <first>Kyle</first>
      <last>Gorman</last>
    </author>
    <author>
      <first>Jan</first>
      <last>van Santen</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–8</pages>
    <url>http://www.aclweb.org/anthology/W17-2301</url>
    <doi>10.18653/v1/W17-2301</doi>
    <abstract>
      We present a system for automatically detecting and classifying
      phonologically anomalous productions in the speech of individuals with
      aphasia. Working from transcribed discourse samples, our system identifies
      neologisms, and uses a combination of string alignment and language models
      to produce a lattice of plausible words that the speaker may have intended
      to produce. We then score this lattice according to various features, and
      attempt to determine whether the anomalous production represented a
      phonemic error or a genuine neologism. This approach has the potential to
      be expanded to consider other types of paraphasic errors, and could be
      applied to a wide variety of screening and therapeutic applications.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>adams-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2302'>
    <title>Extracting Drug-Drug Interactions with Attention CNNs</title>
    <author>
      <first>Masaki</first>
      <last>Asada</last>
    </author>
    <author>
      <first>Makoto</first>
      <last>Miwa</last>
    </author>
    <author>
      <first>Yutaka</first>
      <last>Sasaki</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>9–18</pages>
    <url>http://www.aclweb.org/anthology/W17-2302</url>
    <doi>10.18653/v1/W17-2302</doi>
    <abstract>
      We propose a novel attention mechanism for a Convolutional Neural Network
      (CNN)-based Drug-Drug Interaction (DDI) extraction model. CNNs have been
      shown to have a great potential on DDI extraction tasks; however,
      attention mechanisms, which emphasize important words in the sentence of a
      target-entity pair, have not been investigated with the CNNs despite the
      fact that attention mechanisms are shown to be effective for a general
      domain relation classification task. We evaluated our model on the Task
      9.2 of the DDIExtraction-2013 shared task. As a result, our attention
      mechanism improved the performance of our base CNN-based DDI model, and
      the model achieved an F-score of 69.12%, which is competitive with the
      state-of-the-art models.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>asada-miwa-sasaki:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2303'>
    <title>Insights into Analogy Completion from the Biomedical Domain</title>
    <author>
      <first>Denis</first>
      <last>Newman-Griffis</last>
    </author>
    <author>
      <first>Albert</first>
      <last>Lai</last>
    </author>
    <author>
      <first>Eric</first>
      <last>Fosler-Lussier</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>19–28</pages>
    <url>http://www.aclweb.org/anthology/W17-2303</url>
    <doi>10.18653/v1/W17-2303</doi>
    <abstract>
      Analogy completion has been a popular task in recent years for evaluating
      the semantic properties of word embeddings, but the standard methodology
      makes a number of assumptions about analogies that do not always hold,
      either in recent benchmark datasets or when expanding into other domains.
      Through an analysis of analogies in the biomedical domain, we identify
      three assumptions: that of a Single Answer for any given analogy, that the
      pairs involved describe the Same Relationship, and that each pair is
      Informative with respect to the other. We propose modifying the standard
      methodology to relax these assumptions by allowing for multiple correct
      answers, reporting MAP and MRR in addition to accuracy, and using multiple
      example pairs. We further present BMASS, a novel dataset for evaluating
      linguistic regularities in biomedical embeddings, and demonstrate that the
      relationships described in the dataset pose significant semantic
      challenges to current word embedding methods.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>newmangriffis-lai-foslerlussier:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2304'>
    <title>
      Deep learning for extracting protein-protein interactions from biomedical
      literature
    </title>
    <author>
      <first>Yifan</first>
      <last>Peng</last>
    </author>
    <author>
      <first>Zhiyong</first>
      <last>Lu</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29–38</pages>
    <url>http://www.aclweb.org/anthology/W17-2304</url>
    <doi>10.18653/v1/W17-2304</doi>
    <abstract>
      State-of-the-art methods for protein-protein interaction (PPI) extraction
      are primarily feature-based or kernel-based by leveraging lexical and
      syntactic information. But how to incorporate such knowledge in the recent
      deep learning methods remains an open question. In this paper, we propose
      a multichannel dependency-based convolutional neural network model
      (McDepCNN). It applies one channel to the embedding vector of each word in
      the sentence, and another channel to the embedding vector of the head of
      the corresponding word. Therefore, the model can use richer information
      obtained from different channels. Experiments on two public benchmarking
      datasets, AIMed and BioInfer, demonstrate that McDepCNN provides up to 6%
      F1-score improvement over rich feature-based methods and single-kernel
      methods. In addition, McDepCNN achieves 24.4% relative improvement in
      F1-score over the state-of-the-art methods on cross-corpus evaluation and
      12% improvement in F1-score over kernel-based methods on "difficult"
      instances. These results suggest that McDepCNN generalizes more easily
      over different corpora, and is capable of capturing long distance features
      in the sentences.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>peng-lu:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2305'>
    <title>Stacking With Auxiliary Features for Entity Linking in the Medical Domain</title>
    <author>
      <first>Nazneen Fatema</first>
      <last>Rajani</last>
    </author>
    <author>
      <first>Mihaela</first>
      <last>Bornea</last>
    </author>
    <author>
      <first>Ken</first>
      <last>Barker</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–47</pages>
    <url>http://www.aclweb.org/anthology/W17-2305</url>
    <doi>10.18653/v1/W17-2305</doi>
    <abstract>
      Linking spans of natural language text to concepts in a structured source
      is an important task for many problems. It allows intelligent systems to
      leverage rich knowledge available in those sources (such as concept
      properties and relations) to enhance the semantics of the mentions of
      these concepts in text. In the medical domain, it is common to link text
      spans to medical concepts in large, curated knowledge repositories such as
      the Unified Medical Language System. Different approaches have different
      strengths: some are precision-oriented, some recall-oriented; some better
      at considering context but more prone to hallucination. The variety of
      techniques suggests that ensembling could outperform component
      technologies at this task. In this paper, we describe our process for
      building a Stacking ensemble using additional, auxiliary features for
      Entity Linking in the medical domain. We report experiments that show that
      naive ensembling does not always outperform component Entity Linking
      systems, that stacking usually outperforms naive ensembling, and that
      auxiliary features added to the stacker further improve its performance on
      three distinct datasets. Our best model produces state-of-the-art results
      on several medical datasets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rajani-bornea-barker:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2306'>
    <title>Results of the fifth edition of the BioASQ Challenge</title>
    <author>
      <first>Anastasios</first>
      <last>Nentidis</last>
    </author>
    <author>
      <first>Konstantinos</first>
      <last>Bougiatiotis</last>
    </author>
    <author>
      <first>Anastasia</first>
      <last>Krithara</last>
    </author>
    <author>
      <first>Georgios</first>
      <last>Paliouras</last>
    </author>
    <author>
      <first>Ioannis</first>
      <last>Kakadiaris</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–57</pages>
    <url>http://www.aclweb.org/anthology/W17-2306</url>
    <doi>10.18653/v1/W17-2306</doi>
    <abstract>
      The goal of the BioASQ challenge is to engage researchers into creating
      cuttingedge biomedical information systems. Specifically, it aims at the
      promotion of systems and methodologies that are able to deal with a
      plethora of different tasks in the biomedical domain. This is achieved
      through the organization of challenges. The fifth challenge consisted of
      three tasks: semantic indexing, question answering and a new task on
      information extraction. In total, 29 teams with more than 95 systems
      participated in the challenge. Overall, as in previous years, the best
      systems were able to outperform the strong baselines. This suggests that
      stateof- the art systems are continuously improving, pushing the frontier
      of research.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nentidis-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2307'>
    <title>Tackling Biomedical Text Summarization: OAQA at BioASQ 5B</title>
    <author>
      <first>Khyathi</first>
      <last>Chandu</last>
    </author>
    <author>
      <first>Aakanksha</first>
      <last>Naik</last>
    </author>
    <author>
      <first>Aditya</first>
      <last>Chandrasekar</last>
    </author>
    <author>
      <first>Zi</first>
      <last>Yang</last>
    </author>
    <author>
      <first>Niloy</first>
      <last>Gupta</last>
    </author>
    <author>
      <first>Eric</first>
      <last>Nyberg</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>58–66</pages>
    <url>http://www.aclweb.org/anthology/W17-2307</url>
    <doi>10.18653/v1/W17-2307</doi>
    <abstract>
      In this paper, we describe our participation in phase B of task 5b of the
      fifth edition of the annual BioASQ challenge, which includes answering
      factoid, list, yes-no and summary questions from biomedical data. We
      describe our techniques with an emphasis on ideal answer generation, where
      the goal is to produce a relevant, precise, non-redundant, query-oriented
      summary from multiple relevant documents. We make use of extractive
      summarization techniques to address this task and experiment with
      different biomedical ontologies and various algorithms including
      agglomerative clustering, Maximum Marginal Relevance (MMR) and sentence
      compression. We propose a novel word embedding based tf-idf similarity
      metric and a soft positional constraint which improve our system
      performance. We evaluate our techniques on test batch 4 from the fourth
      edition of the challenge. Our best system achieves a ROUGE-2 score of
      0.6534 and ROUGE-SU4 score of 0.6536.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chandu-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2308'>
    <title>
      Macquarie University at BioASQ 5b – Query-based Summarisation Techniques
      for Selecting the Ideal Answers
    </title>
    <author>
      <first>Diego</first>
      <last>Molla</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <url>http://www.aclweb.org/anthology/W17-2308</url>
    <doi>10.18653/v1/W17-2308</doi>
    <abstract>
      Macquarie University's contribution to the BioASQ challenge (Task 5b Phase
      B) focused on the use of query-based extractive summarisation techniques
      for the generation of the ideal answers. Four runs were submitted, with
      approaches ranging from a trivial system that selected the first $n$
      snippets, to the use of deep learning approaches under a regression
      framework. Our experiments and the ROUGE results of the five test batches
      of BioASQ indicate surprisingly good results for the trivial approach.
      Overall, most of our runs on the first three test batches achieved the
      best ROUGE-SU4 results in the challenge.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>molla:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2309'>
    <title>Neural Question Answering at BioASQ 5B</title>
    <author>
      <first>Georg</first>
      <last>Wiese</last>
    </author>
    <author>
      <first>Dirk</first>
      <last>Weissenborn</last>
    </author>
    <author>
      <first>Mariana</first>
      <last>Neves</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–79</pages>
    <url>http://www.aclweb.org/anthology/W17-2309</url>
    <doi>10.18653/v1/W17-2309</doi>
    <abstract>
      This paper describes our submission to the 2017 BioASQ challenge. We
      participated in Task B, Phase B which is concerned with biomedical
      question answering (QA). We focus on factoid and list question, using an
      extractive QA model, that is, we restrict our system to output substrings
      of the provided text snippets. At the core of our system, we use FastQA, a
      state-of-the-art neural QA system. We extended it with biomedical word
      embeddings and changed its answer layer to be able to answer list
      questions in addition to factoid questions. We pre-trained the model on a
      large-scale open-domain QA dataset, SQuAD, and then fine-tuned the
      parameters on the BioASQ training set. With our approach, we achieve
      state-of-the-art results on factoid questions and competitive results on
      list questions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wiese-weissenborn-neves:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2310'>
    <title>End-to-End System for Bacteria Habitat Extraction</title>
    <author>
      <first>Farrokh</first>
      <last>Mehryary</last>
    </author>
    <author>
      <first>Kai</first>
      <last>Hakala</last>
    </author>
    <author>
      <first>Suwisa</first>
      <last>Kaewphan</last>
    </author>
    <author>
      <first>Jari</first>
      <last>Björne</last>
    </author>
    <author>
      <first>Tapio</first>
      <last>Salakoski</last>
    </author>
    <author>
      <first>Filip</first>
      <last>Ginter</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>80–90</pages>
    <url>http://www.aclweb.org/anthology/W17-2310</url>
    <doi>10.18653/v1/W17-2310</doi>
    <abstract>
      We introduce an end-to-end system capable of named-entity detection,
      normalization and relation extraction for extracting information about
      bacteria and their habitats from biomedical literature. Our system is
      based on deep learning, CRF classifiers and vector space models. We train
      and evaluate the system on the BioNLP 2016 Shared Task Bacteria Biotope
      data. The official evaluation shows that the joint performance of our
      entity detection and relation extraction models outperforms the winning
      team of the Shared Task by 19pp on F1-score, establishing a new top score
      for the task. We also achieve state-of-the-art results in the
      normalization task. Our system is open source and freely available at
      https://github.com/TurkuNLP/BHE.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mehryary-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2311'>
    <title>
      Creation and evaluation of a dictionary-based tagger for virus species and
      proteins
    </title>
    <author>
      <first>Helen</first>
      <last>Cook</last>
    </author>
    <author>
      <first>Rudolfs</first>
      <last>Berzins</last>
    </author>
    <author>
      <first>Cristina Leal</first>
      <last>Rodrıguez</last>
    </author>
    <author>
      <first>Juan Miguel</first>
      <last>Cejuela</last>
    </author>
    <author>
      <first>Lars Juhl</first>
      <last>Jensen</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–98</pages>
    <url>http://www.aclweb.org/anthology/W17-2311</url>
    <doi>10.18653/v1/W17-2311</doi>
    <abstract>
      ext mining automatically extracts information from the literature with the
      goal of making it available for further analysis, for example by
      incorporating it into biomedical databases. A key first step towards this
      goal is to identify and normalize the named entities, such as proteins and
      species, which are mentioned in text. Despite the large detrimental impact
      that viruses have on human and agricultural health, very little previous
      text-mining work has focused on identifying virus species and proteins in
      the literature. Here, we present an improved dictionary-based system for
      viral species and the first dictionary for viral proteins, which we
      benchmark on a new corpus of 300 manually annotated abstracts. We achieve
      81.0% precision and 72.7% recall at the task of recognizing and
      normalizing viral species and 76.2% precision and 34.9% recall on viral
      proteins. These results are achieved despite the many challenges involved
      with the names of viral species and, especially, proteins. This work
      provides a foundation that can be used to extract more complicated
      relations about viruses from the literature.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cook-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2312'>
    <title>
      Representation of complex terms in a vector space structured by an
      ontology for a normalization task
    </title>
    <author>
      <first>Arnaud</first>
      <last>Ferré</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Zweigenbaum</last>
    </author>
    <author>
      <first>Claire</first>
      <last>Nédellec</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>99–106</pages>
    <url>http://www.aclweb.org/anthology/W17-2312</url>
    <doi>10.18653/v1/W17-2312</doi>
    <abstract>
      We propose in this paper a semi-supervised method for labeling terms of
      texts with concepts of a domain ontology. The method generates continuous
      vector representations of complex terms in a semantic space structured by
      the ontology. The proposed method relies on a distributional semantics
      approach, which generates initial vectors for each of the extracted terms.
      Then these vectors are embedded in the vector space constructed from the
      structure of the ontology. This embedding is carried out by training a
      linear model. Finally, we apply a distance calculation to determine the
      proximity between vectors of terms and vectors of concepts and thus to
      assign ontology labels to terms. We have evaluated the quality of these
      representations for a normalization task by using the concepts of an
      ontology as semantic labels. Normalization of terms is an important step
      to extract a part of the information containing in texts, but the vector
      space generated might find other applications. The performance of this
      method is comparable to that of the state of the art for this task of
      standardization, opening up encouraging prospects.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ferre-zweigenbaum-nedellec:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2313'>
    <title>
      Improving Correlation with Human Judgments by Integrating Semantic
      Similarity with Second–Order Vectors
    </title>
    <author>
      <first>Bridget</first>
      <last>McInnes</last>
    </author>
    <author>
      <first>Ted</first>
      <last>Pedersen</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>107–116</pages>
    <url>http://www.aclweb.org/anthology/W17-2313</url>
    <doi>10.18653/v1/W17-2313</doi>
    <abstract>
      Vector space methods that measure semantic similarity and relatedness
      often rely on distributional information such as co–occurrence frequencies
      or statistical measures of association to weight the importance of
      particular co–occurrences. In this paper, we extend these methods by
      incorporating a measure of semantic similarity based on a human curated
      taxonomy into a second–order vector representation. This results in a
      measure of semantic relatedness that combines both the contextual
      information available in a corpus–based vector space representation with
      the semantic knowledge found in a biomedical ontology. Our results show
      that incorporating semantic similarity into a second order co-occurrence
      matrices improves correlation with human judgments for both similarity and
      relatedness, and that our method compares favorably to various different
      word embedding methods that have recently been evaluated on the same
      reference standards we have used.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mcinnes-pedersen:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2314'>
    <title>Proactive Learning for Named Entity Recognition</title>
    <author>
      <first>Maolin</first>
      <last>Li</last>
    </author>
    <author>
      <first>Nhung</first>
      <last>Nguyen</last>
    </author>
    <author>
      <first>Sophia</first>
      <last>Ananiadou</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>117–125</pages>
    <url>http://www.aclweb.org/anthology/W17-2314</url>
    <doi>10.18653/v1/W17-2314</doi>
    <abstract>
      The goal of active learning is to minimise the cost of producing an
      annotated dataset, in which annotators are assumed to be perfect, i.e.,
      they always choose the correct labels. However, in practice, annotators
      are not infallible, and they are likely to assign incorrect labels to some
      instances. Proactive learning is a generalisation of active learning that
      can model different kinds of annotators. Although proactive learning has
      been applied to certain labelling tasks, such as text classification,
      there is little work on its application to named entity (NE) tagging. In
      this paper, we propose a proactive learning method for producing NE
      annotated corpora, using two annotators with different levels of
      expertise, and who charge different amounts based on their levels of
      experience. To optimise both cost and annotation quality, we also propose
      a mechanism to present multiple sentences to annotators at each iteration.
      Experimental results for several corpora show that our method facilitates
      the construction of high-quality NE labelled datasets at minimal cost.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-nguyen-ananiadou:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2315'>
    <title>Biomedical Event Extraction using Abstract Meaning Representation</title>
    <author>
      <first>Sudha</first>
      <last>Rao</last>
    </author>
    <author>
      <first>Daniel</first>
      <last>Marcu</last>
    </author>
    <author>
      <first>Kevin</first>
      <last>Knight</last>
    </author>
    <author>
      <first>Hal</first>
      <last>Daumé III</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>126–135</pages>
    <url>http://www.aclweb.org/anthology/W17-2315</url>
    <doi>10.18653/v1/W17-2315</doi>
    <abstract>
      We propose a novel, Abstract Meaning Representation (AMR) based approach
      to identifying molecular events/interactions in biomedical text. Our key
      contributions are: (1) an empirical validation of our hypothesis that an
      event is a subgraph of the AMR graph, (2) a neural network-based model
      that identifies such an event subgraph given an AMR, and (3) a distant
      supervision based approach to gather additional training data. We evaluate
      our approach on the 2013 Genia Event Extraction dataset and show promising
      results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rao-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2316'>
    <title>
      Detecting Personal Medication Intake in Twitter: An Annotated Corpus and
      Baseline Classification System
    </title>
    <author>
      <first>Ari</first>
      <last>Klein</last>
    </author>
    <author>
      <first>Abeed</first>
      <last>Sarker</last>
    </author>
    <author>
      <first>Masoud</first>
      <last>Rouhizadeh</last>
    </author>
    <author>
      <first>Karen</first>
      <last>O'Connor</last>
    </author>
    <author>
      <first>Graciela</first>
      <last>Gonzalez</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>136–142</pages>
    <url>http://www.aclweb.org/anthology/W17-2316</url>
    <doi>10.18653/v1/W17-2316</doi>
    <abstract>
      Social media sites (e.g., Twitter) have been used for surveillance of drug
      safety at the population level, but studies that focus on the effects of
      medications on specific sets of individuals have had to rely on other
      sources of data. Mining social media data for this in-formation would
      require the ability to distinguish indications of personal medication
      in-take in this media. Towards that end, this paper presents an annotated
      corpus that can be used to train machine learning systems to determine
      whether a tweet that mentions a medication indicates that the individual
      posting has taken that medication at a specific time. To demonstrate the
      utility of the corpus as a training set, we present baseline results of
      supervised classification.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>klein-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2317'>
    <title>
      Unsupervised Context-Sensitive Spelling Correction of Clinical Free-Text
      with Word and Character N-Gram Embeddings
    </title>
    <author>
      <first>Pieter</first>
      <last>Fivez</last>
    </author>
    <author>
      <first>Simon</first>
      <last>Suster</last>
    </author>
    <author>
      <first>Walter</first>
      <last>Daelemans</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>143–148</pages>
    <url>http://www.aclweb.org/anthology/W17-2317</url>
    <doi>10.18653/v1/W17-2317</doi>
    <abstract>
      We present an unsupervised context-sensitive spelling correction method
      for clinical free-text that uses word and character n-gram embeddings. Our
      method generates misspelling replacement candidates and ranks them
      according to their semantic fit, by calculating a weighted cosine
      similarity between the vectorized representation of a candidate and the
      misspelling context. We greatly outperform two baseline off-the-shelf
      spelling correction tools on a manually annotated MIMIC-III test set, and
      counter the frequency bias of an optimized noisy channel model, showing
      that neural embeddings can be successfully exploited to include
      context-awareness in a spelling correction model.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fivez-suster-daelemans:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2318'>
    <title>Characterization of Divergence in Impaired Speech of ALS Patients</title>
    <author>
      <first>Archna</first>
      <last>Bhatia</last>
    </author>
    <author>
      <first>Bonnie</first>
      <last>Dorr</last>
    </author>
    <author>
      <first>Kristy</first>
      <last>Hollingshead</last>
    </author>
    <author>
      <first>Samuel L.</first>
      <last>Phillips</last>
    </author>
    <author>
      <first>Barbara</first>
      <last>McKenzie</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>149–158</pages>
    <url>http://www.aclweb.org/anthology/W17-2318</url>
    <doi>10.18653/v1/W17-2318</doi>
    <abstract>
      Approximately 80% to 95% of patients with Amyotrophic Lateral Sclerosis
      (ALS) eventually develop speech impairments, such as defective
      articulation, slow laborious speech and hypernasality. The relationship
      between impaired speech and asymptomatic speech may be seen as a
      divergence from a baseline. This relationship can be characterized in
      terms of measurable combinations of phonological characteristics that are
      indicative of the degree to which the two diverge. We demonstrate that
      divergence measurements based on phonological characteristics of speech
      correlate with physiological assessments of ALS. Speech-based assessments
      offer benefits over commonly-used physiological assessments in that they
      are inexpensive, non-intrusive, and do not require trained clinical
      personnel for administering and interpreting the results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bhatia-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2319'>
    <title>Deep Learning for Punctuation Restoration in Medical Reports</title>
    <author>
      <first>Wael</first>
      <last>Salloum</last>
    </author>
    <author>
      <first>Greg</first>
      <last>Finley</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Edwards</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Miller</last>
    </author>
    <author>
      <first>David</first>
      <last>Suendermann-Oeft</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>159–164</pages>
    <url>http://www.aclweb.org/anthology/W17-2319</url>
    <doi>10.18653/v1/W17-2319</doi>
    <abstract>
      In clinical dictation, speakers try to be as concise as possible to save
      time, often resulting in utterances without explicit punctuation commands.
      Since the end product of a dictated report, e.g. an out-patient letter,
      does require correct orthography, including exact punctuation, the latter
      need to be restored, preferably by automated means. This paper describes a
      method for punctuation restoration based on a state-of-the-art stack of
      NLP and machine learning techniques including B-RNNs with an attention
      mechanism and late fusion, as well as a feature extraction technique
      tailored to the processing of medical terminology using a novel vocabulary
      reduction model. To the best of our knowledge, the resulting performance
      is superior to that reported in prior art on similar tasks.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>salloum-EtAl:2017:BioNLP171</bibkey>
  </paper>
  <paper id='2320'>
    <title>Unsupervised Domain Adaptation for Clinical Negation Detection</title>
    <author>
      <first>Timothy</first>
      <last>Miller</last>
    </author>
    <author>
      <first>Steven</first>
      <last>Bethard</last>
    </author>
    <author>
      <first>Hadi</first>
      <last>Amiri</last>
    </author>
    <author>
      <first>Guergana</first>
      <last>Savova</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>165–170</pages>
    <url>http://www.aclweb.org/anthology/W17-2320</url>
    <doi>10.18653/v1/W17-2320</doi>
    <abstract>
      Detecting negated concepts in clinical texts is an important part of NLP
      information extraction systems. However, generalizability of negation
      systems is lacking, as cross-domain experiments suffer dramatic
      performance losses. We examine the performance of multiple unsupervised
      domain adaptation algorithms on clinical negation detection, finding only
      modest gains that fall well short of in-domain performance.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>miller-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2321'>
    <title>
      BioCreative VI Precision Medicine Track: creating a training corpus for
      mining protein-protein interactions affected by mutations
    </title>
    <author>
      <first>Rezarta</first>
      <last>Islamaj Dogan</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Chatr-aryamontri</last>
    </author>
    <author>
      <first>Sun</first>
      <last>Kim</last>
    </author>
    <author>
      <first>Chih-Hsuan</first>
      <last>Wei</last>
    </author>
    <author>
      <first>Yifan</first>
      <last>Peng</last>
    </author>
    <author>
      <first>Donald</first>
      <last>Comeau</last>
    </author>
    <author>
      <first>Zhiyong</first>
      <last>Lu</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>171–175</pages>
    <url>http://www.aclweb.org/anthology/W17-2321</url>
    <doi>10.18653/v1/W17-2321</doi>
    <abstract>
      The Precision Medicine Track in BioCre-ative VI aims to bring together the
      Bi-oNLP community for a novel challenge focused on mining the biomedical
      litera-ture in search of mutations and protein-protein interactions (PPI).
      In order to support this track with an effective train-ing dataset with
      limited curator time, the track organizers carefully reviewed Pub-Med
      articles from two different sources: curated public PPI databases, and the
      re-sults of state-of-the-art public text mining tools. We detail here the
      data collection, manual review and annotation process and describe this
      training corpus charac-teristics. We also describe a corpus per-formance
      baseline. This analysis will provide useful information to developers and
      researchers for comparing and devel-oping innovative text mining
      approaches for the BioCreative VI challenge and other Precision Medicine
      related applica-tions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>islamajdogan-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2322'>
    <title>Painless Relation Extraction with Kindred</title>
    <author>
      <first>Jake</first>
      <last>Lever</last>
    </author>
    <author>
      <first>Steven</first>
      <last>Jones</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>176–183</pages>
    <url>http://www.aclweb.org/anthology/W17-2322</url>
    <doi>10.18653/v1/W17-2322</doi>
    <abstract>
      Relation extraction methods are essential for creating robust text mining
      tools to help researchers find useful knowledge in the vast published
      literature. Easy-to-use and generalizable methods are needed to encourage
      an ecosystem in which researchers can easily use shared resources and
      build upon each others' methods. We present the Kindred Python package for
      relation extraction. It builds upon methods from the most successful tools
      in the recent BioNLP Shared Task to predict high-quality predictions with
      low computational cost. It also integrates with PubAnnotation, PubTator,
      and BioNLP Shared Task data in order to allow easy development and
      application of relation extraction models.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lever-jones:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2323'>
    <title>Noise Reduction Methods for Distantly Supervised Biomedical Relation Extraction</title>
    <author>
      <first>Gang</first>
      <last>Li</last>
    </author>
    <author>
      <first>Cathy</first>
      <last>Wu</last>
    </author>
    <author>
      <first>K.</first>
      <last>Vijay-Shanker</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184–193</pages>
    <url>http://www.aclweb.org/anthology/W17-2323</url>
    <doi>10.18653/v1/W17-2323</doi>
    <abstract>
      Distant supervision has been applied to automatically generate labeled
      data for biomedical relation extraction. Noise exists in both positively
      and negatively-labeled data and affects the performance of supervised
      machine learning methods. In this paper, we propose three novel heuristics
      based on the notion of proximity, trigger word and confidence of patterns
      to leverage lexical and syntactic information to reduce the level of noise
      in the distantly labeled data. Experiments on three different tasks,
      extraction of protein-protein-interaction, miRNA-gene regulation relation
      and protein-localization event, show that the proposed methods can improve
      the F-score over the baseline by 6, 10 and 14 points for the three tasks,
      respectively. We also show that when the models are configured to output
      high-confidence results, high precisions can be obtained using the
      proposed methods, making them promising for facilitating manual curation
      for databases.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-wu-vijayshanker:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2324'>
    <title>
      Role-Preserving Redaction of Medical Records to Enable Ontology-Driven
      Processing
    </title>
    <author>
      <first>Seth</first>
      <last>Polsley</last>
    </author>
    <author>
      <first>Atif</first>
      <last>Tahir</last>
    </author>
    <author>
      <first>Muppala</first>
      <last>Raju</last>
    </author>
    <author>
      <first>Akintayo</first>
      <last>Akinleye</last>
    </author>
    <author>
      <first>Duane</first>
      <last>Steward</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>194–199</pages>
    <url>http://www.aclweb.org/anthology/W17-2324</url>
    <doi>10.18653/v1/W17-2324</doi>
    <abstract>
      Electronic medical records (EMR) have largely replaced hand-written
      patient files in healthcare. The growing pool of EMR data presents a
      significant resource in medical research, but the U.S. Health Insurance
      Portability and Accountability Act (HIPAA) mandates redacting medical
      records before performing any analysis on the same. This process
      complicates obtaining medical data and can remove much useful information
      from the record. As part of a larger project involving ontology-driven
      medical processing, we employ a method of recognizing protected health
      information (PHI) that maps to ontological terms. We then use the
      relationships defined in the ontology to redact medical texts so that
      roles and semantics of terms are retained without compromising anonymity.
      The method is evaluated by clinical experts on several hundred medical
      documents, achieving up to a 98.8% f-score, and has already shown promise
      for retaining semantic information in later processing.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>polsley-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2325'>
    <title>
      Annotation of pain and anesthesia events for surgery-related processes and
      outcomes extraction
    </title>
    <author>
      <first>Wen-wai</first>
      <last>Yim</last>
    </author>
    <author>
      <first>Dario</first>
      <last>Tedesco</last>
    </author>
    <author>
      <first>Catherine</first>
      <last>Curtin</last>
    </author>
    <author>
      <first>Tina</first>
      <last>Hernandez-Boussard</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200–205</pages>
    <url>http://www.aclweb.org/anthology/W17-2325</url>
    <doi>10.18653/v1/W17-2325</doi>
    <abstract>
      Pain and anesthesia information are crucial elements to identifying
      surgery-related processes and outcomes. However pain is not consistently
      recorded in the electronic medical record. Even when recorded, the rich
      complex granularity of the pain experience may be lost. Similarly,
      anesthesia information is recorded using local electronic collection
      systems; though the accuracy and completeness of the information is
      unknown. We propose an annotation schema to capture pain, pain management,
      and anesthesia event information.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yim-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2326'>
    <title>Identifying Comparative Structures in Biomedical Text</title>
    <author>
      <first>Samir</first>
      <last>Gupta</last>
    </author>
    <author>
      <first>A.S.M. Ashique</first>
      <last>Mahmood</last>
    </author>
    <author>
      <first>Karen</first>
      <last>Ross</last>
    </author>
    <author>
      <first>Cathy</first>
      <last>Wu</last>
    </author>
    <author>
      <first>K.</first>
      <last>Vijay-Shanker</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>206–215</pages>
    <url>http://www.aclweb.org/anthology/W17-2326</url>
    <doi>10.18653/v1/W17-2326</doi>
    <abstract>
      Comparison sentences are very commonly used by authors in biomedical
      literature to report results of experiments. In such comparisons, authors
      typically make observations under two different scenarios. In this paper,
      we present a system to automatically identify such comparative sentences
      and their components i.e. the compared entities, the scale of the
      comparison and the aspect on which the entities are being compared. Our
      methodology is based on dependencies obtained by applying a parser to
      extract a wide range of comparison structures. We evaluated our system for
      its effectiveness in identifying comparisons and their components. The
      system achieved a F-score of 0.87 for comparison sentence identification
      and 0.77-0.81 for identifying its components.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gupta-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2327'>
    <title>
      Tagging Funding Agencies and Grants in Scientific Articles using
      Sequential Learning Models
    </title>
    <author>
      <first>Subhradeep</first>
      <last>Kayal</last>
    </author>
    <author>
      <first>Zubair</first>
      <last>Afzal</last>
    </author>
    <author>
      <first>George</first>
      <last>Tsatsaronis</last>
    </author>
    <author>
      <first>Sophia</first>
      <last>Katrenko</last>
    </author>
    <author>
      <first>Pascal</first>
      <last>Coupet</last>
    </author>
    <author>
      <first>Marius</first>
      <last>Doornenbal</last>
    </author>
    <author>
      <first>Michelle</first>
      <last>Gregory</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>216–221</pages>
    <url>http://www.aclweb.org/anthology/W17-2327</url>
    <doi>10.18653/v1/W17-2327</doi>
    <abstract>
      In this paper we present a solution for tagging funding bodies and grants
      in scientific articles using a combination of trained sequential learning
      models, namely conditional random fields (CRF), hidden markov models (HMM)
      and maximum entropy models (MaxEnt), on a benchmark set created in-house.
      We apply the trained models to address the BioASQ challenge 5c, which is a
      newly introduced task that aims to solve the problem of funding
      information extraction from scientific articles. Results in the dry-run
      data set of BioASQ task 5c show that the suggested approach can achieve a
      micro-recall of more than 85% in tagging both funding bodies and grants.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kayal-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2328'>
    <title>
      Deep Learning for Biomedical Information Retrieval: Learning Textual
      Relevance from Click Logs
    </title>
    <author>
      <first>Sunil</first>
      <last>Mohan</last>
    </author>
    <author>
      <first>Nicolas</first>
      <last>Fiorini</last>
    </author>
    <author>
      <first>Sun</first>
      <last>Kim</last>
    </author>
    <author>
      <first>Zhiyong</first>
      <last>Lu</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>222–231</pages>
    <url>http://www.aclweb.org/anthology/W17-2328</url>
    <doi>10.18653/v1/W17-2328</doi>
    <abstract>
      We describe a Deep Learning approach to modeling the relevance of a
      document's text to a query, applied to biomedical literature. Instead of
      mapping each document and query to a common semantic space, we compute a
      variable-length difference vector between the query and document which is
      then passed through a deep convolution stage followed by a deep regression
      network to produce the estimated probability of the document's relevance
      to the query. Despite the small amount of training data, this approach
      produces a more robust predictor than computing similarities between
      semantic vector representations of the query and document, and also
      results in significant improvements over traditional IR text factors. In
      the future, we plan to explore its application in improving PubMed search.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mohan-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2329'>
    <title>
      Detecting Dementia through Retrospective Analysis of Routine Blog Posts by
      Bloggers with Dementia
    </title>
    <author>
      <first>Vaden</first>
      <last>Masrani</last>
    </author>
    <author>
      <first>Gabriel</first>
      <last>Murray</last>
    </author>
    <author>
      <first>Thalia</first>
      <last>Field</last>
    </author>
    <author>
      <first>Giuseppe</first>
      <last>Carenini</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>232–237</pages>
    <url>http://www.aclweb.org/anthology/W17-2329</url>
    <doi>10.18653/v1/W17-2329</doi>
    <abstract>
      We investigate if writers with dementia can be automatically distinguished
      from those without by analyzing linguistic markers in written text, in the
      form of blog posts. We have built a corpus of several thousand blog posts,
      some by people with dementia and others by people with loved ones with
      dementia. We use this dataset to train and test several machine learning
      methods, and achieve prediction performance at a level far above the
      baseline.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>masrani-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2330'>
    <title>Protein Word Detection using Text Segmentation Techniques</title>
    <author>
      <first>Devi</first>
      <last>Ganesan</last>
    </author>
    <author>
      <first>Ashish V.</first>
      <last>Tendulkar</last>
    </author>
    <author>
      <first>Sutanu</first>
      <last>Chakraborti</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>238–246</pages>
    <url>http://www.aclweb.org/anthology/W17-2330</url>
    <doi>10.18653/v1/W17-2330</doi>
    <abstract>
      Literature in Molecular Biology is abundant with linguistic metaphors.
      There have been works in the past that attempt to draw parallels between
      linguistics and biology, driven by the fundamental premise that proteins
      have a language of their own. Since word detection is crucial to the
      decipherment of any unknown language, we attempt to establish a problem
      mapping from natural language text to protein sequences at the level of
      words. Towards this end, we explore the use of an unsupervised text
      segmentation algorithm to the task of extracting "biological words" from
      protein sequences. In particular, we demonstrate the effectiveness of
      using domain knowledge to complement data driven approaches in the text
      segmentation task, as well as in its biological counterpart. We also
      propose a novel extrinsic evaluation measure for protein words through
      protein family classification.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ganesan-tendulkar-chakraborti:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2331'>
    <title>
      External Evaluation of Event Extraction Classifiers for Automatic Pathway
      Curation: An extended study of the mTOR pathway
    </title>
    <author>
      <first>Wojciech</first>
      <last>Kusa</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Spranger</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>247–256</pages>
    <url>http://www.aclweb.org/anthology/W17-2331</url>
    <doi>10.18653/v1/W17-2331</doi>
    <abstract>
      This paper evaluates the impact of various event extraction systems on
      automatic pathway curation using the popular mTOR pathway. We quantify the
      impact of training data sets as well as different machine learning
      classifiers and show that some improve the quality of automatically
      extracted pathways.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kusa-spranger:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2332'>
    <title>
      Toward Automated Early Sepsis Alerting: Identifying Infection Patients
      from Nursing Notes
    </title>
    <author>
      <first>Emilia</first>
      <last>Apostolova</last>
    </author>
    <author>
      <first>Tom</first>
      <last>Velez</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>257–262</pages>
    <url>http://www.aclweb.org/anthology/W17-2332</url>
    <doi>10.18653/v1/W17-2332</doi>
    <abstract>
      Severe sepsis and septic shock are conditions that affect millions of
      patients and have close to 50% mortality rate. Early identification of
      at-risk patients significantly improves outcomes. Electronic surveillance
      tools have been developed to monitor structured Electronic Medical Records
      and automatically recognize early signs of sepsis. However, many sepsis
      risk factors (e.g. symptoms and signs of infection) are often captured
      only in free text clinical notes. In this study, we developed a method for
      automatic monitoring of nursing notes for signs and symptoms of infection.
      We utilized a creative approach to automatically generate an annotated
      dataset. The dataset was used to create a Machine Learning model that
      achieved an F1-score ranging from 79 to 96%.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>apostolova-velez:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2333'>
    <title>Enhancing Automatic ICD-9-CM Code Assignment for Medical Texts with PubMed</title>
    <author>
      <first>Danchen</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Daqing</first>
      <last>He</last>
    </author>
    <author>
      <first>Sanqiang</first>
      <last>Zhao</last>
    </author>
    <author>
      <first>Lei</first>
      <last>Li</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>263–271</pages>
    <url>http://www.aclweb.org/anthology/W17-2333</url>
    <doi>10.18653/v1/W17-2333</doi>
    <abstract>
      Assigning a standard ICD-9-CM code to disease symptoms in medical texts is
      an important task in the medical domain. Automating this process could
      greatly reduce the costs. However, the effectiveness of an automatic
      ICD-9-CM code classifier faces a serious problem, which can be triggered
      by unbalanced training data. Frequent diseases often have more training
      data, which helps its classification to perform better than that of an
      infrequent disease. However, a disease’s frequency does not necessarily
      reflect its importance. To resolve this training data shortage problem, we
      propose to strategically draw data from PubMed to enrich the training data
      when there is such need. We validate our method on the CMC dataset, and
      the evaluation results indicate that our method can significantly improve
      the code assignment classifiers' performance at the macro-averaging level.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2334'>
    <title>
      Evaluating Feature Extraction Methods for Knowledge-based Biomedical Word
      Sense Disambiguation
    </title>
    <author>
      <first>Sam</first>
      <last>Henry</last>
    </author>
    <author>
      <first>Clint</first>
      <last>Cuffy</last>
    </author>
    <author>
      <first>Bridget</first>
      <last>McInnes</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>272–281</pages>
    <url>http://www.aclweb.org/anthology/W17-2334</url>
    <doi>10.18653/v1/W17-2334</doi>
    <abstract>
      In this paper, we present an analysis of feature extraction methods via
      dimensionality reduction for the task of biomedical Word Sense
      Disambiguation (WSD). We modify the vector representations in the 2-MRD
      WSD algorithm, and evaluate four dimensionality reduction methods: Word
      Embeddings using Continuous Bag of Words and Skip Gram, Singular Value
      Decomposition (SVD), and Principal Component Analysis (PCA). We also
      evaluate the effects of vector size on the performance of each of these
      methods. Results are evaluated on five standard evaluation datasets
      (Abbrev.100, Abbrev.200, Abbrev.300, NLM-WSD, and MSH-WSD). We find that
      vector sizes of 100 are sufficient for all techniques except SVD, for
      which a vector size of 1500 is referred. We also show that SVD performs on
      par with Word Embeddings for all but one dataset.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>henry-cuffy-mcinnes:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2335'>
    <title>
      Investigating the Documentation of Electronic Cigarette Use in the Veteran
      Affairs Electronic Health Record: A Pilot Study
    </title>
    <author>
      <first>Danielle</first>
      <last>Mowery</last>
    </author>
    <author>
      <first>Brett</first>
      <last>South</last>
    </author>
    <author>
      <first>Olga</first>
      <last>Patterson</last>
    </author>
    <author>
      <first>Shu-Hong</first>
      <last>Zhu</last>
    </author>
    <author>
      <first>Mike</first>
      <last>Conway</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>282–286</pages>
    <url>http://www.aclweb.org/anthology/W17-2335</url>
    <doi>10.18653/v1/W17-2335</doi>
    <abstract>
      In this paper, we present pilot work on characterising the documentation
      of electronic cigarettes (e-cigarettes) in the United States Veterans
      Administration Electronic Health Record. The Veterans Health
      Administration is the largest health care system in the United States with
      1,233 health care facilities nationwide, serving 8.9 million veterans per
      year. We identified a random sample of 2000 Veterans Administration
      patients, coded as current tobacco users, from 2008 to 2014. Using simple
      keyword matching techniques combined with qualitative analysis, we
      investigated the prevalence and distribution of e-cigarette terms in these
      clinical notes, discovering that for current smokers, 11.9% of patient
      records contain an e-cigarette related term.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mowery-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2336'>
    <title>Automated Preamble Detection in Dictated Medical Reports</title>
    <author>
      <first>Wael</first>
      <last>Salloum</last>
    </author>
    <author>
      <first>Greg</first>
      <last>Finley</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Edwards</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Miller</last>
    </author>
    <author>
      <first>David</first>
      <last>Suendermann-Oeft</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>287–295</pages>
    <url>http://www.aclweb.org/anthology/W17-2336</url>
    <doi>10.18653/v1/W17-2336</doi>
    <abstract>
      Dictated medical reports very often feature a preamble containing
      metainformation about the report such as patient and physician names,
      location and name of the clinic, date of procedure, and so on. In the
      medical transcription process, the preamble is usually omitted from the
      final report, as it contains information already available in the
      electronic medical record. We present a method which is able to
      automatically identify preambles in medical dictations. The method makes
      use of stateof- the-art NLP techniques including word embeddings and
      Bi-LSTMs and achieves preamble detection performance superior to humans.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>salloum-EtAl:2017:BioNLP172</bibkey>
  </paper>
  <paper id='2337'>
    <title>A Biomedical Question Answering System in BioASQ 2017</title>
    <author>
      <first>Mourad</first>
      <last>Sarrouti</last>
    </author>
    <author>
      <first>Said</first>
      <last>Ouatik El Alaoui</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>296–301</pages>
    <url>http://www.aclweb.org/anthology/W17-2337</url>
    <doi>10.18653/v1/W17-2337</doi>
    <abstract>
      Question answering, the identification of short accurate answers to users
      questions, is a longstanding challenge widely studied over the last
      decades in the open domain. However, it still requires further efforts in
      the biomedical domain. In this paper, we describe our participation in
      phase B of task 5b in the 2017 BioASQ challenge using our biomedical
      question answering system. Our system, dealing with four types of
      questions (i.e., yes/no, factoid, list, and summary), is based on (1) a
      dictionary-based approach for generating the exact answers of yes/no
      questions, (2) UMLS metathesaurus and term frequency metric for extracting
      the exact answers of factoid and list questions, and (3) the BM25 model
      and UMLS concepts for retrieving the ideal answers (i.e., paragraph-sized
      summaries). Preliminary results show that our system achieves good and
      competitive results in both exact and ideal answers extraction tasks as
      compared with the participating systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sarrouti-ouatikelalaoui:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2338'>
    <title>Adapting Pre-trained Word Embeddings For Use In Medical Coding</title>
    <author>
      <first>Kevin</first>
      <last>Patel</last>
    </author>
    <author>
      <first>Divya</first>
      <last>Patel</last>
    </author>
    <author>
      <first>Mansi</first>
      <last>Golakiya</last>
    </author>
    <author>
      <first>Pushpak</first>
      <last>Bhattacharyya</last>
    </author>
    <author>
      <first>Nilesh</first>
      <last>Birari</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>302–306</pages>
    <url>http://www.aclweb.org/anthology/W17-2338</url>
    <doi>10.18653/v1/W17-2338</doi>
    <abstract>
      Word embeddings are a crucial component in modern NLP. Pre-trained
      embeddings released by different groups have been a major reason for their
      popularity. However, they are trained on generic corpora, which limits
      their direct use for domain specific tasks. In this paper, we propose a
      method to add task specific information to pre-trained word embeddings.
      Such information can improve their utility. We add information from
      medical coding data, as well as the first level from the hierarchy of
      ICD-10 medical code set to different pre-trained word embeddings. We adapt
      CBOW algorithm from the word2vec package for our purpose. We evaluated our
      approach on five different pre-trained word embeddings. Both the original
      word embeddings, and their modified versions (the ones with added
      information) were used for automated review of medical coding. The
      modified word embeddings give an improvement in f-score by 1% on the
      5-fold evaluation on a private medical claims dataset. Our results show
      that adding extra information is possible and beneficial for the task at
      hand.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>patel-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2339'>
    <title>Initializing neural networks for hierarchical multi-label text classification</title>
    <author>
      <first>Simon</first>
      <last>Baker</last>
    </author>
    <author>
      <first>Anna</first>
      <last>Korhonen</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>307–315</pages>
    <url>http://www.aclweb.org/anthology/W17-2339</url>
    <doi>10.18653/v1/W17-2339</doi>
    <abstract>
      Many tasks in the biomedical domain require the assignment of one or more
      predefined labels to input text, where the labels are a part of a
      hierarchical structure (such as a taxonomy). The conventional approach is
      to use a one-vs.-rest (OVR) classification setup, where a binary
      classifier is trained for each label in the taxonomy or ontology where all
      instances not belonging to the class are considered negative examples. The
      main drawbacks to this approach are that dependencies between classes are
      not leveraged in the training and classification process, and the
      additional computational cost of training parallel classifiers. In this
      paper, we apply a new method for hierarchical multi-label text
      classification that initializes a neural network model final hidden layer
      such that it leverages label co-occurrence relations such as hypernymy.
      This approach elegantly lends itself to hierarchical classification. We
      evaluated this approach using two hierarchical multi-label text
      classification tasks in the biomedical domain using both sentence- and
      document-level classification. Our evaluation shows promising results for
      this approach.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>baker-korhonen:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2340'>
    <title>
      Biomedical Event Trigger Identification Using Bidirectional Recurrent
      Neural Network Based Models
    </title>
    <author>
      <first>Rahul</first>
      <last>V S S Patchigolla</last>
    </author>
    <author>
      <first>Sunil</first>
      <last>Sahu</last>
    </author>
    <author>
      <first>Ashish</first>
      <last>Anand</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>316–321</pages>
    <url>http://www.aclweb.org/anthology/W17-2340</url>
    <doi>10.18653/v1/W17-2340</doi>
    <abstract>
      Biomedical events describe complex interactions between various biomedical
      entities. Event trigger is a word or a phrase which typically signifies
      the occurrence of an event. Event trigger identification is an important
      first step in all event extraction methods. However many of the current
      approaches either rely on complex hand-crafted features or consider
      features only within a window. In this paper we propose a method that
      takes the advantage of recurrent neural network (RNN) to extract higher
      level features present across the sentence. Thus hidden state
      representation of RNN along with word and entity type embedding as
      features avoid relying on the complex hand-crafted features generated
      using various NLP toolkits. Our experiments have shown to achieve
      state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We
      have also performed category-wise analysis of the result and discussed the
      importance of various features in trigger identification task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vsspatchigolla-sahu-anand:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2341'>
    <title>
      Representations of Time Expressions for Temporal Relation Extraction with
      Convolutional Neural Networks
    </title>
    <author>
      <first>Chen</first>
      <last>Lin</last>
    </author>
    <author>
      <first>Timothy</first>
      <last>Miller</last>
    </author>
    <author>
      <first>Dmitriy</first>
      <last>Dligach</last>
    </author>
    <author>
      <first>Steven</first>
      <last>Bethard</last>
    </author>
    <author>
      <first>Guergana</first>
      <last>Savova</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>322–327</pages>
    <url>http://www.aclweb.org/anthology/W17-2341</url>
    <doi>10.18653/v1/W17-2341</doi>
    <abstract>
      Token sequences are often used as the input for Convolutional Neural
      Networks (CNNs) in natural language processing. However, they might not be
      an ideal representation for time expressions, which are long, highly
      varied, and semantically complex. We describe a method for representing
      time expressions with single pseudo-tokens for CNNs. With this method, we
      establish a new state-of-the-art result for a clinical temporal relation
      extraction task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lin-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2342'>
    <title>
      Automatic Diagnosis Coding of Radiology Reports: A Comparison of Deep
      Learning and Conventional Classification Methods
    </title>
    <author>
      <first>Sarvnaz</first>
      <last>Karimi</last>
    </author>
    <author>
      <first>Xiang</first>
      <last>Dai</last>
    </author>
    <author>
      <first>Hamedh</first>
      <last>Hassanzadeh</last>
    </author>
    <author>
      <first>Anthony</first>
      <last>Nguyen</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>328–332</pages>
    <url>http://www.aclweb.org/anthology/W17-2342</url>
    <doi>10.18653/v1/W17-2342</doi>
    <abstract>
      Diagnosis autocoding services and research intend to both improve the
      productivity of clinical coders and the accuracy of the coding. It is an
      important step in data analysis for funding and reimbursement, as well as
      health services planning and resource allocation. We investigate the
      applicability of deep learning at autocoding of radiology reports using
      International Classification of Diseases (ICD). Deep learning methods are
      known to require large training data. Our goal is to explore how to use
      these methods when the training data is sparse, skewed and relatively
      small, and how their effectiveness compares to conventional methods. We
      identify optimal parameters that could be used in setting up a
      convolutional neural network for autocoding with comparable results to
      that of conventional methods.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>karimi-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2343'>
    <title>
      Automatic classification of doctor-patient questions for a virtual patient
      record query task
    </title>
    <author>
      <first>Leonardo</first>
      <last>Campillos Llanos</last>
    </author>
    <author>
      <first>Sophie</first>
      <last>Rosset</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Zweigenbaum</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>333–341</pages>
    <url>http://www.aclweb.org/anthology/W17-2343</url>
    <doi>10.18653/v1/W17-2343</doi>
    <abstract>
      We present the work-in-progress of automating the classification of
      doctor-patient questions in the context of a simulated consultation with a
      virtual patient. We classify questions according to the computational
      strategy (rule-based or other) needed for looking up data in the clinical
      record. We compare ‘traditional’ machine learning methods (Gaussian and
      Multinomial Naive Bayes, and Support Vector Machines) and a neural network
      classifier (FastText). We obtained the best results with the SVM using
      semantic annotations, whereas the neural classifier achieved promising
      results without it.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>campillosllanos-rosset-zweigenbaum:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2344'>
    <title>
      Assessing the performance of Olelo, a real-time biomedical question
      answering application
    </title>
    <author>
      <first>Mariana</first>
      <last>Neves</last>
    </author>
    <author>
      <first>Fabian</first>
      <last>Eckert</last>
    </author>
    <author>
      <first>Hendrik</first>
      <last>Folkerts</last>
    </author>
    <author>
      <first>Matthias</first>
      <last>Uflacker</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>342–350</pages>
    <url>http://www.aclweb.org/anthology/W17-2344</url>
    <doi>10.18653/v1/W17-2344</doi>
    <abstract>
      Question answering (QA) can support physicians and biomedical researchers
      to find answers to their questions in the scientific literature. Such
      systems process large collections of documents in real time and include
      many natural language processing (NLP) procedures. We recently developed
      Olelo, a QA system for biomedicine which includes various NLP components,
      such as question processing, document and passage retrieval, answer
      processing and multi-document summarization. In this work, we present an
      evaluation of our system on the the fifth BioASQ challenge. We
      participated with the current state of the application and with an
      extension based on semantic role labeling that we are currently
      investigating. In addition to the BioASQ evaluation, we compared our
      system to other on-line biomedical QA systems in terms of the response
      time and the quality of the answers.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>neves-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2345'>
    <title>Clinical Event Detection with Hybrid Neural Architecture</title>
    <author>
      <first>Adyasha</first>
      <last>Maharana</last>
    </author>
    <author>
      <first>Meliha</first>
      <last>Yetisgen</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>351–355</pages>
    <url>http://www.aclweb.org/anthology/W17-2345</url>
    <doi>10.18653/v1/W17-2345</doi>
    <abstract>
      Event detection from clinical notes has been traditionally solved with
      rule based and statistical natural language processing (NLP) approaches
      that require extensive domain knowledge and feature engineering. In this
      paper, we have explored the feasibility of approaching this task with
      recurrent neural networks, clinical word embeddings and introduced a
      hybrid architecture to improve detection for entities with smaller
      representation in the dataset. A comparative analysis is also done which
      reveals the complementary behavior of neural networks and conditional
      random fields in clinical entity detection.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>maharana-yetisgen:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2346'>
    <title>
      Extracting Personal Medical Events for User Timeline Construction using
      Minimal Supervision
    </title>
    <author>
      <first>Aakanksha</first>
      <last>Naik</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Bogart</last>
    </author>
    <author>
      <first>Carolyn</first>
      <last>Rose</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>356–364</pages>
    <url>http://www.aclweb.org/anthology/W17-2346</url>
    <doi>10.18653/v1/W17-2346</doi>
    <abstract>
      In this paper, we describe a system for automatic construction of user
      disease progression timelines from their posts in online support groups
      using minimal supervision. In recent years, several online support groups
      have been established which has led to a huge increase in the amount of
      patient-authored text available. Creating systems which can automatically
      extract important medical events and create disease progression timelines
      for users from such text can help in patient health monitoring as well as
      studying links between medical events and users' participation in support
      groups. Prior work in this domain has used manually constructed keyword
      sets to detect medical events. In this work, our aim is to perform medical
      event detection using minimal supervision in order to develop a more
      general timeline construction system. Our system achieves an accuracy of
      55.17%, which is 92% of the performance achieved by a supervised baseline
      system.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>naik-bogart-rose:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2347'>
    <title>Detecting mentions of pain and acute confusion in Finnish clinical text</title>
    <author>
      <first>Hans</first>
      <last>Moen</last>
    </author>
    <author>
      <first>Kai</first>
      <last>Hakala</last>
    </author>
    <author>
      <first>Farrokh</first>
      <last>Mehryary</last>
    </author>
    <author>
      <first>Laura-Maria</first>
      <last>Peltonen</last>
    </author>
    <author>
      <first>Tapio</first>
      <last>Salakoski</last>
    </author>
    <author>
      <first>Filip</first>
      <last>Ginter</last>
    </author>
    <author>
      <first>Sanna</first>
      <last>Salanterä</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>365–372</pages>
    <url>http://www.aclweb.org/anthology/W17-2347</url>
    <doi>10.18653/v1/W17-2347</doi>
    <abstract>
      We study and compare two different approaches to the task of automatic
      assignment of predefined classes to clinical free-text narratives. In the
      first approach this is treated as a traditional mention-level named-entity
      recognition task, while the second approach treats it as a sentence-level
      multi-label classification task. Performance comparison across these two
      approaches is conducted in the form of sentence-level evaluation and
      state-of-the-art methods for both approaches are evaluated. The
      experiments are done on two data sets consisting of Finnish clinical text,
      manually annotated with respect to the topics pain and acute confusion.
      Our results suggest that the mention-level named-entity recognition
      approach outperforms sentence-level classification overall, but the latter
      approach still manages to achieve the best prediction scores on several
      annotation classes.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>moen-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2348'>
    <title>
      A Multi-strategy Query Processing Approach for Biomedical Question
      Answering: USTB_PRIR at BioASQ 2017 Task 5B
    </title>
    <author>
      <first>Zan-Xia</first>
      <last>Jin</last>
    </author>
    <author>
      <first>Bo-Wen</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Fan</first>
      <last>Fang</last>
    </author>
    <author>
      <first>Le-Le</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Xu-Cheng</first>
      <last>Yin</last>
    </author>
    <booktitle>BioNLP 2017</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada,</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>373–380</pages>
    <url>http://www.aclweb.org/anthology/W17-2348</url>
    <doi>10.18653/v1/W17-2348</doi>
    <abstract>
      This paper describes the participation of USTB_PRIR team in the 2017
      BioASQ 5B on question answering, including document retrieval, snippet
      retrieval, and concept retrieval task. We introduce different multimodal
      query processing strategies to enrich query terms and assign different
      weights to them. Specifically, sequential dependence model (SDM),
      pseudo-relevance feedback (PRF), fielded sequential dependence model
      (FSDM) and Divergence from Randomness model (DFRM) are respectively
      performed on different fields of PubMed articles, sentences extracted from
      relevant articles, the five terminologies or ontologies (MeSH, GO, Jochem,
      Uniprot and DO) to achieve better search performances. Preliminary results
      show that our systems outperform others in the document and snippet
      retrieval task in the first two batches.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jin-EtAl:2017:BioNLP17</bibkey>
  </paper>
  <paper id='2400'>
    <title>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </title>
    <editor>Martin Riedl</editor>
    <editor>Swapna Somasundaran</editor>
    <editor>Goran Glavaš</editor>
    <editor>Eduard Hovy</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-24</url>
    <doi>10.18653/v1/W17-24</doi>
    <bibtype>book</bibtype>
    <bibkey>TextGraphs-11:2017</bibkey>
  </paper>
  <paper id='2401'>
    <title>On the "Calligraphy" of Books</title>
    <author>
      <first>Vanessa Queiroz</first>
      <last>Marinho</last>
    </author>
    <author>
      <first>Henrique Ferraz</first>
      <last>de Arruda</last>
    </author>
    <author>
      <first>Thales</first>
      <last>Sinelli</last>
    </author>
    <author>
      <first>Luciano da Fontoura</first>
      <last>Costa</last>
    </author>
    <author>
      <first>Diego Raphael</first>
      <last>Amancio</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-2401</url>
    <doi>10.18653/v1/W17-2401</doi>
    <abstract>
      Authorship attribution is a natural language processing task that has been
      widely studied, often by considering small order statistics. In this
      paper, we explore a complex network approach to assign the authorship of
      texts based on their mesoscopic representation, in an attempt to capture
      the flow of the narrative. Indeed, as reported in this work, such an
      approach allowed the identification of the dominant narrative structure of
      the studied authors. This has been achieved due to the ability of the
      mesoscopic approach to take into account relationships between different,
      not necessarily adjacent, parts of the text, which is able to capture the
      story flow. The potential of the proposed approach has been illustrated
      through principal component analysis, a comparison with the chance
      baseline method, and network visualization. Such visualizations reveal
      individual characteristics of the authors, which can be understood as a
      kind of calligraphy.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marinho-EtAl:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2402'>
    <title>
      Adapting predominant and novel sense discovery algorithms for identifying
      corpus-specific sense differences
    </title>
    <author>
      <first>Binny</first>
      <last>Mathew</last>
    </author>
    <author>
      <first>Suman Kalyan</first>
      <last>Maity</last>
    </author>
    <author>
      <first>Pratip</first>
      <last>Sarkar</last>
    </author>
    <author>
      <first>Animesh</first>
      <last>Mukherjee</last>
    </author>
    <author>
      <first>Pawan</first>
      <last>Goyal</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–20</pages>
    <url>http://www.aclweb.org/anthology/W17-2402</url>
    <doi>10.18653/v1/W17-2402</doi>
    <abstract>
      Word senses are not static and may have temporal, spatial or
      corpus-specific scopes. Identifying such scopes might benefit the existing
      WSD systems largely. In this paper, while studying corpus specific word
      senses, we adapt three existing predominant and novel-sense discovery
      algorithms to identify these corpus-specific senses. We make use of text
      data available in the form of millions of digitized books and newspaper
      archives as two different sources of corpora and propose automated methods
      to identify corpus-specific word senses at various time points. We conduct
      an extensive and thorough human judgement experiment to rigorously
      evaluate and compare the performance of these approaches. Post adaptation,
      the output of the three algorithms are in the same format and the accuracy
      results are also comparable, with roughly 45-60% of the reported
      corpus-specific senses being judged as genuine.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mathew-EtAl:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2403'>
    <title>Merging knowledge bases in different languages</title>
    <author>
      <first>Jerónimo</first>
      <last>Hernández-González</last>
    </author>
    <author>
      <first>Estevam R.</first>
      <last>Hruschka Jr.</last>
    </author>
    <author>
      <first>Tom M.</first>
      <last>Mitchell</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–29</pages>
    <url>http://www.aclweb.org/anthology/W17-2403</url>
    <doi>10.18653/v1/W17-2403</doi>
    <abstract>
      Recently, different systems which learn to populate and extend a knowledge
      base (KB) from the web in different languages have been presented.
      Although a large set of concepts should be learnt independently from the
      language used to read, there are facts which are expected to be more
      easily gathered in local language (e.g., culture or geography). A system
      that merges KBs learnt in different languages will benefit from the
      complementary information as long as common beliefs are identified, as
      well as from redundancy present in web pages written in different
      languages. In this paper, we deal with the problem of identifying
      equivalent beliefs (or concepts) across language specific KBs, assuming
      that they share the same ontology of categories and relations. In a case
      study with two KBs independently learnt from different inputs, namely web
      pages written in English and web pages written in Portuguese respectively,
      we report on the results of two methodologies: an approach based on
      personalized PageRank and an inference technique to find out common
      relevant paths through the KBs. The proposed inference technique
      efficiently identifies relevant paths, outperforming the baseline (a
      dictionary-based classifier) in the vast majority of tested categories.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hernandezgonzalez-hruschkajr-mitchell:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2404'>
    <title>
      Parameter Free Hierarchical Graph-Based Clustering for Analyzing
      Continuous Word Embeddings
    </title>
    <author>
      <first>Thomas Alexander</first>
      <last>Trost</last>
    </author>
    <author>
      <first>Dietrich</first>
      <last>Klakow</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>30–38</pages>
    <url>http://www.aclweb.org/anthology/W17-2404</url>
    <doi>10.18653/v1/W17-2404</doi>
    <abstract>
      Word embeddings are high-dimensional vector representations of words and
      are thus difficult to interpret. In order to deal with this, we introduce
      an unsupervised parameter free method for creating a hierarchical
      graphical clustering of the full ensemble of word vectors and show that
      this structure is a geometrically meaningful representation of the
      original relations between the words. This newly obtained representation
      can be used for better understanding and thus improving the embedding
      algorithm and exhibits semantic meaning, so it can also be utilized in a
      variety of language processing tasks like categorization or measuring
      similarity.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>trost-klakow:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2405'>
    <title>Spectral Graph-Based Method of Multimodal Word Embedding</title>
    <author>
      <first>Kazuki</first>
      <last>Fukui</last>
    </author>
    <author>
      <first>Takamasa</first>
      <last>Oshikiri</last>
    </author>
    <author>
      <first>Hidetoshi</first>
      <last>Shimodaira</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39–44</pages>
    <url>http://www.aclweb.org/anthology/W17-2405</url>
    <doi>10.18653/v1/W17-2405</doi>
    <abstract>
      In this paper, we propose a novel method for multimodal word embedding,
      which exploit a generalized framework of multi-view spectral graph
      embedding to take into account visual appearances or scenes denoted by
      words in a corpus. We evaluated our method through word similarity tasks
      and a concept-to-image search task, having found that it provides word
      representations that reflect visual information, while somewhat
      trading-off the performance on the word similarity tasks. Moreover, we
      demonstrate that our method captures multimodal linguistic regularities,
      which enable recovering relational similarities between words and images
      by vector arithmetics.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fukui-oshikiri-shimodaira:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2406'>
    <title>Graph Methods for Multilingual FrameNets</title>
    <author>
      <first>Collin</first>
      <last>Baker</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Ellsworth</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45–50</pages>
    <url>http://www.aclweb.org/anthology/W17-2406</url>
    <doi>10.18653/v1/W17-2406</doi>
    <abstract>
      This paper introduces a new, graph-based view of the data of the FrameNet
      project, which we hope will make it easier to understand the mixture of
      semantic and syntactic information contained in FrameNet annotation. We
      show how English FrameNet and other Frame Semantic resources can be
      represented as sets of interconnected graphs of frames, frame elements,
      semantic types, and annotated instances of them in text. We display
      examples of the new graphical representation based on the annotations,
      which combine Frame Semantics and Construction Grammar, thus capturing
      most of the syntax and semantics of each sentence. We consider how graph
      theory could help researchers to make better use of FrameNet data for
      tasks such as automatic Frame Semantic role labeling, paraphrasing, and
      translation. Finally, we describe the development of FrameNet-like lexical
      resources for other languages in the current Multilingual FrameNet
      project. which seeks to discover cross-lingual alignments, both in the
      lexicon (for frames and lexical units within frames) and across parallel
      or comparable texts. We conclude with an example showing graphically the
      semantic and syntactic similarities and differences between parallel
      sentences in English and Japanese. We will release software for displaying
      such graphs from the current data releases.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>baker-ellsworth:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2407'>
    <title>Extract with Order for Coherent Multi-Document Summarization</title>
    <author>
      <first>Mir Tafseer</first>
      <last>Nayeem</last>
    </author>
    <author>
      <first>Yllias</first>
      <last>Chali</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>51–56</pages>
    <url>http://www.aclweb.org/anthology/W17-2407</url>
    <doi>10.18653/v1/W17-2407</doi>
    <abstract>
      In this work, we aim at developing an extractive summarizer in the
      multi-document setting. We implement a rank based sentence selection using
      continuous vector representations along with key-phrases. Furthermore, we
      propose a model to tackle summary coherence for increasing readability. We
      conduct experiments on the Document Understanding Conference (DUC) 2004
      datasets using ROUGE toolkit. Our experiments demonstrate that the methods
      bring significant improvements over the state of the art methods in terms
      of informativity and coherence.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nayeem-chali:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2408'>
    <title>Work Hard, Play Hard: Email Classification on the Avocado and Enron Corpora</title>
    <author>
      <first>Sakhar</first>
      <last>Alkhereyf</last>
    </author>
    <author>
      <first>Owen</first>
      <last>Rambow</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–65</pages>
    <url>http://www.aclweb.org/anthology/W17-2408</url>
    <doi>10.18653/v1/W17-2408</doi>
    <abstract>
      In this paper, we present an empirical study of email classification into
      two main categories “Business" and “Personal". We train on the Enron email
      corpus, and test on the Enron and Avocado email corpora. We show that
      information from the email exchange networks improves the performance of
      classification. We represent the email exchange networks as social
      networks with graph structures. For this classification task, we extract
      social networks features from the graphs in addition to lexical features
      from email content and we compare the performance of SVM and Extra-Trees
      classifiers using these features. Combining graph features with lexical
      features improves the performance on both classifiers. We also provide
      manually annotated sets of the Avocado and Enron email corpora as a
      supplementary contribution.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alkhereyf-rambow:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2409'>
    <title>
      A Graph Based Semi-Supervised Approach for Analysis of Derivational Nouns
      in Sanskrit
    </title>
    <author>
      <first>Amrith</first>
      <last>Krishna</last>
    </author>
    <author>
      <first>Pavankumar</first>
      <last>Satuluri</last>
    </author>
    <author>
      <first>Harshavardhan</first>
      <last>Ponnada</last>
    </author>
    <author>
      <first>Muneeb</first>
      <last>Ahmed</last>
    </author>
    <author>
      <first>Gulab</first>
      <last>Arora</last>
    </author>
    <author>
      <first>Kaustubh</first>
      <last>Hiware</last>
    </author>
    <author>
      <first>Pawan</first>
      <last>Goyal</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–75</pages>
    <url>http://www.aclweb.org/anthology/W17-2409</url>
    <doi>10.18653/v1/W17-2409</doi>
    <abstract>
      Derivational nouns are widely used in Sanskrit corpora and represent an
      important cornerstone of productivity in the language. Currently there
      exists no analyser that identifies the derivational nouns. We propose a
      semi supervised approach for identification of derivational nouns in
      Sanskrit. We not only identify the derivational words, but also link them
      to their corresponding source words. Our novelty comes in the design of
      the network structure for the task. The edge weights are featurised based
      on the phonetic, morphological, syntactic and the semantic similarity
      shared between the words to be identified. We find that our model is
      effective for the task, even when we employ a labelled dataset which is
      only 5 % to that of the entire dataset.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>krishna-EtAl:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2410'>
    <title>Evaluating text coherence based on semantic similarity graph</title>
    <author>
      <first>Jan Wira Gotama</first>
      <last>Putra</last>
    </author>
    <author>
      <first>Takenobu</first>
      <last>Tokunaga</last>
    </author>
    <booktitle>
      Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for
      Natural Language Processing
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–85</pages>
    <url>http://www.aclweb.org/anthology/W17-2410</url>
    <doi>10.18653/v1/W17-2410</doi>
    <abstract>
      Coherence is a crucial feature of text because it is indispensable for
      conveying its communication purpose and meaning to its readers. In this
      paper, we propose an unsupervised text coherence scoring based on graph
      construction in which edges are established between semantically similar
      sentences represented by vertices. The sentence similarity is calculated
      based on the cosine similarity of semantic vectors representing sentences.
      We provide three graph construction methods establishing an edge from a
      given vertex to a preceding adjacent vertex, to a single similar vertex,
      or to multiple similar vertices. We evaluated our methods in the document
      discrimination task and the insertion task by comparing our proposed
      methods to the supervised (Entity Grid) and unsupervised (Entity Graph)
      baselines. In the document discrimination task, our method outperformed
      the unsupervised baseline but could not do the supervised baseline, while
      in the insertion task, our method outperformed both baselines.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>putra-tokunaga:2017:TextGraphs-11</bibkey>
  </paper>
  <paper id='2500'>
    <title>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</title>
    <editor>
      <first>Serge</first>
      <last>Sharoff</last>
    </editor>
    <editor>
      <first>Pierre</first>
      <last>Zweigenbaum</last>
    </editor>
    <editor>
      <first>Reinhard</first>
      <last>Rapp</last>
    </editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-25</url>
    <doi>10.18653/v1/W17-25</doi>
    <bibtype>book</bibtype>
    <bibkey>BUCC:2017</bibkey>
  </paper>
  <paper id='2501'>
    <title>
      Users and Data: The Two Neglected Children of Bilingual Natural Language
      Processing Research
    </title>
    <author>
      <first>Phillippe</first>
      <last>Langlais</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–5</pages>
    <url>http://www.aclweb.org/anthology/W17-2501</url>
    <doi>10.18653/v1/W17-2501</doi>
    <abstract>
      Despite numerous studies devoted to mining parallel material from
      bilingual data, we have yet to see the resulting technologies
      wholeheartedly adopted by professional translators and terminologists
      alike. I argue that this state of affairs is mainly due to two factors:
      the emphasis published authors put on models (even though data is as
      important), and the conspicuous lack of concern for actual end-users.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>langlais:2017:BUCC</bibkey>
  </paper>
  <paper id='2502'>
    <title>Deep Investigation of Cross-Language Plagiarism Detection Methods</title>
    <author>
      <first>Jérémy</first>
      <last>Ferrero</last>
    </author>
    <author>
      <first>Laurent</first>
      <last>Besacier</last>
    </author>
    <author>
      <first>Didier</first>
      <last>Schwab</last>
    </author>
    <author>
      <first>Frédéric</first>
      <last>Agnès</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>6–15</pages>
    <url>http://www.aclweb.org/anthology/W17-2502</url>
    <doi>10.18653/v1/W17-2502</doi>
    <abstract>
      This paper is a deep investigation of cross-language plagiarism detection
      methods on a new recently introduced open dataset, which contains parallel
      and com- parable collections of documents with multiple characteristics
      (different genres, languages and sizes of texts). We investigate
      cross-language plagiarism detection methods for 6 language pairs on 2
      granularities of text units in order to draw robust conclusions on the
      best methods while deeply analyzing correlations across document styles
      and languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ferrero-EtAl:2017:BUCC</bibkey>
  </paper>
  <paper id='2503'>
    <title>Sentence Alignment using Unfolding Recursive Autoencoders</title>
    <author>
      <first>Jeenu</first>
      <last>Grover</last>
    </author>
    <author>
      <first>Pabitra</first>
      <last>Mitra</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16–20</pages>
    <url>http://www.aclweb.org/anthology/W17-2503</url>
    <doi>10.18653/v1/W17-2503</doi>
    <abstract>
      In this paper, we propose a novel two step algorithm for sentence
      alignment in monolingual corpora using Unfolding Recursive Autoencoders.
      First, we use unfolding recursive auto-encoders (RAE) to learn feature
      vectors for phrases in syntactical tree of the sentence. To compare two
      sentences we use a similarity matrix which has dimensions proportional to
      the size of the two sentences. Since the similarity matrix generated to
      compare two sentences has varying dimension due to different sentence
      lengths, a dynamic pooling layer is used to map it to a matrix of fixed
      dimension. The resulting matrix is used to calculate the similarity scores
      between the two sentences. The second step of the algorithm captures the
      contexts in which the sentences occur in the document by using a dynamic
      programming algorithm for global alignment.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grover-mitra:2017:BUCC</bibkey>
  </paper>
  <paper id='2504'>
    <title>
      Acquisition of Translation Lexicons for Historically Unwritten Languages
      via Bridging Loanwords
    </title>
    <author>
      <first>Michael</first>
      <last>Bloodgood</last>
    </author>
    <author>
      <first>Benjamin</first>
      <last>Strauss</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–25</pages>
    <url>http://www.aclweb.org/anthology/W17-2504</url>
    <doi>10.18653/v1/W17-2504</doi>
    <abstract>
      With the advent of informal electronic communications such as social
      media, colloquial languages that were historically unwritten are being
      written for the first time in heavily code-switched environments. We
      present a method for inducing portions of translation lexicons through the
      use of expert knowledge in these settings where there are approximately
      zero resources available other than a language informant, potentially not
      even large amounts of monolingual data. We investigate inducing a Moroccan
      Darija-English translation lexicon via French loanwords bridging into
      English and find that a useful lexicon is induced for human-assisted
      translation and statistical machine translation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bloodgood-strauss:2017:BUCC</bibkey>
  </paper>
  <paper id='2505'>
    <title>Toward a Comparable Corpus of Latvian, Russian and English Tweets</title>
    <author>
      <first>Dmitrijs</first>
      <last>Milajevs</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–30</pages>
    <url>http://www.aclweb.org/anthology/W17-2505</url>
    <doi>10.18653/v1/W17-2505</doi>
    <abstract>
      Twitter has become a rich source for linguistic data. Here, a possibility
      of building a trilingual Latvian-Russian-English corpus of tweets from
      Riga, Latvia is investigated. Such a corpus, once constructed, might be of
      great use for multiple purposes including training machine translation
      models, examining cross-lingual phenomena and studying the population of
      Riga. This pilot study shows that it is feasible to build such a resource
      by collecting and analysing a pilot corpus, which is made publicly
      available and can be used to construct a large comparable corpus.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>milajevs:2017:BUCC</bibkey>
  </paper>
  <paper id='2506'>
    <title>Automatic Extraction of Parallel Speech Corpora from Dubbed Movies</title>
    <author>
      <first>Alp</first>
      <last>Öktem</last>
    </author>
    <author>
      <first>Mireia</first>
      <last>Farrús</last>
    </author>
    <author>
      <first>Leo</first>
      <last>Wanner</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–35</pages>
    <url>http://www.aclweb.org/anthology/W17-2506</url>
    <doi>10.18653/v1/W17-2506</doi>
    <abstract>
      This paper presents a methodology to extract parallel speech corpora based
      on any language pair from dubbed movies, together with an application
      framework in which some corresponding prosodic parameters are extracted.
      The obtained parallel corpora are especially suitable for speech-to-speech
      translation applications when a prosody transfer between source and target
      languages is desired.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>oktem-farrus-wanner:2017:BUCC</bibkey>
  </paper>
  <paper id='2507'>
    <title>A parallel collection of clinical trials in Portuguese and English</title>
    <author>
      <first>Mariana</first>
      <last>Neves</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–40</pages>
    <url>http://www.aclweb.org/anthology/W17-2507</url>
    <doi>10.18653/v1/W17-2507</doi>
    <abstract>
      Parallel collections of documents are crucial resources for training and
      evaluating machine translation (MT) systems. Even though large collections
      are available for certain domains and language pairs, these are still
      scarce in the biomedical domain. We developed a parallel corpus of
      clinical trials in Portuguese and English. The documents are derived from
      the Brazilian Clinical Trials Registry and the corpus currently contains a
      total of 1188 documents. In this paper, we describe the corpus
      construction and discuss the quality of the translation and the sentence
      alignment that we obtained.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>neves:2017:BUCC</bibkey>
  </paper>
  <paper id='2508'>
    <title>Weighted Set-Theoretic Alignment of Comparable Sentences</title>
    <author>
      <first>Andoni</first>
      <last>Azpeitia</last>
    </author>
    <author>
      <first>Thierry</first>
      <last>Etchegoyhen</last>
    </author>
    <author>
      <first>Eva</first>
      <last>Martínez Garcia</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–45</pages>
    <url>http://www.aclweb.org/anthology/W17-2508</url>
    <doi>10.18653/v1/W17-2508</doi>
    <abstract>
      This article presents the STACCw system for the BUCC 2017 shared task on
      parallel sentence extraction from comparable corpora. The original STACC
      approach, based on set-theoretic operations over bags of words, had been
      previously shown to be efficient and portable across domains and alignment
      scenarios. Wedescribe an extension of this approach with a new weighting
      scheme and show that it provides significant improvements on the datasets
      provided for the shared task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>azpeitia-etchegoyhen-martinezgarcia:2017:BUCC</bibkey>
  </paper>
  <paper id='2509'>
    <title>
      BUCC 2017 Shared Task: a First Attempt Toward a Deep Learning Framework
      for Identifying Parallel Sentences in Comparable Corpora
    </title>
    <author>
      <first>Francis</first>
      <last>Grégoire</last>
    </author>
    <author>
      <first>Philippe</first>
      <last>Langlais</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–50</pages>
    <url>http://www.aclweb.org/anthology/W17-2509</url>
    <doi>10.18653/v1/W17-2509</doi>
    <abstract>
      This paper describes our participation in BUCC 2017 shared task:
      identifying parallel sentences in comparable corpora. Our goal is to
      leverage continuous vector representations and distributional semantics
      with a minimal use of external preprocessing and postprocessing tools. We
      report experiments that were conducted after transmitting our results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gregoire-langlais:2017:BUCC</bibkey>
  </paper>
  <paper id='2510'>
    <title>zNLP: Identifying Parallel Sentences in Chinese-English Comparable Corpora</title>
    <author>
      <first>Zheng</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Pierre</first>
      <last>Zweigenbaum</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>51–55</pages>
    <url>http://www.aclweb.org/anthology/W17-2510</url>
    <doi>10.18653/v1/W17-2510</doi>
    <abstract>
      This paper describes the zNLP system for the BUCC 2017 shared task. Our
      system identifies parallel sentence pairs in Chinese-English comparable
      corpora by translating word-by-word Chinese sentences into English, using
      the search engine Solr to select near-parallel sentences and then by using
      an SVM classifier to identify true parallel sentences from the previous
      results. It obtains an F1-score of 45% (resp. 32%) on the test (training)
      set.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-zweigenbaum:2017:BUCC</bibkey>
  </paper>
  <paper id='2511'>
    <title>
      BUCC2017: A Hybrid Approach for Identifying Parallel Sentences in
      Comparable Corpora
    </title>
    <author>
      <first>Sainik</first>
      <last>Mahata</last>
    </author>
    <author>
      <first>Dipankar</first>
      <last>Das</last>
    </author>
    <author>
      <first>Sivaji</first>
      <last>Bandyopadhyay</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–59</pages>
    <url>http://www.aclweb.org/anthology/W17-2511</url>
    <doi>10.18653/v1/W17-2511</doi>
    <abstract>
      A Statistical Machine Translation (SMT) system is always trained using
      large parallel corpus to produce effective translation. Not only is the
      corpus scarce, it also involves a lot of manual labor and cost. Parallel
      corpus can be prepared by employing comparable corpora where a pair of
      corpora is in two different languages pointing to the same domain. In the
      present work, we try to build a parallel corpus for French-English
      language pair from a given comparable corpus. The data and the problem set
      are provided as part of the shared task organized by BUCC 2017. We have
      proposed a system that first translates the sentences by heavily relying
      on Moses and then group the sentences based on sentence length similarity.
      Finally, the one to one sentence selection was done based on Cosine
      Similarity algorithm.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mahata-das-bandyopadhyay:2017:BUCC</bibkey>
  </paper>
  <paper id='2512'>
    <title>
      Overview of the Second BUCC Shared Task: Spotting Parallel Sentences in
      Comparable Corpora
    </title>
    <author>
      <first>Pierre</first>
      <last>Zweigenbaum</last>
    </author>
    <author>
      <first>Serge</first>
      <last>Sharoff</last>
    </author>
    <author>
      <first>Reinhard</first>
      <last>Rapp</last>
    </author>
    <booktitle>Proceedings of the 10th Workshop on Building and Using Comparable Corpora</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60–67</pages>
    <url>http://www.aclweb.org/anthology/W17-2512</url>
    <doi>10.18653/v1/W17-2512</doi>
    <abstract>
      This paper presents the BUCC 2017 shared task on parallel sentence
      extraction from comparable corpora. It recalls the design of the datasets,
      presents their final construction and statistics and the methods used to
      evaluate system results. 13 runs were submitted to the shared task by 4
      teams, covering three of the four proposed language pairs: French-English
      (7 runs), German-English (3 runs), and Chinese-English (3 runs). The best
      F-scores as measured against the gold standard were 0.84 (German-English),
      0.80 (French-English), and 0.43 (Chinese-English). Because of the design
      of the dataset, in which not all gold parallel sentence pairs are known,
      these are only minimum values. We examined manually a small sample of the
      false negative sentence pairs for the most precise French-English runs and
      estimated the number of parallel sentence pairs not yet in the provided
      gold standard. Adding them to the gold standard leads to revised estimates
      for the French-English F-scores of at most +1.5pt. This suggests that the
      BUCC 2017 datasets provide a reasonable approximate evaluation of the
      parallel sentence spotting task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zweigenbaum-sharoff-rapp:2017:BUCC</bibkey>
  </paper>
  <paper id='2600'>
    <title>Proceedings of the 2nd Workshop on Representation Learning for NLP</title>
    <editor>Phil Blunsom</editor>
    <editor>Antoine Bordes</editor>
    <editor>Kyunghyun Cho</editor>
    <editor>Shay Cohen</editor>
    <editor>Chris Dyer</editor>
    <editor>Edward Grefenstette</editor>
    <editor>Karl Moritz Hermann</editor>
    <editor>Laura Rimell</editor>
    <editor>Jason Weston</editor>
    <editor>Scott Yih</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-26</url>
    <doi>10.18653/v1/W17-26</doi>
    <bibtype>book</bibtype>
    <bibkey>RepL4NLP:2017</bibkey>
  </paper>
  <paper id='2601'>
    <title>
      Sense Contextualization in a Dependency-Based Compositional Distributional
      Model
    </title>
    <author>
      <first>Pablo</first>
      <last>Gamallo</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W17-2601</url>
    <doi>10.18653/v1/W17-2601</doi>
    <abstract>
      Little attention has been paid to distributional compositional methods
      which employ syntactically structured vector models. As word vectors
      belonging to different syntactic categories have incompatible syntactic
      distributions, no trivial compositional operation can be applied to
      combine them into a new compositional vector. In this article, we
      generalize the method described by Erk and Padó (2009) by proposing a
      dependency-base framework that contextualize not only lemmas but also
      selectional preferences. The main contribution of the article is to expand
      their model to a fully compositional framework in which syntactic
      dependencies are put at the core of semantic composition. We claim that
      semantic composition is mainly driven by syntactic dependencies. Each
      syntactic dependency generates two new compositional vectors representing
      the contextualized sense of the two related lemmas. The sequential
      application of the compositional operations associated to the dependencies
      results in as many contextualized vectors as lemmas the composite
      expression contains. At the end of the semantic process, we do not obtain
      a single compositional vector representing the semantic denotation of the
      whole composite expression, but one contextualized vector for each lemma
      of the whole expression. Our method avoids the troublesome high-order
      tensor representations by defining lemmas and selectional restrictions as
      first-order tensors (i.e. standard vectors). A corpus-based experiment is
      performed to both evaluate the quality of the compositional vectors built
      with our strategy, and to compare them to other approaches on
      distributional compositional semantics. The experiments show that our
      dependency-based compositional method performs as (or even better than)
      the state-of-the-art.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gamallo:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2602'>
    <title>Context encoders as a simple but powerful extension of word2vec</title>
    <author>
      <first>Franziska</first>
      <last>Horn</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–14</pages>
    <url>http://www.aclweb.org/anthology/W17-2602</url>
    <doi>10.18653/v1/W17-2602</doi>
    <abstract>
      With a strikingly simple architecture and the ability to learn meaningful
      word embeddings efficiently from texts containing billions of words,
      word2vec remains one of the most popular neural language models used
      today. However, as only a single embedding is learned for every word in
      the vocabulary, the model fails to optimally represent words with multiple
      meanings and, additionally, it is not possible to create embeddings for
      new (out-of-vocabulary) words on the spot. Based on an intuitive
      interpretation of the continuous bag-of-words (CBOW) word2vec model's
      negative sampling training objective in terms of predicting context based
      similarities, we motivate an extension of the model we call context
      encoders (ConEc). By multiplying the matrix of trained word2vec embeddings
      with a word's average context vector, out-of-vocabulary (OOV) embeddings
      and representations for words with multiple meanings can be created based
      on the words' local contexts. The benefits of this approach are
      illustrated by using these word embeddings as features in the CoNLL 2003
      named entity recognition (NER) task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>horn:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2603'>
    <title>Machine Comprehension by Text-to-Text Neural Question Generation</title>
    <author>
      <first>Xingdi</first>
      <last>Yuan</last>
    </author>
    <author>
      <first>Tong</first>
      <last>Wang</last>
    </author>
    <author>
      <first>Caglar</first>
      <last>Gulcehre</last>
    </author>
    <author>
      <first>Alessandro</first>
      <last>Sordoni</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Bachman</last>
    </author>
    <author>
      <first>Saizheng</first>
      <last>Zhang</last>
    </author>
    <author>
      <first>Sandeep</first>
      <last>Subramanian</last>
    </author>
    <author>
      <first>Adam</first>
      <last>Trischler</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>15–25</pages>
    <url>http://www.aclweb.org/anthology/W17-2603</url>
    <doi>10.18653/v1/W17-2603</doi>
    <abstract>
      We propose a recurrent neural model that generates natural-language
      questions from documents, conditioned on answers. We show how to train the
      model using a combination of supervised and reinforcement learning. After
      teacher forcing for standard maximum likelihood training, we fine-tune the
      model using policy gradient techniques to maximize several rewards that
      measure question quality. Most notably, one of these rewards is the
      performance of a question-answering system. We motivate question
      generation as a means to improve the performance of question answering
      systems. Our model is trained and evaluated on the recent
      question-answering dataset SQuAD.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yuan-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2604'>
    <title>Emergent Predication Structure in Hidden State Vectors of Neural Readers</title>
    <author>
      <first>Hai</first>
      <last>Wang</last>
    </author>
    <author>
      <first>Takeshi</first>
      <last>Onishi</last>
    </author>
    <author>
      <first>Kevin</first>
      <last>Gimpel</last>
    </author>
    <author>
      <first>David</first>
      <last>McAllester</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–36</pages>
    <url>http://www.aclweb.org/anthology/W17-2604</url>
    <doi>10.18653/v1/W17-2604</doi>
    <abstract>
      A significant number of neural architectures for reading comprehension
      have recently been developed and evaluated on large cloze-style datasets.
      We present experiments supporting the emergence of "predication structure"
      in the hidden state vectors of these readers. More specifically, we
      provide evidence that the hidden state vectors represent atomic formulas
      Phi[c] where Phi is a semantic property (predicate) and c is a constant
      symbol entity identifier.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2605'>
    <title>Towards Harnessing Memory Networks for Coreference Resolution</title>
    <author>
      <first>Joe</first>
      <last>Cheri</last>
    </author>
    <author>
      <first>Pushpak</first>
      <last>Bhattacharyya</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–42</pages>
    <url>http://www.aclweb.org/anthology/W17-2605</url>
    <doi>10.18653/v1/W17-2605</doi>
    <abstract>
      Coreference resolution task demands comprehending a discourse, especially
      for anaphoric mentions which require semantic information for resolving
      antecedents. We investigate into how memory networks can be helpful for
      coreference resolution when posed as question answering problem. The
      comprehension capability of memory networks assists coreference
      resolution, particularly for the mentions that require semantic and
      context information. We experiment memory networks for coreference
      resolution, with 4 synthetic datasets generated for coreference resolu-
      tion with varying difficulty levels. Our system’s performance is compared
      with a traditional coreference resolution system to show why memory
      network can be promising for coreference resolution.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cheri-bhattacharyya:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2606'>
    <title>
      Combining Word-Level and Character-Level Representations for Relation
      Classification of Informal Text
    </title>
    <author>
      <first>Dongyun</first>
      <last>Liang</last>
    </author>
    <author>
      <first>Weiran</first>
      <last>Xu</last>
    </author>
    <author>
      <first>Yinge</first>
      <last>Zhao</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–47</pages>
    <url>http://www.aclweb.org/anthology/W17-2606</url>
    <doi>10.18653/v1/W17-2606</doi>
    <abstract>
      Word representation models have achieved great success in natural language
      processing tasks, such as relation classification. However, it does not
      always work on informal text, and the morphemes of some misspelling words
      may carry important short-distance semantic information. We propose a
      hybrid model, combining the merits of word-level and character-level
      representations to learn better representations on informal text.
      Experiments on two dataset of relation classification, SemEval-2010 Task8
      and a large-scale one we compile from informal text, show that our model
      achieves a competitive result in the former and state-of-the-art with the
      other.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>liang-xu-zhao:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2607'>
    <title>Transfer Learning for Neural Semantic Parsing</title>
    <author>
      <first>Xing</first>
      <last>Fan</last>
    </author>
    <author>
      <first>Emilio</first>
      <last>Monti</last>
    </author>
    <author>
      <first>Lambert</first>
      <last>Mathias</last>
    </author>
    <author>
      <first>Markus</first>
      <last>Dreyer</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–56</pages>
    <url>http://www.aclweb.org/anthology/W17-2607</url>
    <doi>10.18653/v1/W17-2607</doi>
    <abstract>
      The goal of semantic parsing is to map natural language to a machine
      interpretable meaning representation language (MRL). One of the
      constraints that limits full exploration of deep learning technologies for
      semantic parsing is the lack of sufficient annotation training data. In
      this paper, we propose using sequence-to-sequence in a multi-task setup
      for semantic parsing with focus on transfer learning. We explore three
      multi-task architectures for sequence-to-sequence model and compare their
      performance with the independently trained model. Our experiments show
      that the multi-task setup aids transfer learning from an auxiliary task
      with large labeled data to the target task with smaller labeled data. We
      see an absolute accuracy gain ranging from 1.0% to 4.4% in in our in-house
      data set and we also see good gains ranging from 2.5% to 7.0% on the ATIS
      semantic parsing tasks with syntactic and semantic auxiliary tasks.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fan-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2608'>
    <title>
      Modeling Large-Scale Structured Relationships with Shared Memory for
      Knowledge Base Completion
    </title>
    <author>
      <first>Yelong</first>
      <last>Shen</last>
    </author>
    <author>
      <first>Po-Sen</first>
      <last>Huang</last>
    </author>
    <author>
      <first>Ming-Wei</first>
      <last>Chang</last>
    </author>
    <author>
      <first>Jianfeng</first>
      <last>Gao</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–68</pages>
    <url>http://www.aclweb.org/anthology/W17-2608</url>
    <doi>10.18653/v1/W17-2608</doi>
    <abstract>
      Recent studies on knowledge base completion, the task of recovering
      missing relationships based on recorded relations, demonstrate the
      importance of learning embeddings from multi-step relations. However, due
      to the size of knowledge bases, learning multi-step relations directly on
      top of observed triplets could be costly. Hence, a manually designed
      procedure is often used when training the models. In this paper, we
      propose Implicit ReasoNets (IRNs), which is designed to perform multi-step
      inference implicitly through a controller and shared memory. Without a
      human-designed inference procedure, IRNs use training data to learn to
      perform multi-step inference in an embedding neural space through the
      shared memory and controller. While the inference procedure does not
      explicitly operate on top of observed triplets, our proposed model
      outperforms all previous approaches on the popular FB15k benchmark by more
      than 5.7%.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shen-EtAl:2017:RepL4NLP1</bibkey>
  </paper>
  <paper id='2609'>
    <title>Knowledge Base Completion: Baselines Strike Back</title>
    <author>
      <first>Rudolf</first>
      <last>Kadlec</last>
    </author>
    <author>
      <first>Ondrej</first>
      <last>Bajgar</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Kleindienst</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69–74</pages>
    <url>http://www.aclweb.org/anthology/W17-2609</url>
    <doi>10.18653/v1/W17-2609</doi>
    <abstract>
      Many papers have been published on the knowledge base completion task in
      the past few years. Most of these introduce novel architectures for
      relation learning that are evaluated on standard datasets like FB15k and
      WN18. This paper shows that the accuracy of almost all models published on
      the FB15k can be outperformed by an appropriately tuned baseline –- our
      reimplementation of the DistMult model. Our findings cast doubt on the
      claim that the performance improvements of recent models are due to
      architectural changes as opposed to hyper-parameter tuning or different
      training objectives. This should prompt future research to re-consider how
      the performance of models is evaluated and reported.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kadlec-bajgar-kleindienst:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2610'>
    <title>Sequential Attention: A Context-Aware Alignment Function for Machine Reading</title>
    <author>
      <first>Sebastian</first>
      <last>Brarda</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Yeres</last>
    </author>
    <author>
      <first>Samuel</first>
      <last>Bowman</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>75–80</pages>
    <url>http://www.aclweb.org/anthology/W17-2610</url>
    <doi>10.18653/v1/W17-2610</doi>
    <abstract>
      In this paper we propose a neural network model with a novel Sequential
      Attention layer that extends soft attention by assigning weights to words
      in an input sequence in a way that takes into account not just how well
      that word matches a query, but how well surrounding words match. We
      evaluate this approach on the task of reading comprehension (on the Who
      did What and CNN datasets) and show that it dramatically improves a strong
      baseline–-the Stanford Reader–-and is competitive with the state of the
      art.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>brarda-yeres-bowman:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2611'>
    <title>Semantic Vector Encoding and Similarity Search Using Fulltext Search Engines</title>
    <author>
      <first>Jan</first>
      <last>Rygl</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Pomikálek</last>
    </author>
    <author>
      <first>Radim</first>
      <last>Řehůřek</last>
    </author>
    <author>
      <first>Michal</first>
      <last>Růžička</last>
    </author>
    <author>
      <first>Vít</first>
      <last>Novotný</last>
    </author>
    <author>
      <first>Petr</first>
      <last>Sojka</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>81–90</pages>
    <url>http://www.aclweb.org/anthology/W17-2611</url>
    <doi>10.18653/v1/W17-2611</doi>
    <abstract>
      Vector representations and vector space modeling (VSM) play a central role
      in modern machine learning. We propose a novel approach to ‘vector
      similarity searching’ over dense semantic representations of words and
      documents that can be deployed on top of traditional inverted-index-based
      fulltext engines, taking advantage of their robustness, stability,
      scalability and ubiquity. We show that this approach allows the indexing
      and querying of dense vectors in text domains. This opens up exciting
      avenues for major efficiency gains, along with simpler deployment, scaling
      and monitoring. The end result is a fast and scalable vector database with
      a tunable trade-off between vector search performance and quality, backed
      by a standard fulltext engine such as Elasticsearch. We empirically
      demonstrate its querying performance and quality by applying this solution
      to the task of semantic searching over a dense vector representation of
      the entire English Wikipedia.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rygl-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2612'>
    <title>Multi-task Domain Adaptation for Sequence Tagging</title>
    <author>
      <first>Nanyun</first>
      <last>Peng</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Dredze</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–100</pages>
    <url>http://www.aclweb.org/anthology/W17-2612</url>
    <doi>10.18653/v1/W17-2612</doi>
    <abstract>
      Many domain adaptation approaches rely on learning cross domain shared
      representations to transfer the knowledge learned in one domain to other
      domains. Traditional domain adaptation only considers adapting for one
      task. In this paper, we explore multi-task representation learning under
      the domain adaptation scenario. We propose a neural network framework that
      supports domain adaptation for multiple tasks simultaneously, and learns
      shared representations that better generalize for domain adaptation. We
      apply the proposed framework to domain adaptation for sequence tagging
      problems considering two tasks: Chinese word segmentation and named entity
      recognition. Experiments show that multi-task domain adaptation works
      better than disjoint domain adaptation for each task, and achieves the
      state-of-the-art results for both tasks in the social media domain.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>peng-dredze:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2613'>
    <title>Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context</title>
    <author>
      <first>Shyam</first>
      <last>Upadhyay</last>
    </author>
    <author>
      <first>Kai-Wei</first>
      <last>Chang</last>
    </author>
    <author>
      <first>Matt</first>
      <last>Taddy</last>
    </author>
    <author>
      <first>Adam</first>
      <last>Kalai</last>
    </author>
    <author>
      <first>James</first>
      <last>Zou</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>101–110</pages>
    <url>http://www.aclweb.org/anthology/W17-2613</url>
    <doi>10.18653/v1/W17-2613</doi>
    <abstract>
      Word embeddings, which represent a word as a point in a vector space, have
      become ubiquitous to several NLP tasks. A recent line of work uses
      bilingual (two languages) corpora to learn a different vector for each
      sense of a word, by exploiting crosslingual signals to aid sense
      identification. We present a multi-view Bayesian non-parametric algorithm
      which improves multi-sense wor d embeddings by (a) using multilingual
      (i.e., more than two languages) corpora to significantly improve sense
      embeddings beyond what one achieves with bilingual information, and (b)
      uses a principled approach to learn a variable number of senses per word,
      in a data-driven manner. Ours is the first approach with the ability to
      leverage multilingual corpora efficiently for multi-sense representation
      learning. Experiments show that multilingual training significantly
      improves performance over monolingual and bilingual training, by allowing
      us to combine different parallel corpora to leverage multilingual context.
      Multilingual training yields comparable performance to a state of the art
      monolingual model trained on five times more training data.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>upadhyay-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2614'>
    <title>
      DocTag2Vec: An Embedding Based Multi-label Learning Approach for Document
      Tagging
    </title>
    <author>
      <first>Sheng</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Akshay</first>
      <last>Soni</last>
    </author>
    <author>
      <first>Aasish</first>
      <last>Pappu</last>
    </author>
    <author>
      <first>Yashar</first>
      <last>Mehdad</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>111–120</pages>
    <url>http://www.aclweb.org/anthology/W17-2614</url>
    <doi>10.18653/v1/W17-2614</doi>
    <abstract>
      Tagging news articles or blog posts with relevant tags from a collection
      of predefined ones is coined as document tagging in this work. Accurate
      tagging of articles can benefit several downstream applications such as
      recommendation and search. In this work, we propose a novel yet simple
      approach called DocTag2Vec to accomplish this task. We substantially
      extend Word2Vec and Doc2Vec – two popular models for learning distributed
      representation of words and documents. In DocTag2Vec, we simultaneously
      learn the representation of words, documents, and tags in a joint vector
      space during training, and employ the simple k-nearest neighbor search to
      predict tags for unseen documents. In contrast to previous multi-label
      learning methods, DocTag2Vec directly deals with raw text instead of
      provided feature vector, and in addition, enjoys advantages like the
      learning of tag representation, and the ability of handling newly created
      tags. To demonstrate the effectiveness of our approach, we conduct
      experiments on several datasets and show promising results against
      state-of-the-art methods.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2615'>
    <title>Binary Paragraph Vectors</title>
    <author>
      <first>Karol</first>
      <last>Grzegorczyk</last>
    </author>
    <author>
      <first>Marcin</first>
      <last>Kurdziel</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>121–130</pages>
    <url>http://www.aclweb.org/anthology/W17-2615</url>
    <doi>10.18653/v1/W17-2615</doi>
    <abstract>
      Recently Le & Mikolov described two log-linear models, called
      Paragraph Vector, that can be used to learn state-of-the-art distributed
      representations of documents. Inspired by this work, we present Binary
      Paragraph Vector models: simple neural networks that learn short binary
      codes for fast information retrieval. We show that binary paragraph
      vectors outperform autoencoder-based binary codes, despite using fewer
      bits. We also evaluate their precision in transfer learning settings,
      where binary codes are inferred for documents unrelated to the training
      corpus. Results from these experiments indicate that binary paragraph
      vectors can capture semantics relevant for various domain-specific
      documents. Finally, we present a model that simultaneously learns short
      binary codes and longer, real-valued representations. This model can be
      used to rapidly retrieve a short list of highly relevant documents from a
      large document collection.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grzegorczyk-kurdziel:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2616'>
    <title>
      Representing Compositionality based on Multiple Timescales Gated Recurrent
      Neural Networks with Adaptive Temporal Hierarchy for Character-Level
      Language Models
    </title>
    <author>
      <first>Dennis Singh</first>
      <last>Moirangthem</last>
    </author>
    <author>
      <first>Jegyung</first>
      <last>Son</last>
    </author>
    <author>
      <first>Minho</first>
      <last>Lee</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>131–138</pages>
    <url>http://www.aclweb.org/anthology/W17-2616</url>
    <doi>10.18653/v1/W17-2616</doi>
    <abstract>
      A novel character-level neural language model is proposed in this paper.
      The proposed model incorporates a biologically inspired temporal hierarchy
      in the architecture for representing multiple compositions of language in
      order to handle longer sequences for the character-level language model.
      The temporal hierarchy is introduced in the language model by utilizing a
      Gated Recurrent Neural Network with multiple timescales. The proposed
      model incorporates a timescale adaptation mechanism for enhancing the
      performance of the language model. We evaluate our proposed model using
      the popular Penn Treebank and Text8 corpora. The experiments show that the
      use of multiple timescales in a Neural Language Model (NLM) enables
      improved performance despite having fewer parameters and with no
      additional computation requirements. Our experiments also demonstrate the
      ability of the adaptive temporal hierarchies to represent multiple
      compositonality without the help of complex hierarchical architectures and
      shows that better representation of the longer sequences lead to enhanced
      performance of the probabilistic language model.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>moirangthem-son-lee:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2617'>
    <title>
      Learning Bilingual Projections of Embeddings for Vocabulary Expansion in
      Machine Translation
    </title>
    <author>
      <first>Pranava Swaroop</first>
      <last>Madhyastha</last>
    </author>
    <author>
      <first>Cristina</first>
      <last>España-Bonet</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>139–145</pages>
    <url>http://www.aclweb.org/anthology/W17-2617</url>
    <doi>10.18653/v1/W17-2617</doi>
    <abstract>
      We propose a simple log-bilinear softmax-based model to deal with
      vocabulary expansion in machine translation. Our model uses word
      embeddings trained on significantly large unlabelled monolingual corpora
      and learns over a fairly small, word-to-word bilingual dictionary. Given
      an out-of-vocabulary source word, the model generates a probabilistic list
      of possible translations in the target language using the trained
      bilingual embeddings. We integrate these translation options into a
      standard phrase-based statistical machine translation system and obtain
      consistent improvements in translation quality on the English–Spanish
      language pair. When tested over an out-of-domain testset, we get a
      significant improvement of 3.9 BLEU points.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>madhyastha-espanabonet:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2618'>
    <title>
      Prediction of Frame-to-Frame Relations in the FrameNet Hierarchy with
      Frame Embeddings
    </title>
    <author>
      <first>Teresa</first>
      <last>Botschen</last>
    </author>
    <author>
      <first>Hatem</first>
      <last>Mousselly Sergieh</last>
    </author>
    <author>
      <first>Iryna</first>
      <last>Gurevych</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>146–156</pages>
    <url>http://www.aclweb.org/anthology/W17-2618</url>
    <doi>10.18653/v1/W17-2618</doi>
    <abstract>
      Automatic completion of frame-to-frame (F2F) relations in the FrameNet
      (FN) hierarchy has received little attention, although they incorporate
      meta-level commonsense knowledge and are used in downstream approaches. We
      address the problem of sparsely annotated F2F relations. First, we examine
      whether the manually defined F2F relations emerge from text by learning
      text-based frame embeddings. Our analysis reveals insights about the
      difficulty of reconstructing F2F relations purely from text. Second, we
      present different systems for predicting F2F relations; our
      best-performing one uses the FN hierarchy to train on and to ground
      embeddings in. A comparison of systems and embeddings exposes the crucial
      influence of knowledge-based embeddings to a system’s performance in
      predicting F2F relations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>botschen-moussellysergieh-gurevych:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2619'>
    <title>
      Learning Joint Multilingual Sentence Representations with Neural Machine
      Translation
    </title>
    <author>
      <first>Holger</first>
      <last>Schwenk</last>
    </author>
    <author>
      <first>Matthijs</first>
      <last>Douze</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>157–167</pages>
    <url>http://www.aclweb.org/anthology/W17-2619</url>
    <doi>10.18653/v1/W17-2619</doi>
    <abstract>
      In this paper, we use the framework of neural machine translation to learn
      joint sentence representations across six very different languages. Our
      aim is that a representation which is independent of the language, is
      likely to capture the underlying semantics. We define a new cross-lingual
      similarity measure, compare up to 1.4M sentence representations and study
      the characteristics of close sentences. We provide experimental evidence
      that sentences that are close in embedding space are indeed semantically
      highly related, but often have quite different structure and syntax. These
      relations also hold when comparing sentences in different languages.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schwenk-douze:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2620'>
    <title>Transfer Learning for Speech Recognition on a Budget</title>
    <author>
      <first>Julius</first>
      <last>Kunze</last>
    </author>
    <author>
      <first>Louis</first>
      <last>Kirsch</last>
    </author>
    <author>
      <first>Ilia</first>
      <last>Kurenkov</last>
    </author>
    <author>
      <first>Andreas</first>
      <last>Krug</last>
    </author>
    <author>
      <first>Jens</first>
      <last>Johannsmeier</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Stober</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>168–177</pages>
    <url>http://www.aclweb.org/anthology/W17-2620</url>
    <doi>10.18653/v1/W17-2620</doi>
    <abstract>
      End-to-end training of automated speech recognition (ASR) systems requires
      massive data and compute resources. We explore transfer learning based on
      model adaptation as an approach for training ASR models under constrained
      GPU memory, throughput and training data. We conduct several systematic
      experiments adapting a Wav2Letter convolutional neural network originally
      trained for English ASR to the German language. We show that this
      technique allows faster training on consumer-grade resources while
      requiring less training data in order to achieve the same accuracy,
      thereby lowering the cost of training ASR models in other languages. Model
      introspection revealed that small adaptations to the network's weights
      were sufficient for good performance, especially for inner layers.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kunze-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2621'>
    <title>Gradual Learning of Matrix-Space Models of Language for Sentiment Analysis</title>
    <author>
      <first>Shima</first>
      <last>Asaadi</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Rudolph</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>178–185</pages>
    <url>http://www.aclweb.org/anthology/W17-2621</url>
    <doi>10.18653/v1/W17-2621</doi>
    <abstract>
      Learning word representations to capture the semantics and
      compositionality of language has received much research interest in
      natural language processing. Beyond the popular vector space models,
      matrix representations for words have been proposed, since then, matrix
      multiplication can serve as natural composition operation. In this work,
      we investigate the problem of learning matrix representations of words. We
      present a learning approach for compositional matrix-space models for the
      task of sentiment analysis. We show that our approach, which learns the
      matrices gradually in two steps, outperforms other approaches and a
      gradient-descent baseline in terms of quality and computational cost.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>asaadi-rudolph:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2622'>
    <title>Improving Language Modeling using Densely Connected Recurrent Neural Networks</title>
    <author>
      <first>Fréderic</first>
      <last>Godin</last>
    </author>
    <author>
      <first>Joni</first>
      <last>Dambre</last>
    </author>
    <author>
      <first>Wesley</first>
      <last>De Neve</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>186–190</pages>
    <url>http://www.aclweb.org/anthology/W17-2622</url>
    <doi>10.18653/v1/W17-2622</doi>
    <abstract>
      In this paper, we introduce the novel concept of densely connected layers
      into recurrent neural networks. We evaluate our proposed architecture on
      the Penn Treebank language modeling task. We show that we can obtain
      similar perplexity scores with six times fewer parameters compared to a
      standard stacked 2- layer LSTM model trained with dropout (Zaremba et al.,
      2014). In contrast with the current usage of skip connections, we show
      that densely connecting only a few stacked layers with skip connections
      already yields significant perplexity reductions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>godin-dambre-deneve:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2623'>
    <title>NewsQA: A Machine Comprehension Dataset</title>
    <author>
      <first>Adam</first>
      <last>Trischler</last>
    </author>
    <author>
      <first>Tong</first>
      <last>Wang</last>
    </author>
    <author>
      <first>Xingdi</first>
      <last>Yuan</last>
    </author>
    <author>
      <first>Justin</first>
      <last>Harris</last>
    </author>
    <author>
      <first>Alessandro</first>
      <last>Sordoni</last>
    </author>
    <author>
      <first>Philip</first>
      <last>Bachman</last>
    </author>
    <author>
      <first>Kaheer</first>
      <last>Suleman</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>191–200</pages>
    <url>http://www.aclweb.org/anthology/W17-2623</url>
    <doi>10.18653/v1/W17-2623</doi>
    <abstract>
      We present NewsQA, a challenging machine comprehension dataset of over
      100,000 human-generated question-answer pairs. Crowdworkers supply
      questions and answers based on a set of over 10,000 news articles from
      CNN, with answers consisting of spans of text in the articles. We collect
      this dataset through a four-stage process designed to solicit exploratory
      questions that require reasoning. Analysis confirms that NewsQA demands
      abilities beyond simple word matching and recognizing textual entailment.
      We measure human performance on the dataset and compare it to several
      strong neural models. The performance gap between humans and machines
      (13.3% F1) indicates that significant progress can be made on NewsQA
      through future research. The dataset is freely available online.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>trischler-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2624'>
    <title>
      Intrinsic and Extrinsic Evaluation of Spatiotemporal Text Representations
      in Twitter Streams
    </title>
    <author>
      <first>Lawrence</first>
      <last>Phillips</last>
    </author>
    <author>
      <first>Kyle</first>
      <last>Shaffer</last>
    </author>
    <author>
      <first>Dustin</first>
      <last>Arendt</last>
    </author>
    <author>
      <first>Nathan</first>
      <last>Hodas</last>
    </author>
    <author>
      <first>Svitlana</first>
      <last>Volkova</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>201–210</pages>
    <url>http://www.aclweb.org/anthology/W17-2624</url>
    <doi>10.18653/v1/W17-2624</doi>
    <abstract>
      Language in social media is a dynamic system, constantly evolving and
      adapting, with words and concepts rapidly emerging, disappearing, and
      changing their meaning. These changes can be estimated using word
      representations in context, over time and across locations. A number of
      methods have been proposed to track these spatiotemporal changes but no
      general method exists to evaluate the quality of these representations.
      Previous work largely focused on qualitative evaluation, which we improve
      by proposing a set of visualizations that highlight changes in text
      representation over both space and time. We demonstrate usefulness of
      novel spatiotemporal representations to explore and characterize specific
      aspects of the corpus of tweets collected from European countries over a
      two-week period centered around the terrorist attacks in Brussels in March
      2016. In addition, we quantitatively evaluate spatiotemporal
      representations by feeding them into a downstream classification task –
      event type prediction. Thus, our work is the first to provide both
      intrinsic (qualitative) and extrinsic (quantitative) evaluation of text
      representations for spatiotemporal trends.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>phillips-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2625'>
    <title>Rethinking Skip-thought: A Neighborhood based Approach</title>
    <author>
      <first>Shuai</first>
      <last>Tang</last>
    </author>
    <author>
      <first>Hailin</first>
      <last>Jin</last>
    </author>
    <author>
      <first>Chen</first>
      <last>Fang</last>
    </author>
    <author>
      <first>Zhaowen</first>
      <last>Wang</last>
    </author>
    <author>
      <first>Virginia</first>
      <last>de Sa</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>211–218</pages>
    <url>http://www.aclweb.org/anthology/W17-2625</url>
    <doi>10.18653/v1/W17-2625</doi>
    <abstract>
      We study the skip-thought model with neighborhood information as weak
      supervision. More specifically, we propose a skip-thought neighbor model
      to consider the adjacent sentences as a neighborhood. We train our
      skip-thought neighbor model on a large corpus with continuous sentences,
      and then evaluate the trained model on 7 tasks, which include semantic
      relatedness, paraphrase detection, and classification benchmarks. Both
      quantitative comparison and qualitative investigation are conducted. We
      empirically show that, our skip-thought neighbor model performs as well as
      the skip-thought model on evaluation tasks. In addition, we found that,
      incorporating an autoencoder path in our model didn't aid our model to
      perform better, while it hurts the performance of the skip-thought model.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tang-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2626'>
    <title>A Frame Tracking Model for Memory-Enhanced Dialogue Systems</title>
    <author>
      <first>Hannes</first>
      <last>Schulz</last>
    </author>
    <author>
      <first>Jeremie</first>
      <last>Zumer</last>
    </author>
    <author>
      <first>Layla</first>
      <last>El Asri</last>
    </author>
    <author>
      <first>Shikhar</first>
      <last>Sharma</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>219–227</pages>
    <url>http://www.aclweb.org/anthology/W17-2626</url>
    <doi>10.18653/v1/W17-2626</doi>
    <abstract>
      Recently, resources and tasks were proposed to go beyond state tracking in
      dialogue systems. An example is the frame tracking task, which requires
      recording multiple frames, one for each user goal set during the dialogue.
      This allows a user, for instance, to compare items corresponding to
      different goals. This paper proposes a model which takes as input the list
      of frames created so far during the dialogue, the current user utterance
      as well as the dialogue acts, slot types, and slot values associated with
      this utterance. The model then outputs the frame being referenced by each
      triple of dialogue act, slot type, and slot value. We show that on the
      recently published Frames dataset, this model significantly outperforms a
      previously proposed rule-based baseline. In addition, we propose an
      extensive analysis of the frame tracking task by dividing it into
      sub-tasks and assessing their difficulty with respect to our model.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schulz-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2627'>
    <title>
      Plan, Attend, Generate: Character-Level Neural Machine Translation with
      Planning
    </title>
    <author>
      <first>Caglar</first>
      <last>Gulcehre</last>
    </author>
    <author>
      <first>Francis</first>
      <last>Dutil</last>
    </author>
    <author>
      <first>Adam</first>
      <last>Trischler</last>
    </author>
    <author>
      <first>Yoshua</first>
      <last>Bengio</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>228–234</pages>
    <url>http://www.aclweb.org/anthology/W17-2627</url>
    <doi>10.18653/v1/W17-2627</doi>
    <abstract>
      We investigate the integration of a planning mechanism into an
      encoder-decoder architecture with attention. We develop a model that can
      plan ahead when it computes alignments between the source and target
      sequences not only for a single time-step but for the next k time-steps as
      well by constructing a matrix of proposed future alignments and a
      commitment vector that governs whether to follow or recompute the plan.
      This mechanism is inspired by strategic attentive reader and writer
      (STRAW) model, a recent neural architecture for planning with hierarchical
      reinforcement learning that can also learn higher level temporal
      abstractions. Our proposed model is end-to-end trainable with
      differentiable operations. We show that our model outperforms strong
      baselines on character-level translation task from WMT'15 with fewer
      parameters and computes alignments that are qualitatively intuitive.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gulcehre-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2628'>
    <title>
      Does the Geometry of Word Embeddings Help Document Classification? A Case
      Study on Persistent Homology-Based Representations
    </title>
    <author>
      <first>Paul</first>
      <last>Michel</last>
    </author>
    <author>
      <first>Abhilasha</first>
      <last>Ravichander</last>
    </author>
    <author>
      <first>Shruti</first>
      <last>Rijhwani</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>235–240</pages>
    <url>http://www.aclweb.org/anthology/W17-2628</url>
    <doi>10.18653/v1/W17-2628</doi>
    <abstract>
      We investigate the pertinence of methods from algebraic topology for text
      data analysis. These methods enable the development of
      mathematically-principled isometric-invariant mappings from a set of
      vectors to a document embedding, which is stable with respect to the
      geometry of the document in the selected metric space. In this work, we
      evaluate the utility of these topology-based document representations in
      traditional NLP tasks, specifically document clustering and sentiment
      classification. We find that the embeddings do not benefit text analysis.
      In fact, performance is worse than simple techniques like tf-idf,
      indicating that the geometry of the document does not provide enough
      variability for classification on the basis of topic or sentiment in the
      chosen datasets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>michel-ravichander-rijhwani:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2629'>
    <title>Adversarial Generation of Natural Language</title>
    <author>
      <first>Sandeep</first>
      <last>Subramanian</last>
    </author>
    <author>
      <first>Sai</first>
      <last>Rajeswar</last>
    </author>
    <author>
      <first>Francis</first>
      <last>Dutil</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Pal</last>
    </author>
    <author>
      <first>Aaron</first>
      <last>Courville</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>241–251</pages>
    <url>http://www.aclweb.org/anthology/W17-2629</url>
    <doi>10.18653/v1/W17-2629</doi>
    <abstract>
      Generative Adversarial Networks (GANs) have gathered a lot of attention
      from the computer vision community, yielding impressive results for image
      generation. Advances in the adversarial generation of natural language
      from noise however are not commensurate with the progress made in
      generating images, and still lag far behind likelihood based methods. In
      this paper, we take a step towards generating natural language with a GAN
      objective alone. We introduce a simple baseline that addresses the
      discrete output space problem without relying on gradient estimators and
      show that it is able to achieve state-of-the-art results on a Chinese poem
      generation dataset. We present quantitative results on generating
      sentences from context-free and probabilistic context-free grammars, and
      qualitative language modeling results. A conditional version is also
      described that can generate sequences conditioned on sentence
      characteristics.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>subramanian-EtAl:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2630'>
    <title>Deep Active Learning for Named Entity Recognition</title>
    <author>
      <first>Yanyao</first>
      <last>Shen</last>
    </author>
    <author>
      <first>Hyokun</first>
      <last>Yun</last>
    </author>
    <author>
      <first>Zachary</first>
      <last>Lipton</last>
    </author>
    <author>
      <first>Yakov</first>
      <last>Kronrod</last>
    </author>
    <author>
      <first>Animashree</first>
      <last>Anandkumar</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>252–256</pages>
    <url>http://www.aclweb.org/anthology/W17-2630</url>
    <doi>10.18653/v1/W17-2630</doi>
    <abstract>
      Deep neural networks have advanced the state of the art in named entity
      recognition. However, under typical training procedures, advantages over
      classical methods emerge only with large datasets. As a result, deep
      learning is employed only when large public datasets or a large budget for
      manually labeling data is available. In this work, we show otherwise: by
      combining deep learning with active learning, we can outperform classical
      methods even with a significantly smaller amount of training data.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shen-EtAl:2017:RepL4NLP2</bibkey>
  </paper>
  <paper id='2631'>
    <title>Learning when to skim and when to read</title>
    <author>
      <first>Alexander</first>
      <last>Johansen</last>
    </author>
    <author>
      <first>Richard</first>
      <last>Socher</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>257–264</pages>
    <url>http://www.aclweb.org/anthology/W17-2631</url>
    <doi>10.18653/v1/W17-2631</doi>
    <abstract>
      Many recent advances in deep learning for natural language processing have
      come at increasing computational cost, but the power of these
      state-of-the-art models is not needed for every example in a dataset. We
      demonstrate two approaches to reducing unnecessary computation in cases
      where a fast but weak baseline classier and a stronger, slower model are
      both available. Applying an AUC-based metric to the task of sentiment
      classification, we find significant efficiency gains with both a
      probability-threshold method for reducing computational cost and one that
      uses a secondary decision network.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>johansen-socher:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2632'>
    <title>Learning to Embed Words in Context for Syntactic Tasks</title>
    <author>
      <first>Lifu</first>
      <last>Tu</last>
    </author>
    <author>
      <first>Kevin</first>
      <last>Gimpel</last>
    </author>
    <author>
      <first>Karen</first>
      <last>Livescu</last>
    </author>
    <booktitle>Proceedings of the 2nd Workshop on Representation Learning for NLP</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>265–275</pages>
    <url>http://www.aclweb.org/anthology/W17-2632</url>
    <doi>10.18653/v1/W17-2632</doi>
    <abstract>
      We present models for embedding words in the context of surrounding words.
      Such models, which we refer to as token embeddings, represent the
      characteristics of a word that are specific to a given context, such as
      word sense, syntactic category, and semantic role. We explore simple,
      efficient token embedding models based on standard neural network
      architectures. We learn token embeddings on a large amount of unannotated
      text and evaluate them as features for part-of-speech taggers and
      dependency parsers trained on much smaller amounts of annotated data. We
      find that predictors endowed with token embeddings consistently outperform
      baseline predictors across a range of context window and training set
      sizes.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tu-gimpel-livescu:2017:RepL4NLP</bibkey>
  </paper>
  <paper id='2700'>
    <title>Proceedings of the Events and Stories in the News Workshop</title>
    <editor>Tommaso Caselli</editor>
    <editor>Ben Miller</editor>
    <editor>Marieke van Erp</editor>
    <editor>Piek Vossen</editor>
    <editor>Martha Palmer</editor>
    <editor>Eduard Hovy</editor>
    <editor>Teruko Mitamura</editor>
    <editor>David Caswell</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-27</url>
    <doi>10.18653/v1/W17-27</doi>
    <bibtype>book</bibtype>
    <bibkey>EventStory:2017</bibkey>
  </paper>
  <paper id='2701'>
    <title>newsLens: building and visualizing long-ranging news stories</title>
    <author>
      <first>Philippe</first>
      <last>Laban</last>
    </author>
    <author>
      <first>Marti</first>
      <last>Hearst</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W17-2701</url>
    <doi>10.18653/v1/W17-2701</doi>
    <abstract>
      We propose a method to aggregate and organize a large, multi-source
      dataset of news articles into a collection of major stories, and
      automatically name and visualize these stories in a working system. The
      approach is able to run online, as new articles are added, processing 4
      million news articles from 20 news sources, and extracting 80000 major
      stories, some of which span several years. The visual interface consists
      of lanes of timelines, each annotated with information that is deemed
      important for the story, including extracted quotations. The working
      system allows a user to search and navigate 8 years of story information.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>laban-hearst:2017:EventStory</bibkey>
  </paper>
  <paper id='2702'>
    <title>Detecting Changes in Twitter Streams using Temporal Clusters of Hashtags</title>
    <author>
      <first>Yunli</first>
      <last>Wang</last>
    </author>
    <author>
      <first>Cyril</first>
      <last>Goutte</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–14</pages>
    <url>http://www.aclweb.org/anthology/W17-2702</url>
    <doi>10.18653/v1/W17-2702</doi>
    <abstract>
      Detecting events from social media data has important applications in
      public security, political issues, and public health. Many studies have
      focused on detecting specific or unspecific events from Twitter streams.
      However, not much attention has been paid to detecting changes, and their
      impact, in online conversations related to an event. We propose methods
      for detecting such changes, using clustering of temporal profiles of
      hashtags, and three change point detection algorithms. The methods were
      tested on two Twitter datasets: one covering the 2014 Ottawa shooting
      event, and one covering the Sochi winter Olympics. We compare our approach
      to a baseline consisting of detecting change from raw counts in the
      conversation. We show that our method produces large gains in change
      detection accuracy on both datasets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wang-goutte:2017:EventStory</bibkey>
  </paper>
  <paper id='2703'>
    <title>Event Detection Using Frame-Semantic Parser</title>
    <author>
      <first>Evangelia</first>
      <last>Spiliopoulou</last>
    </author>
    <author>
      <first>Eduard</first>
      <last>Hovy</last>
    </author>
    <author>
      <first>Teruko</first>
      <last>Mitamura</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>15–20</pages>
    <url>http://www.aclweb.org/anthology/W17-2703</url>
    <doi>10.18653/v1/W17-2703</doi>
    <abstract>
      Recent methods for Event Detection focus on Deep Learning for automatic
      feature generation and feature ranking. However, most of those approaches
      fail to exploit rich semantic information, which results in relatively
      poor recall. This paper is a small & focused contribution, where we
      introduce an Event Detection and classification system, based on deep
      semantic information retrieved from a frame-semantic parser. Our
      experiments show that our system achieves higher recall than
      state-of-the-art systems. Further, we claim that enhancing our system with
      deep learning techniques like feature ranking can achieve even better
      results, as it can benefit from both approaches.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>spiliopoulou-hovy-mitamura:2017:EventStory</bibkey>
  </paper>
  <paper id='2704'>
    <title>
      Improving Shared Argument Identification in Japanese Event Knowledge
      Acquisition
    </title>
    <author>
      <first>Yin Jou</first>
      <last>Huang</last>
    </author>
    <author>
      <first>Sadao</first>
      <last>Kurohashi</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21–30</pages>
    <url>http://www.aclweb.org/anthology/W17-2704</url>
    <doi>10.18653/v1/W17-2704</doi>
    <abstract>
      Event knowledge represents the knowledge of causal and temporal relations
      between events. Shared arguments of event knowledge encode patterns of
      role shifting in successive events. A two-stage framework was proposed for
      the task of Japanese event knowledge acquisition, in which related event
      pairs are first extracted, and shared arguments are then identified to
      form the complete event knowledge. This paper focuses on the second stage
      of this framework, and proposes a method to improve the shared argument
      identification of related event pairs. We constructed a gold dataset for
      shared argument learning. By evaluating our system on this gold dataset,
      we found that our proposed model outperformed the baseline models by a
      large margin.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>huang-kurohashi:2017:EventStory</bibkey>
  </paper>
  <paper id='2705'>
    <title>Tracing armed conflicts with diachronic word embedding models</title>
    <author>
      <first>Andrey</first>
      <last>Kutuzov</last>
    </author>
    <author>
      <first>Erik</first>
      <last>Velldal</last>
    </author>
    <author>
      <first>Lilja</first>
      <last>Øvrelid</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–36</pages>
    <url>http://www.aclweb.org/anthology/W17-2705</url>
    <doi>10.18653/v1/W17-2705</doi>
    <abstract>
      Recent studies have shown that word embedding models can be used to trace
      time-related (diachronic) semantic shifts in particular words. In this
      paper, we evaluate some of these approaches on the new task of predicting
      the dynamics of global armed conflicts on a year-to-year basis, using a
      dataset from the conflict research field as the gold standard and the
      Gigaword news corpus as the training data. The results show that much work
      still remains in extracting `cultural' semantic shifts from diachronic
      word embedding models. At the same time, we present a new task complete
      with an evaluation set and introduce the `anchor words' method which
      outperforms previous approaches on this set.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kutuzov-velldal-ovrelid:2017:EventStory</bibkey>
  </paper>
  <paper id='2706'>
    <title>The Circumstantial Event Ontology (CEO)</title>
    <author>
      <first>Roxane</first>
      <last>Segers</last>
    </author>
    <author>
      <first>Tommaso</first>
      <last>Caselli</last>
    </author>
    <author>
      <first>Piek</first>
      <last>Vossen</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–41</pages>
    <url>http://www.aclweb.org/anthology/W17-2706</url>
    <doi>10.18653/v1/W17-2706</doi>
    <abstract>
      In this paper we describe the ongoing work on the Circumstantial Event
      Ontology (CEO), a newly developed ontology for calamity events that models
      semantic circumstantial relations between event classes. The
      circumstantial relations are designed manually, based on the shared
      properties of each event class. We discuss and contrast two types of event
      circumstantial relations: semantic circumstantial relations and episodic
      circumstantial relations. Further, we show the metamodel and the current
      contents of the ontology and outline the evaluation of the CEO.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>segers-caselli-vossen:2017:EventStory</bibkey>
  </paper>
  <paper id='2707'>
    <title>
      Event Detection and Semantic Storytelling: Generating a Travelogue from a
      large Collection of Personal Letters
    </title>
    <author>
      <first>Georg</first>
      <last>Rehm</last>
    </author>
    <author>
      <first>Julian</first>
      <last>Moreno Schneider</last>
    </author>
    <author>
      <first>peter</first>
      <last>bourgonje</last>
    </author>
    <author>
      <first>Ankit</first>
      <last>Srivastava</last>
    </author>
    <author>
      <first>Jan</first>
      <last>Nehring</last>
    </author>
    <author>
      <first>Armin</first>
      <last>Berger</last>
    </author>
    <author>
      <first>Luca</first>
      <last>König</last>
    </author>
    <author>
      <first>Sören</first>
      <last>Räuchle</last>
    </author>
    <author>
      <first>Jens</first>
      <last>Gerth</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>42–51</pages>
    <url>http://www.aclweb.org/anthology/W17-2707</url>
    <doi>10.18653/v1/W17-2707</doi>
    <abstract>
      We present an approach at identifying a specific class of events, movement
      action events (MAEs), in a data set that consists of ca. 2,800 personal
      letters exchanged by the German architect Erich Mendelsohn and his wife,
      Luise. A backend system uses these and other semantic analysis results as
      input for an authoring environment that digital curators can use to
      produce new pieces of digital content. In our example case, the human
      expert will receive recommendations from the system with the goal of
      putting together a travelogue, i.e., a description of the trips and
      journeys undertaken by the couple. We describe the components and
      architecture and also apply the system to news data.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rehm-EtAl:2017:EventStory</bibkey>
  </paper>
  <paper id='2708'>
    <title>Inference of Fine-Grained Event Causality from Blogs and Films</title>
    <author>
      <first>Zhichao</first>
      <last>Hu</last>
    </author>
    <author>
      <first>Elahe</first>
      <last>Rahimtoroghi</last>
    </author>
    <author>
      <first>Marilyn</first>
      <last>Walker</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–58</pages>
    <url>http://www.aclweb.org/anthology/W17-2708</url>
    <doi>10.18653/v1/W17-2708</doi>
    <abstract>
      Human understanding of narrative is mainly driven by reasoning about
      causal relations between events and thus recognizing them is a key
      capability for computational models of language understanding.
      Computational work in this area has approached this via two different
      routes: by focusing on acquiring a knowledge base of common causal
      relations between events, or by attempting to understand a particular
      story or macro-event, along with its storyline. In this position paper, we
      focus on knowledge acquisition approach and claim that newswire is a
      relatively poor source for learning fine-grained causal relations between
      everyday events. We describe experiments using an unsupervised method to
      learn causal relations between events in the narrative genres of
      first-person narratives and film scene descriptions. We show that our
      method learns fine-grained causal relations, judged by humans as likely to
      be causal over 80% of the time. We also demonstrate that the learned event
      pairs do not exist in publicly available event-pair datasets extracted
      from newswire.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hu-rahimtoroghi-walker:2017:EventStory</bibkey>
  </paper>
  <paper id='2709'>
    <title>On the Creation of a Security-Related Event Corpus</title>
    <author>
      <first>Martin</first>
      <last>Atkinson</last>
    </author>
    <author>
      <first>Jakub</first>
      <last>Piskorski</last>
    </author>
    <author>
      <first>Hristo</first>
      <last>Tanev</last>
    </author>
    <author>
      <first>Vanni</first>
      <last>Zavarella</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–65</pages>
    <url>http://www.aclweb.org/anthology/W17-2709</url>
    <doi>10.18653/v1/W17-2709</doi>
    <abstract>
      This paper reports on an effort of creating a corpus of structured
      information on security-related events automatically extracted from
      on-line news, part of which has been manually curated. The main motivation
      behind this effort is to provide material to the NLP community working on
      event extraction that could be used both for training and evaluation
      purposes.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>atkinson-EtAl:2017:EventStory</bibkey>
  </paper>
  <paper id='2710'>
    <title>Inducing Event Types and Roles in Reverse: Using Function to Discover Theme</title>
    <author>
      <first>Natalie</first>
      <last>Ahn</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–76</pages>
    <url>http://www.aclweb.org/anthology/W17-2710</url>
    <doi>10.18653/v1/W17-2710</doi>
    <abstract>
      With growing interest in automated event extraction, there is an
      increasing need to overcome the labor costs of hand-written event
      templates, entity lists, and annotated corpora. In the last few years,
      more inductive approaches have emerged, seeking to discover unknown event
      types and roles in raw text. The main recent efforts use probabilistic
      generative models, as in topic modeling, which are formally concise but do
      not always yield stable or easily interpretable results. We argue that
      event schema induction can benefit from greater structure in the process
      and in linguistic features that distinguish words' functions and themes.
      To maximize our use of limited data, we reverse the typical schema
      induction steps and introduce new similarity measures, building an
      intuitive process for inducing the structure of unknown events.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ahn:2017:EventStory</bibkey>
  </paper>
  <paper id='2711'>
    <title>
      The Event StoryLine Corpus: A New Benchmark for Causal and Temporal
      Relation Extraction
    </title>
    <author>
      <first>Tommaso</first>
      <last>Caselli</last>
    </author>
    <author>
      <first>Piek</first>
      <last>Vossen</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>77–86</pages>
    <url>http://www.aclweb.org/anthology/W17-2711</url>
    <doi>10.18653/v1/W17-2711</doi>
    <abstract>
      This paper reports on the Event StoryLine Corpus (ESC) v1.0, a new
      benchmark dataset for the temporal and causal relation detection. By
      developing this dataset, we also introduce a new task, the StoryLine
      Extraction from news data, which aims at extracting and classifying events
      relevant for stories, from across news documents spread in time and
      clustered around a single seminal event or topic. In addition to
      describing the dataset, we also report on three baselines systems whose
      results show the complexity of the task and suggest directions for the
      development of more robust systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>caselli-vossen:2017:EventStory</bibkey>
  </paper>
  <paper id='2712'>
    <title>The Rich Event Ontology</title>
    <author>
      <first>Susan</first>
      <last>Brown</last>
    </author>
    <author>
      <first>Claire</first>
      <last>Bonial</last>
    </author>
    <author>
      <first>Leo</first>
      <last>Obrst</last>
    </author>
    <author>
      <first>Martha</first>
      <last>Palmer</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87–97</pages>
    <url>http://www.aclweb.org/anthology/W17-2712</url>
    <doi>10.18653/v1/W17-2712</doi>
    <abstract>
      In this paper we describe a new lexical semantic resource, The Rich Event
      On-tology, which provides an independent conceptual backbone to unify
      existing semantic role labeling (SRL) schemas and augment them with
      event-to-event causal and temporal relations. By unifying the FrameNet,
      VerbNet, Automatic Content Extraction, and Rich Entities, Relations and
      Events resources, the ontology serves as a shared hub for the disparate
      annotation schemas and therefore enables the combination of SRL training
      data into a larger, more diverse corpus. By adding temporal and causal
      relational information not found in any of the independent resources, the
      ontology facilitates reasoning on and across documents, revealing
      relationships between events that come together in temporal and causal
      chains to build more complex scenarios. We envision the open resource
      serving as a valuable tool for both moving from the ontology to text to
      query for event types and scenarios of interest, and for moving from text
      to the ontology to access interpretations of events using the combined
      semantic information housed there.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>brown-EtAl:2017:EventStory</bibkey>
  </paper>
  <paper id='2713'>
    <title>Integrating Decompositional Event Structures into Storylines</title>
    <author>
      <first>William</first>
      <last>Croft</last>
    </author>
    <author>
      <first>Pavlina</first>
      <last>Peskova</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Regan</last>
    </author>
    <booktitle>Proceedings of the Events and Stories in the News Workshop</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>98–109</pages>
    <url>http://www.aclweb.org/anthology/W17-2713</url>
    <doi>10.18653/v1/W17-2713</doi>
    <abstract>
      Storyline research links together events in stories and specifies shared
      participants in those stories. In these analyses, an atomic event is
      assumed to be a single clause headed by a single verb. However, many
      analyses of verbal semantics assume a decompositional analysis of events
      expressed in single clauses. We present a formalization of a
      decompositional analysis of events in which each participant in a clausal
      event has their own temporally extended subevent, and the subevents are
      related through causal and other interactions. This decomposition allows
      us to represent storylines as an evolving set of interactions between
      participants over time.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>croft-peskova-regan:2017:EventStory</bibkey>
  </paper>
  <paper id='2800'>
    <title>Proceedings of the First Workshop on Language Grounding for Robotics</title>
    <editor>Mohit Bansal</editor>
    <editor>Cynthia Matuszek</editor>
    <editor>Jacob Andreas</editor>
    <editor>Yoav Artzi</editor>
    <editor>Yonatan Bisk</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-28</url>
    <doi>10.18653/v1/W17-28</doi>
    <bibtype>book</bibtype>
    <bibkey>RoboNLP:2017</bibkey>
  </paper>
  <paper id='2801'>
    <title>Grounding Language for Interactive Task Learning</title>
    <author>
      <first>Peter</first>
      <last>Lindes</last>
    </author>
    <author>
      <first>Aaron</first>
      <last>Mininger</last>
    </author>
    <author>
      <first>James R.</first>
      <last>Kirk</last>
    </author>
    <author>
      <first>John E.</first>
      <last>Laird</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>http://www.aclweb.org/anthology/W17-2801</url>
    <doi>10.18653/v1/W17-2801</doi>
    <abstract>
      This paper describes how language is grounded by a comprehension system
      called Lucia within a robotic agent called Rosie that can manipulate
      objects and navigate indoors. The whole system is built within the Soar
      cognitive architecture and uses Embodied Construction Grammar (ECG) as a
      formalism for describing linguistic knowledge. Grounding is performed
      using knowledge from the grammar itself, from the linguistic context, from
      the agents perception, and from an ontology of long-term knowledge about
      object categories and properties and actions the agent can perform. The
      paper also describes a benchmark corpus of 200 sentences in this domain
      along with test versions of the world model and ontology and gold-standard
      meanings for each of the sentences. The benchmark is contained in the
      supplemental materials.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lindes-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2802'>
    <title>
      Learning how to Learn: An Adaptive Dialogue Agent for Incrementally
      Learning Visually Grounded Word Meanings
    </title>
    <author>
      <first>Yanchao</first>
      <last>Yu</last>
    </author>
    <author>
      <first>Arash</first>
      <last>Eshghi</last>
    </author>
    <author>
      <first>Oliver</first>
      <last>Lemon</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–19</pages>
    <url>http://www.aclweb.org/anthology/W17-2802</url>
    <doi>10.18653/v1/W17-2802</doi>
    <abstract>
      We present an optimised multi-modal dialogue agent for interactive
      learning of visually grounded word meanings from a human tutor, trained on
      real human-human tutoring data. Within a life-long interactive learning
      period, the agent, trained using Reinforcement Learning (RL), must be able
      to handle natural conversations with human users, and achieve good
      learning performance (i.e. accuracy) while minimising human effort in the
      learning process. We train and evaluate this system in interaction with a
      simulated human tutor, which is built on the BURCHAK corpus – a
      Human-Human Dialogue dataset for the visual learning task. The results
      show that: 1) The learned policy can coherently interact with the
      simulated user to achieve the goal of the task (i.e. learning visual
      attributes of objects, e.g. colour and shape); and 2) it finds a better
      trade-off between classifier accuracy and tutoring costs than hand-crafted
      rule-based policies, including ones with dynamic policies.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yu-eshghi-lemon:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2803'>
    <title>Guiding Interaction Behaviors for Multi-modal Grounded Language Learning</title>
    <author>
      <first>Jesse</first>
      <last>Thomason</last>
    </author>
    <author>
      <first>Jivko</first>
      <last>Sinapov</last>
    </author>
    <author>
      <first>Raymond</first>
      <last>Mooney</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20–24</pages>
    <url>http://www.aclweb.org/anthology/W17-2803</url>
    <doi>10.18653/v1/W17-2803</doi>
    <abstract>
      Multi-modal grounded language learning connects language predicates to
      physical properties of objects in the world. Sensing with multiple
      modalities, such as audio, haptics, and visual colors and shapes while
      performing interaction behaviors like lifting, dropping, and looking on
      objects enables a robot to ground non-visual predicates like “empty” as
      well as visual predicates like “red”. Previous work has established that
      grounding in multi-modal space improves performance on object retrieval
      from human descriptions. In this work, we gather behavior annotations from
      humans and demonstrate that these improve language grounding performance
      by allowing a system to focus on relevant behaviors for words like “white”
      or “half-full” that can be understood by looking or lifting, respectively.
      We also explore adding modality annotations (whether to focus on audio or
      haptics when performing a behavior), which improves performance, and
      sharing information between linguistically related predicates (if “green”
      is a color, “white” is a color), which improves grounding recall but at
      the cost of precision.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>thomason-sinapov-mooney:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2804'>
    <title>
      Structured Learning for Context-aware Spoken Language Understanding of
      Robotic Commands
    </title>
    <author>
      <first>Andrea</first>
      <last>Vanzo</last>
    </author>
    <author>
      <first>Danilo</first>
      <last>Croce</last>
    </author>
    <author>
      <first>Roberto</first>
      <last>Basili</last>
    </author>
    <author>
      <first>Daniele</first>
      <last>Nardi</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–34</pages>
    <url>http://www.aclweb.org/anthology/W17-2804</url>
    <doi>10.18653/v1/W17-2804</doi>
    <abstract>
      Service robots are expected to operate in specific environments, where the
      presence of humans plays a key role. A major feature of such robotics
      platforms is thus the ability to react to spoken commands. This requires
      the understanding of the user utterance with an accuracy able to trigger
      the robot reaction. Such correct interpretation of linguistic exchanges
      depends on physical, cognitive and language-dependent aspects related to
      the environment. In this work, we present the empirical evaluation of an
      adaptive Spoken Language Understanding chain for robotic commands, that
      explicitly depends on the operational environment during both the learning
      and recognition stages. The effectiveness of such a context-sensitive
      command interpretation is tested against an extension of an already
      existing corpus of commands, that introduced explicit perceptual
      knowledge: this enabled deeper measures proving that more accurate
      disambiguation capabilities can be actually obtained.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vanzo-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2805'>
    <title>
      Natural Language Grounding and Grammar Induction for Robotic Manipulation
      Commands
    </title>
    <author>
      <first>Muhannad</first>
      <last>Alomari</last>
    </author>
    <author>
      <first>Paul</first>
      <last>Duckworth</last>
    </author>
    <author>
      <first>Majd</first>
      <last>Hawasly</last>
    </author>
    <author>
      <first>David C.</first>
      <last>Hogg</last>
    </author>
    <author>
      <first>Anthony G.</first>
      <last>Cohn</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35–43</pages>
    <url>http://www.aclweb.org/anthology/W17-2805</url>
    <doi>10.18653/v1/W17-2805</doi>
    <abstract>
      We present a cognitively plausible system capable of acquiring knowledge
      in language and vision from pairs of short video clips and linguistic
      descriptions. The aim of this work is to teach a robot manipulator how to
      execute natural language commands by demonstration. This is achieved by
      first learning a set of visual `concepts' that abstract the visual feature
      spaces into concepts that have human-level meaning. Second, learning the
      mapping/grounding between words and the extracted visual concepts. Third,
      inducing grammar rules via a semantic representation known as Robot
      Control Language (RCL). We evaluate our approach against state-of-the-art
      supervised and unsupervised grounding and grammar induction systems, and
      show that a robot can learn to execute never seen-before commands from
      pairs of unlabelled linguistic and visual inputs.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alomari-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2806'>
    <title>Communication with Robots using Multilayer Recurrent Networks</title>
    <author>
      <first>Bedřich</first>
      <last>Pišl</last>
    </author>
    <author>
      <first>David</first>
      <last>Mareček</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>44–48</pages>
    <url>http://www.aclweb.org/anthology/W17-2806</url>
    <doi>10.18653/v1/W17-2806</doi>
    <abstract>
      In this paper, we describe an improvement on the task of giving
      instructions to robots in a simulated block world using unrestricted
      natural language commands.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pivsl-marevcek:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2807'>
    <title>Grounding Symbols in Multi-Modal Instructions</title>
    <author>
      <first>Yordan</first>
      <last>Hristov</last>
    </author>
    <author>
      <first>Svetlin</first>
      <last>Penkov</last>
    </author>
    <author>
      <first>Alex</first>
      <last>Lascarides</last>
    </author>
    <author>
      <first>Subramanian</first>
      <last>Ramamoorthy</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>49–57</pages>
    <url>http://www.aclweb.org/anthology/W17-2807</url>
    <doi>10.18653/v1/W17-2807</doi>
    <abstract>
      As robots begin to cohabit with humans in semi-structured environments,
      the need arises to understand instructions involving rich variability–-for
      instance, learning to ground symbols in the physical world. Realistically,
      this task must cope with small datasets consisting of a particular users'
      contextual assignment of meaning to terms. We present a method for
      processing a raw stream of cross-modal input–-i.e., linguistic
      instructions, visual perception of a scene and a concurrent trace of 3D
      eye tracking fixations–-to produce the segmentation of objects with a
      correspondent association to high-level concepts. To test our framework we
      present experiments in a table-top object manipulation scenario. Our
      results show our model learns the user's notion of colour and shape from a
      small number of physical demonstrations, generalising to identifying
      physical referents for novel combinations of the words.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hristov-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2808'>
    <title>
      Exploring Variation of Natural Human Commands to a Robot in a
      Collaborative Navigation Task
    </title>
    <author>
      <first>Matthew</first>
      <last>Marge</last>
    </author>
    <author>
      <first>Claire</first>
      <last>Bonial</last>
    </author>
    <author>
      <first>Ashley</first>
      <last>Foots</last>
    </author>
    <author>
      <first>Cory</first>
      <last>Hayes</last>
    </author>
    <author>
      <first>Cassidy</first>
      <last>Henry</last>
    </author>
    <author>
      <first>Kimberly</first>
      <last>Pollard</last>
    </author>
    <author>
      <first>Ron</first>
      <last>Artstein</last>
    </author>
    <author>
      <first>Clare</first>
      <last>Voss</last>
    </author>
    <author>
      <first>David</first>
      <last>Traum</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>58–66</pages>
    <url>http://www.aclweb.org/anthology/W17-2808</url>
    <doi>10.18653/v1/W17-2808</doi>
    <abstract>
      Robot-directed communication is variable, and may change based on human
      perception of robot capabilities. To collect training data for a dialogue
      system and to investigate possible communication changes over time, we
      developed a Wizard-of-Oz study that (a) simulates a robot's limited
      understanding, and (b) collects dialogues where human participants build a
      progressively better mental model of the robot's understanding. With ten
      participants, we collected ten hours of human-robot dialogue. We analyzed
      the structure of instructions that participants gave to a remote robot
      before it responded. Our findings show a general initial preference for
      including metric information (e.g., move forward 3 feet) over landmarks
      (e.g., move to the desk) in motion commands, but this decreased over time,
      suggesting changes in perception.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marge-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2809'>
    <title>
      A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting Action-Oriented
      and Goal-Oriented Instructions
    </title>
    <author>
      <first>Siddharth</first>
      <last>Karamcheti</last>
    </author>
    <author>
      <first>Edward Clem</first>
      <last>Williams</last>
    </author>
    <author>
      <first>Dilip</first>
      <last>Arumugam</last>
    </author>
    <author>
      <first>Mina</first>
      <last>Rhee</last>
    </author>
    <author>
      <first>Nakul</first>
      <last>Gopalan</last>
    </author>
    <author>
      <first>Lawson L.S.</first>
      <last>Wong</last>
    </author>
    <author>
      <first>Stefanie</first>
      <last>Tellex</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67–75</pages>
    <url>http://www.aclweb.org/anthology/W17-2809</url>
    <doi>10.18653/v1/W17-2809</doi>
    <abstract>
      Robots operating alongside humans in diverse, stochastic environments must
      be able to accurately interpret natural language commands. These
      instructions often fall into one of two categories: those that specify a
      goal condition or target state, and those that specify explicit actions,
      or how to perform a given task. Recent approaches have used reward
      functions as a semantic representation of goal-based commands, which
      allows for the use of a state-of-the-art planner to find a policy for the
      given task. However, these reward functions cannot be directly used to
      represent action-oriented commands. We introduce a new hybrid approach,
      the Deep Recurrent Action-Goal Grounding Network (DRAGGN), for task
      grounding and execution that handles natural language from either category
      as input, and generalizes to unseen environments. Our robot-simulation
      results demonstrate that a system successfully interpreting both
      goal-oriented and action-oriented task specifications brings us closer to
      robust natural language understanding for human-robot interaction.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>karamcheti-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2810'>
    <title>
      Are Distributional Representations Ready for the Real World? Evaluating
      Word Vectors for Grounded Perceptual Meaning
    </title>
    <author>
      <first>Li</first>
      <last>Lucy</last>
    </author>
    <author>
      <first>Jon</first>
      <last>Gauthier</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76–85</pages>
    <url>http://www.aclweb.org/anthology/W17-2810</url>
    <doi>10.18653/v1/W17-2810</doi>
    <abstract>
      Distributional word representation methods exploit word co-occurrences to
      build compact vector encodings of words. While these representations enjoy
      widespread use in modern natural language processing, it is unclear
      whether they accurately encode all necessary facets of conceptual meaning.
      In this paper, we evaluate how well these representations can predict
      perceptual and conceptual features of concrete concepts, drawing on two
      semantic norm datasets sourced from human participants. We find that
      several standard word representations fail to encode many salient
      perceptual features of concepts, and show that these deficits correlate
      with word-word similarity prediction errors. Our analyses provide
      motivation for grounded and embodied language learning approaches, which
      may help to remedy these deficits.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lucy-gauthier:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2811'>
    <title>
      Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of
      Multimodal Features in Spoken Human-Robot Interaction
    </title>
    <author>
      <first>Jekaterina</first>
      <last>Novikova</last>
    </author>
    <author>
      <first>Christian</first>
      <last>Dondrup</last>
    </author>
    <author>
      <first>Ioannis</first>
      <last>Papaioannou</last>
    </author>
    <author>
      <first>Oliver</first>
      <last>Lemon</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>86–94</pages>
    <url>http://www.aclweb.org/anthology/W17-2811</url>
    <doi>10.18653/v1/W17-2811</doi>
    <abstract>
      Recognition of social signals, coming from human facial expressions or
      prosody of human speech, is a popular research topic in human-robot
      interaction studies. There is also a long line of research in the spoken
      dialogue community that investigates user satisfaction in relation to
      dialogue characteristics. However, very little research relates a
      combination of multimodal social signals and language features detected
      during spoken face-to-face human-robot interaction to the resulting user
      perception of a robot. In this paper we show how different emotional
      facial expressions of human users, in combination with prosodic
      characteristics of human speech and features of human-robot dialogue,
      correlate with users’ impressions of the robot after a conversation. We
      find that happiness in the user’s recognised facial expression strongly
      correlates with likeability of a robot, while dialogue-related features
      (such as number of human turns or number of sentences per robot utterance)
      correlate with perceiving a robot as intelligent. In addition, we show
      that the facial expression emotional features and prosody are better
      predictors of human ratings related to perceived robot likeability and
      anthropomorphism, while linguistic and non-linguistic features more often
      predict perceived robot intelligence and interpretability. As such, these
      characteristics may in future be used as an online reward signal for
      in-situ Reinforcement Learning-based adaptive human-robot dialogue
      systems.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>novikova-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2812'>
    <title>Towards Problem Solving Agents that Communicate and Learn</title>
    <author>
      <first>Anjali</first>
      <last>Narayan-Chen</last>
    </author>
    <author>
      <first>Colin</first>
      <last>Graber</last>
    </author>
    <author>
      <first>Mayukh</first>
      <last>Das</last>
    </author>
    <author>
      <first>Md Rakibul</first>
      <last>Islam</last>
    </author>
    <author>
      <first>Soham</first>
      <last>Dan</last>
    </author>
    <author>
      <first>Sriraam</first>
      <last>Natarajan</last>
    </author>
    <author>
      <first>Janardhan Rao</first>
      <last>Doppa</last>
    </author>
    <author>
      <first>Julia</first>
      <last>Hockenmaier</last>
    </author>
    <author>
      <first>Martha</first>
      <last>Palmer</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Roth</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Language Grounding for Robotics</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>95–103</pages>
    <url>http://www.aclweb.org/anthology/W17-2812</url>
    <doi>10.18653/v1/W17-2812</doi>
    <abstract>
      Agents that communicate back and forth with humans to help them execute
      non-linguistic tasks are a long sought goal of AI. These agents need to
      translate between utterances and actionable meaning representations that
      can be interpreted by task-specific problem solvers in a context-dependent
      manner. They should also be able to learn such actionable interpretations
      for new predicates on the fly. We define an agent architecture for this
      scenario and present a series of experiments in the Blocks World domain
      that illustrate how our architecture supports language learning and
      problem solving in this domain.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>narayanchen-EtAl:2017:RoboNLP</bibkey>
  </paper>
  <paper id='2900'>
    <title>Proceedings of the Second Workshop on NLP and Computational Social Science</title>
    <editor>Dirk Hovy</editor>
    <editor>Svitlana Volkova</editor>
    <editor>David Bamman</editor>
    <editor>David Jurgens</editor>
    <editor>Brendan O'Connor</editor>
    <editor>Oren Tsur</editor>
    <editor>A. Seza Doğruöz</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-29</url>
    <doi>10.18653/v1/W17-29</doi>
    <bibtype>book</bibtype>
    <bibkey>NLPandCSS:2017</bibkey>
  </paper>
  <paper id='2901'>
    <title>Language-independent Gender Prediction on Twitter</title>
    <author>
      <first>Nikola</first>
      <last>Ljubešić</last>
    </author>
    <author>
      <first>Darja</first>
      <last>Fišer</last>
    </author>
    <author>
      <first>Tomaž</first>
      <last>Erjavec</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–6</pages>
    <url>http://www.aclweb.org/anthology/W17-2901</url>
    <doi>10.18653/v1/W17-2901</doi>
    <abstract>
      In this paper we present a set of experiments and analyses on predicting
      the gender of Twitter users based on language-independent features
      extracted either from the text or the metadata of users' tweets. We
      perform our experiments on the TwiSty dataset containing manual gender
      annotations for users speaking six different languages. Our classification
      results show that, while the prediction model based on
      language-independent features performs worse than the bag-of-words model
      when training and testing on the same language, it regularly outperforms
      the bag-of-words model when applied to different languages, showing very
      stable results across various languages. Finally we perform a comparative
      analysis of feature effect sizes across the six languages and show that
      differences in our features correspond to cultural distances.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ljubevsic-fivser-erjavec:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2902'>
    <title>
      When does a compliment become sexist? Analysis and classification of
      ambivalent sexism using twitter data
    </title>
    <author>
      <first>Akshita</first>
      <last>Jha</last>
    </author>
    <author>
      <first>Radhika</first>
      <last>Mamidi</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7–16</pages>
    <url>http://www.aclweb.org/anthology/W17-2902</url>
    <doi>10.18653/v1/W17-2902</doi>
    <abstract>
      Sexism is prevalent in today’s society, both offline and online, and poses
      a credible threat to social equality with respect to gender. According to
      ambivalent sexism theory (Glick and Fiske, 1996), it comes in two forms:
      Hostile and Benevolent. While hostile sexism is characterized by an
      explicitly negative attitude, benevolent sexism is more subtle. Previous
      works on computationally detecting sexism present online are restricted to
      identifying the hostile form. Our objective is to investigate the less
      pronounced form of sexism demonstrated online. We achieve this by creating
      and analyzing a dataset of tweets that exhibit benevolent sexism. By using
      Support Vector Machines (SVM), sequence-to-sequence models and FastText
      classifier, we classify tweets into ‘Hostile’, ‘Benevolent’ or ‘Others’
      class depending on the kind of sexism they exhibit. We have been able to
      achieve an F1-score of 87.22% using FastText classifier. Our work helps
      analyze and understand the much prevalent ambivalent sexism in social
      media.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jha-mamidi:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2903'>
    <title>Personality Driven Differences in Paraphrase Preference</title>
    <author>
      <first>Daniel</first>
      <last>Preoţiuc-Pietro</last>
    </author>
    <author>
      <first>Jordan</first>
      <last>Carpenter</last>
    </author>
    <author>
      <first>Lyle</first>
      <last>Ungar</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17–26</pages>
    <url>http://www.aclweb.org/anthology/W17-2903</url>
    <doi>10.18653/v1/W17-2903</doi>
    <abstract>
      Personality plays a decisive role in how people behave in different
      scenarios, including online social media. Researchers have used such data
      to study how personality can be predicted from language use. In this
      paper, we study phrase choice as a particular stylistic linguistic
      difference, as opposed to the mostly topical differences identified
      previously. Building on previous work on demographic preferences, we
      quantify differences in paraphrase choice from a massive Facebook data set
      with posts from over 115,000 users. We quantify the predictive power of
      phrase choice in user profiling and use phrase choice to study
      psycholinguistic hypotheses. This work is relevant to future applications
      that aim to personalize text generation to specific personality types.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>preoiucpietro-carpenter-ungar:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2904'>
    <title>
      community2vec: Vector representations of online communities encode
      semantic relationships
    </title>
    <author>
      <first>Trevor</first>
      <last>Martin</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–31</pages>
    <url>http://www.aclweb.org/anthology/W17-2904</url>
    <doi>10.18653/v1/W17-2904</doi>
    <abstract>
      Vector embeddings of words have been shown to encode meaningful semantic
      relationships that enable solving of complex analogies. This vector
      embedding concept has been extended successfully to many different domains
      and in this paper we both create and visualize vector representations of
      an unstructured collection of online communities based on user
      participation. Further, we quantitatively and qualitatively show that
      these representations allow solving of semantically meaningful community
      analogies and also other more general types of relationships. These
      results could help improve community recommendation engines and also serve
      as a tool for sociological studies of community relatedness.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>martin:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2905'>
    <title>
      Telling Apart Tweets Associated with Controversial versus
      Non-Controversial Topics
    </title>
    <author>
      <first>Aseel</first>
      <last>Addawood</last>
    </author>
    <author>
      <first>Rezvaneh</first>
      <last>Rezapour</last>
    </author>
    <author>
      <first>Omid</first>
      <last>Abdar</last>
    </author>
    <author>
      <first>Jana</first>
      <last>Diesner</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–41</pages>
    <url>http://www.aclweb.org/anthology/W17-2905</url>
    <doi>10.18653/v1/W17-2905</doi>
    <abstract>
      In this paper, we evaluate the predictability of tweets associated with
      controversial versus non-controversial topics. As a first step, we
      crowd-sourced the scoring of a predefined set of topics on a Likert scale
      from non-controversial to controversial. Our feature set entails and goes
      beyond sentiment features, e.g., by leveraging empathic language and other
      features that have been previously used but are new for this particular
      study. We find focusing on the structural characteristics of tweets to be
      beneficial for this task. Using a combination of emphatic,
      language-specific, and Twitter-specific features for supervised learning
      resulted in 87% accuracy (F1) for cross-validation of the training set and
      63.4% accuracy when using the test set. Our analysis shows that features
      specific to Twitter or social media, in general, are more prevalent in
      tweets on controversial topics than in non-controversial ones. To test the
      premise of the paper, we conducted two additional sets of experiments,
      which led to mixed results. This finding will inform our future
      investigations into the relationship between language use on social media
      and the perceived controversiality of topics.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>addawood-EtAl:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2906'>
    <title>Cross-Lingual Classification of Topics in Political Texts</title>
    <author>
      <first>Goran</first>
      <last>Glavaš</last>
    </author>
    <author>
      <first>Federico</first>
      <last>Nanni</last>
    </author>
    <author>
      <first>Simone Paolo</first>
      <last>Ponzetto</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>42–46</pages>
    <url>http://www.aclweb.org/anthology/W17-2906</url>
    <doi>10.18653/v1/W17-2906</doi>
    <abstract>
      In this paper, we propose an approach for cross-lingual topical coding of
      sentences from electoral manifestos of political parties in different
      languages. To this end, we exploit continuous semantic text
      representations and induce a joint multilingual semantic vector spaces to
      enable supervised learning using manually-coded sentences across different
      languages. Our experimental results show that classifiers trained on
      multilingual data yield performance boosts over monolingual topic
      classification.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>glavavs-nanni-ponzetto:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2907'>
    <title>Mining Social Science Publications for Survey Variables</title>
    <author>
      <first>Andrea</first>
      <last>Zielinski</last>
    </author>
    <author>
      <first>Peter</first>
      <last>Mutschke</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–52</pages>
    <url>http://www.aclweb.org/anthology/W17-2907</url>
    <doi>10.18653/v1/W17-2907</doi>
    <abstract>
      Research in Social Science is usually based on survey data where
      individual research questions relate to observable concepts (variables).
      However, due to a lack of standards for data citations a reliable
      identification of the variables used is often difficult. In this paper, we
      present a work-in-progress study that seeks to provide a solution to the
      variable detection task based on supervised machine learning algorithms,
      using a linguistic analysis pipeline to extract a rich feature set,
      including terminological concepts and similarity metric scores. Further,
      we present preliminary results on a small dataset that has been
      specifically designed for this task, yielding a significant increase in
      performance over the random baseline.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zielinski-mutschke:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2908'>
    <title>Linguistic Markers of Influence in Informal Interactions</title>
    <author>
      <first>Shrimai</first>
      <last>Prabhumoye</last>
    </author>
    <author>
      <first>Samridhi</first>
      <last>Choudhary</last>
    </author>
    <author>
      <first>Evangelia</first>
      <last>Spiliopoulou</last>
    </author>
    <author>
      <first>Christopher</first>
      <last>Bogart</last>
    </author>
    <author>
      <first>Carolyn</first>
      <last>Rose</last>
    </author>
    <author>
      <first>Alan W</first>
      <last>Black</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53–62</pages>
    <url>http://www.aclweb.org/anthology/W17-2908</url>
    <doi>10.18653/v1/W17-2908</doi>
    <abstract>
      There has been a long standing interest in understanding `Social
      Influence' both in Social Sciences and in Computational Linguistics. In
      this paper, we present a novel approach to study and measure interpersonal
      influence in daily interactions. Motivated by the basic principles of
      influence, we attempt to identify indicative linguistic features of the
      posts in an online knitting community. We present the scheme used to
      operationalize and label the posts as influential or non-influential.
      Experiments with the identified features show an improvement in the
      classification accuracy of influence by 3.15%. Our results illustrate the
      important correlation between the structure of the language and its
      potential to influence others.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>prabhumoye-EtAl:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2909'>
    <title>Non-lexical Features Encode Political Affiliation on Twitter</title>
    <author>
      <first>Rachael</first>
      <last>Tatman</last>
    </author>
    <author>
      <first>Leo</first>
      <last>Stewart</last>
    </author>
    <author>
      <first>Amandalynne</first>
      <last>Paullada</last>
    </author>
    <author>
      <first>Emma</first>
      <last>Spiro</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>63–67</pages>
    <url>http://www.aclweb.org/anthology/W17-2909</url>
    <doi>10.18653/v1/W17-2909</doi>
    <abstract>
      Previous work on classifying Twitter users' political alignment has mainly
      focused on lexical and social network features. This study provides
      evidence that political affiliation is also reflected in features which
      have been previously overlooked: users' discourse patterns (proportion of
      Tweets that are retweets or replies) and their rate of use of
      capitalization and punctuation. We find robust differences between
      politically left- and right-leaning communities with respect to these
      discourse and sub-lexical features, although they are not enough to train
      a high-accuracy classifier.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tatman-EtAl:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2910'>
    <title>
      Modelling Participation in Small Group Social Sequences with Markov
      Rewards Analysis
    </title>
    <author>
      <first>Gabriel</first>
      <last>Murray</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>68–72</pages>
    <url>http://www.aclweb.org/anthology/W17-2910</url>
    <doi>10.18653/v1/W17-2910</doi>
    <abstract>
      We explore a novel computational approach for analyzing member
      participation in small group social sequences. Using a complex state
      representation combining information about dialogue act types, sentiment
      expression, and participant roles, we explore which sequence states are
      associated with high levels of member participation. Using a Markov
      Rewards framework, we associate particular states with immediate positive
      and negative rewards, and employ a Value Iteration algorithm to calculate
      the expected value of all states. In our findings, we focus on discourse
      states belonging to team leaders and project managers which are either
      very likely or very unlikely to lead to participation from the rest of the
      group members.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>murray:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2911'>
    <title>Code-Switching as a Social Act: The Case of Arabic Wikipedia Talk Pages</title>
    <author>
      <first>Michael</first>
      <last>Yoder</last>
    </author>
    <author>
      <first>Shruti</first>
      <last>Rijhwani</last>
    </author>
    <author>
      <first>Carolyn</first>
      <last>Rosé</last>
    </author>
    <author>
      <first>Lori</first>
      <last>Levin</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73–82</pages>
    <url>http://www.aclweb.org/anthology/W17-2911</url>
    <doi>10.18653/v1/W17-2911</doi>
    <abstract>
      Code-switching has been found to have social motivations in addition to
      syntactic constraints. In this work, we explore the social effect of
      code-switching in an online community. We present a task from the Arabic
      Wikipedia to capture language choice, in this case code-switching between
      Arabic and other languages, as a predictor of social influence in
      collaborative editing. We find that code-switching is positively
      associated with Wikipedia editor success, particularly borrowing technical
      language on pages with topics less directly related to Arabic-speaking
      regions.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yoder-EtAl:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2912'>
    <title>How Does Twitter User Behavior Vary Across Demographic Groups?</title>
    <author>
      <first>Zach</first>
      <last>Wood-Doughty</last>
    </author>
    <author>
      <first>Michael</first>
      <last>Smith</last>
    </author>
    <author>
      <first>David</first>
      <last>Broniatowski</last>
    </author>
    <author>
      <first>Mark</first>
      <last>Dredze</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>83–89</pages>
    <url>http://www.aclweb.org/anthology/W17-2912</url>
    <doi>10.18653/v1/W17-2912</doi>
    <abstract>
      Demographically-tagged social media messages are a common source of data
      for computational social science. While these messages can indicate
      differences in beliefs and behaviors between demographic groups, we do not
      have a clear understanding of how different demographic groups use
      platforms such as Twitter. This paper presents a preliminary analysis of
      how groups' differing behaviors may confound analyses of the groups
      themselves. We analyzed one million Twitter users by first inferring
      demographic attributes, and then measuring several indicators of Twitter
      behavior. We find differences in these indicators across demographic
      groups, suggesting that there may be underlying differences in how
      different demographic groups use Twitter.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wooddoughty-EtAl:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='2913'>
    <title>
      Ideological Phrase Indicators for Classification of Political Discourse
      Framing on Twitter
    </title>
    <author>
      <first>Kristen</first>
      <last>Johnson</last>
    </author>
    <author>
      <first>I-Ta</first>
      <last>Lee</last>
    </author>
    <author>
      <first>Dan</first>
      <last>Goldwasser</last>
    </author>
    <booktitle>Proceedings of the Second Workshop on NLP and Computational Social Science</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>90–99</pages>
    <url>http://www.aclweb.org/anthology/W17-2913</url>
    <doi>10.18653/v1/W17-2913</doi>
    <abstract>
      Politicians carefully word their statements in order to influence how
      others view an issue, a political strategy called framing. Simultaneously,
      these frames may also reveal the beliefs or positions on an issue of the
      politician. Simple language features such as unigrams, bigrams, and
      trigrams are important indicators for identifying the general frame of a
      text, for both longer congressional speeches and shorter tweets of
      politicians. However, tweets may contain multiple unigrams across
      different frames which limits the effectiveness of this approach. In this
      paper, we present a joint model which uses both linguistic features of
      tweets and ideological phrase indicators extracted from a state-of-the-art
      embedding-based model to predict the general frame of political tweets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>johnson-lee-goldwasser:2017:NLPandCSS</bibkey>
  </paper>
  <paper id='3000'>
    <title>Proceedings of the First Workshop on Abusive Language Online</title>
    <editor>Zeerak Waseem</editor>
    <editor>Wendy Hui Kyong Chung</editor>
    <editor>Dirk Hovy</editor>
    <editor>Joel Tetreault</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-30</url>
    <doi>10.18653/v1/W17-30</doi>
    <bibtype>book</bibtype>
    <bibkey>ALW1:2017</bibkey>
  </paper>
  <paper id='3001'>
    <title>Dimensions of Abusive Language on Twitter</title>
    <author>
      <first>Isobelle</first>
      <last>Clarke</last>
    </author>
    <author>
      <first>Dr. Jack</first>
      <last>Grieve</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-3001</url>
    <doi>10.18653/v1/W17-3001</doi>
    <abstract>
      In this paper, we use a new categorical form of multidimensional register
      analysis to identify the main dimensions of functional linguistic
      variation in a corpus of abusive language, consisting of racist and sexist
      Tweets. By analysing the use of a wide variety of parts-of-speech and
      grammatical constructions, as well as various features related to Twitter
      and computer-mediated communication, we discover three dimensions of
      linguistic variation in this corpus, which we interpret as being related
      to the degree of interactive, antagonistic and attitudinal language
      exhibited by individual Tweets. We then demonstrate that there is a
      significant functional difference between racist and sexist Tweets, with
      sexists Tweets tending to be more interactive and attitudinal than racist
      Tweets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>clarke-grieve:2017:ALW1</bibkey>
  </paper>
  <paper id='3002'>
    <title>Constructive Language in News Comments</title>
    <author>
      <first>Varada</first>
      <last>Kolhatkar</last>
    </author>
    <author>
      <first>Maite</first>
      <last>Taboada</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–17</pages>
    <url>http://www.aclweb.org/anthology/W17-3002</url>
    <doi>10.18653/v1/W17-3002</doi>
    <abstract>
      We discuss the characteristics of constructive news comments, and present
      methods to identify them. First, we define the notion of constructiveness.
      Second, we annotate a corpus for constructiveness. Third, we explore
      whether available argumentation corpora can be useful to identify
      constructiveness in news comments. Our model trained on argumentation
      corpora achieves a top accuracy of 72.59% (baseline=49.44%) on our
      crowd-annotated test data. Finally, we examine the relation between
      constructiveness and toxicity. In our crowd-annotated data, 21.42% of the
      non-constructive comments and 17.89% of the constructive comments are
      toxic, suggesting that non-constructive comments are not much more toxic
      than constructive comments.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kolhatkar-taboada:2017:ALW1</bibkey>
  </paper>
  <paper id='3003'>
    <title>Rephrasing Profanity in Chinese Text</title>
    <author>
      <first>Hui-Po</first>
      <last>Su</last>
    </author>
    <author>
      <first>Zhen-Jie</first>
      <last>Huang</last>
    </author>
    <author>
      <first>Hao-Tsung</first>
      <last>Chang</last>
    </author>
    <author>
      <first>Chuan-Jie</first>
      <last>Lin</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–24</pages>
    <url>http://www.aclweb.org/anthology/W17-3003</url>
    <doi>10.18653/v1/W17-3003</doi>
    <abstract>
      This paper proposes a system that can detect and rephrase profanity in
      Chinese text. Rather than just masking detected profanity, we want to
      revise the input sentence by using inoffensive words while keeping their
      original meanings. 29 of such rephrasing rules were invented after
      observing sentences on real-word social websites. The overall accuracy of
      the proposed system is 85.56%
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>su-EtAl:2017:ALW1</bibkey>
  </paper>
  <paper id='3004'>
    <title>Deep Learning for User Comment Moderation</title>
    <author>
      <first>John</first>
      <last>Pavlopoulos</last>
    </author>
    <author>
      <first>Prodromos</first>
      <last>Malakasiotis</last>
    </author>
    <author>
      <first>Ion</first>
      <last>Androutsopoulos</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25–35</pages>
    <url>http://www.aclweb.org/anthology/W17-3004</url>
    <doi>10.18653/v1/W17-3004</doi>
    <abstract>
      Experimenting with a new dataset of 1.6M user comments from a Greek news
      portal and existing datasets of EnglishWikipedia comments, we show that an
      RNN outperforms the previous state of the art in moderation. A deep,
      classification-specific attention mechanism improves further the overall
      performance of the RNN. We also compare against a CNN and a word-list
      baseline, considering both fully automatic and semi-automatic moderation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>pavlopoulos-malakasiotis-androutsopoulos:2017:ALW1</bibkey>
  </paper>
  <paper id='3005'>
    <title>
      Class-based Prediction Errors to Detect Hate Speech with Out-of-vocabulary
      Words
    </title>
    <author>
      <first>Joan</first>
      <last>Serrà</last>
    </author>
    <author>
      <first>Ilias</first>
      <last>Leontiadis</last>
    </author>
    <author>
      <first>Dimitris</first>
      <last>Spathis</last>
    </author>
    <author>
      <first>Gianluca</first>
      <last>Stringhini</last>
    </author>
    <author>
      <first>Jeremy</first>
      <last>Blackburn</last>
    </author>
    <author>
      <first>Athena</first>
      <last>Vakali</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–40</pages>
    <url>http://www.aclweb.org/anthology/W17-3005</url>
    <doi>10.18653/v1/W17-3005</doi>
    <abstract>
      Common approaches to text categorization essentially rely either on n-gram
      counts or on word embeddings. This presents important difficulties in
      highly dynamic or quickly-interacting environments, where the appearance
      of new words and/or varied misspellings is the norm. A paradigmatic
      example of this situation is abusive online behavior, with social networks
      and media platforms struggling to effectively combat uncommon or
      non-blacklisted hate words. To better deal with these issues in those
      fast-paced environments, we propose using the error signal of class-based
      language models as input to text classification algorithms. In particular,
      we train a next-character prediction model for any given class and then
      exploit the error of such class-based models to inform a neural network
      classifier. This way, we shift from the ‘ability to describe’ seen
      documents to the ‘ability to predict’ unseen content. Preliminary studies
      using out-of-vocabulary splits from abusive tweet data show promising
      results, outperforming competitive text categorization strategies by
      4-11%.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>serra-EtAl:2017:ALW1</bibkey>
  </paper>
  <paper id='3006'>
    <title>One-step and Two-step Classification for Abusive Language Detection on Twitter</title>
    <author>
      <first>Ji Ho</first>
      <last>Park</last>
    </author>
    <author>
      <first>Pascale</first>
      <last>Fung</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–45</pages>
    <url>http://www.aclweb.org/anthology/W17-3006</url>
    <doi>10.18653/v1/W17-3006</doi>
    <abstract>
      Automatic abusive language detection is a difficult but important task for
      online social media. Our research explores a two-step approach of
      performing classification on abusive language and then classifying into
      specific types and compares it with one-step approach of doing one
      multi-class classification for detecting sexist and racist languages. With
      a public English Twitter corpus of 20 thousand tweets in the type of
      sexism and racism, our approach shows a promising performance of 0.827
      F-measure by using HybridCNN in one-step and 0.824 F-measure by using
      logistic regression in two-steps. Author{2}Affiliation
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>park-fung:2017:ALW1</bibkey>
  </paper>
  <paper id='3007'>
    <title>
      Legal Framework, Dataset and Annotation Schema for Socially Unacceptable
      Online Discourse Practices in Slovene
    </title>
    <author>
      <first>Darja</first>
      <last>Fišer</last>
    </author>
    <author>
      <first>Tomaž</first>
      <last>Erjavec</last>
    </author>
    <author>
      <first>Nikola</first>
      <last>Ljubešić</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46–51</pages>
    <url>http://www.aclweb.org/anthology/W17-3007</url>
    <doi>10.18653/v1/W17-3007</doi>
    <abstract>
      In this paper we present the legal framework, dataset and annotation
      schema of socially unacceptable discourse practices on social networking
      platforms in Slovenia. On this basis we aim to train an automatic
      identification and classification system with which we wish contribute
      towards an improved methodology, understanding and treatment of such
      practices in the contemporary, increasingly multicultural information
      society.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fivser-erjavec-ljubevsic:2017:ALW1</bibkey>
  </paper>
  <paper id='3008'>
    <title>Abusive Language Detection on Arabic Social Media</title>
    <author>
      <first>Hamdy</first>
      <last>Mubarak</last>
    </author>
    <author>
      <first>Kareem</first>
      <last>Darwish</last>
    </author>
    <author>
      <first>Walid</first>
      <last>Magdy</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–56</pages>
    <url>http://www.aclweb.org/anthology/W17-3008</url>
    <doi>10.18653/v1/W17-3008</doi>
    <abstract>
      In this paper, we present our work on detecting abusive language on Arabic
      social media. We extract a list of obscene words and hashtags using common
      patterns used in offensive and rude communications. We also classify
      Twitter users according to whether they use any of these words or not in
      their tweets. We expand the list of obscene words using this
      classification, and we report results on a newly created dataset of
      classified Arabic tweets (obscene, offensive, and clean). We make this
      dataset freely available for research, in addition to the list of obscene
      words and hashtags. We are also publicly releasing a large corpus of
      classified user comments that were deleted from a popular Arabic news site
      due to violations the site’s rules and guidelines.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mubarak-darwish-magdy:2017:ALW1</bibkey>
  </paper>
  <paper id='3009'>
    <title>Vectors for Counterspeech on Twitter</title>
    <author>
      <first>Lucas</first>
      <last>Wright</last>
    </author>
    <author>
      <first>Derek</first>
      <last>Ruths</last>
    </author>
    <author>
      <first>Kelly P</first>
      <last>Dillon</last>
    </author>
    <author>
      <first>Haji Mohammad</first>
      <last>Saleem</last>
    </author>
    <author>
      <first>Susan</first>
      <last>Benesch</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57–62</pages>
    <url>http://www.aclweb.org/anthology/W17-3009</url>
    <doi>10.18653/v1/W17-3009</doi>
    <abstract>
      A study of conversations on Twitter found that some arguments between
      strangers led to favorable change in discourse and even in attitudes. The
      authors propose that such exchanges can be usefully distinguished
      according to whether individuals or groups take part on each side, since
      the opportunity for a constructive exchange of views seems to vary
      accordingly.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wright-EtAl:2017:ALW1</bibkey>
  </paper>
  <paper id='3010'>
    <title>Detecting Nastiness in Social Media</title>
    <author>
      <first>Niloofar</first>
      <last>Safi Samghabadi</last>
    </author>
    <author>
      <first>Suraj</first>
      <last>Maharjan</last>
    </author>
    <author>
      <first>Alan</first>
      <last>Sprague</last>
    </author>
    <author>
      <first>Raquel</first>
      <last>Diaz-Sprague</last>
    </author>
    <author>
      <first>Thamar</first>
      <last>Solorio</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>63–72</pages>
    <url>http://www.aclweb.org/anthology/W17-3010</url>
    <doi>10.18653/v1/W17-3010</doi>
    <abstract>
      Although social media has made it easy for people to connect on a
      virtually unlimited basis, it has also opened doors to people who misuse
      it to undermine, harass, humiliate, threaten and bully others. There is a
      lack of adequate resources to detect and hinder its occurrence. In this
      paper, we present our initial NLP approach to detect invective posts as a
      first step to eventually detect and deter cyberbullying. We crawl data
      containing profanities and then determine whether or not it contains
      invective. Annotations on this data are improved iteratively by in-lab
      annotations and crowdsourcing. We pursue different NLP approaches
      containing various typical and some newer techniques to distinguish the
      use of swear words in a neutral way from those instances in which they are
      used in an insulting way. We also show that this model not only works for
      our data set, but also can be successfully applied to different data sets.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>safisamghabadi-EtAl:2017:ALW1</bibkey>
  </paper>
  <paper id='3011'>
    <title>Technology Solutions to Combat Online Harassment</title>
    <author>
      <first>George</first>
      <last>Kennedy</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>McCollough</last>
    </author>
    <author>
      <first>Edward</first>
      <last>Dixon</last>
    </author>
    <author>
      <first>Alexei</first>
      <last>Bastidas</last>
    </author>
    <author>
      <first>John</first>
      <last>Ryan</last>
    </author>
    <author>
      <first>Chris</first>
      <last>Loo</last>
    </author>
    <author>
      <first>Saurav</first>
      <last>Sahay</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73–77</pages>
    <url>http://www.aclweb.org/anthology/W17-3011</url>
    <doi>10.18653/v1/W17-3011</doi>
    <abstract>
      This work is part of a new initiative to use machine learning to identify
      online harassment in social media and comment streams. Online harassment
      goes under-reported due to the reliance on humans to identify and report
      harassment, reporting that is further slowed by requirements to fill out
      forms providing context. In addition, the time for moderators to respond
      and apply human judgment can take days, but response times in terms of
      minutes are needed in the online context. Though some of the major social
      media companies have been doing proprietary work in automating the
      detection of harassment, there are few tools available for use by the
      public. In addition, the amount of labeled online harassment data and
      availability of cross-platform online harassment datasets is limited. We
      present the methodology used to create a harassment dataset and classifier
      and the dataset used to help the system learn what harassment looks like.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kennedy-EtAl:2017:ALW1</bibkey>
  </paper>
  <paper id='3012'>
    <title>Understanding Abuse: A Typology of Abusive Language Detection Subtasks</title>
    <author>
      <first>Zeerak</first>
      <last>Waseem</last>
    </author>
    <author>
      <first>Thomas</first>
      <last>Davidson</last>
    </author>
    <author>
      <first>Dana</first>
      <last>Warmsley</last>
    </author>
    <author>
      <first>Ingmar</first>
      <last>Weber</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>78–84</pages>
    <url>http://www.aclweb.org/anthology/W17-3012</url>
    <doi>10.18653/v1/W17-3012</doi>
    <abstract>
      As the body of research on abusive language detection and analysis grows,
      there is a need for critical consideration of the relationships between
      different subtasks that have been grouped under this label. Based on work
      on hate speech, cyberbullying, and online abuse we propose a typology that
      captures central similarities and differences between subtasks and discuss
      the implications of this for data annotation and feature construction. We
      emphasize the practical actions that can be taken by researchers to best
      approach their abusive language detection subtask of interest.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>waseem-EtAl:2017:ALW1</bibkey>
  </paper>
  <paper id='3013'>
    <title>Using Convolutional Neural Networks to Classify Hate-Speech</title>
    <author>
      <first>Björn</first>
      <last>Gambäck</last>
    </author>
    <author>
      <first>Utpal Kumar</first>
      <last>Sikdar</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>85–90</pages>
    <url>http://www.aclweb.org/anthology/W17-3013</url>
    <doi>10.18653/v1/W17-3013</doi>
    <abstract>
      The paper introduces a deep learning-based Twitter hate-speech text
      classification system. The classifier assigns each tweet to one of four
      predefined categories: racism, sexism, both (racism and sexism) and
      non-hate-speech. Four Convolutional Neural Network models were trained on
      resp. character 4-grams, word vectors based on semantic information built
      using word2vec, randomly generated word vectors, and word vectors combined
      with character n-grams. The feature set was down-sized in the networks by
      max-pooling, and a softmax function used to classify tweets. Tested by
      10-fold cross-validation, the model based on word2vec embeddings performed
      best, with higher precision than recall, and a 78.3% F-score.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gamback-sikdar:2017:ALW1</bibkey>
  </paper>
  <paper id='3014'>
    <title>
      Illegal is not a Noun: Linguistic Form for Detection of Pejorative
      Nominalizations
    </title>
    <author>
      <first>Alexis</first>
      <last>Palmer</last>
    </author>
    <author>
      <first>Melissa</first>
      <last>Robinson</last>
    </author>
    <author>
      <first>Kristy K.</first>
      <last>Phillips</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Abusive Language Online</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC, Canada</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–100</pages>
    <url>http://www.aclweb.org/anthology/W17-3014</url>
    <doi>10.18653/v1/W17-3014</doi>
    <abstract>
      This paper focuses on a particular type of abusive language, targeting
      expressions in which typically neutral adjectives take on pejorative
      meaning when used as nouns - compare 'gay people' to 'the gays'. We first
      collect and analyze a corpus of hand-curated, expert-annotated pejorative
      nominalizations for four target adjectives: female, gay, illegal, and
      poor. We then collect a second corpus of automatically-extracted and
      POS-tagged, crowd-annotated tweets. For both corpora, we find support for
      the hypothesis that some adjectives, when nominalized, take on negative
      meaning. The targeted constructions are non-standard yet widely-used, and
      part-of-speech taggers mistag some nominal forms as adjectives. We
      implement a tool called NomCatcher to correct these mistaggings, and find
      that the same tool is effective for identifying new adjectives subject to
      transformation via nominalization into abusive language.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>palmer-robinson-phillips:2017:ALW1</bibkey>
  </paper>
  <paper id='3100'>
    <title>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </title>
    <editor>Kristy Hollingshead</editor>
    <editor>Molly E. Ireland</editor>
    <editor>Kate Loveys</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-31</url>
    <doi>10.18653/v1/W17-31</doi>
    <bibtype>book</bibtype>
    <bibkey>CLPsych:2017</bibkey>
  </paper>
  <paper id='3101'>
    <title>A Cross-modal Review of Indicators for Depression Detection Systems</title>
    <author>
      <first>Michelle</first>
      <last>Morales</last>
    </author>
    <author>
      <first>Stefan</first>
      <last>Scherer</last>
    </author>
    <author>
      <first>Rivka</first>
      <last>Levitan</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <url>http://www.aclweb.org/anthology/W17-3101</url>
    <doi>10.18653/v1/W17-3101</doi>
    <abstract>
      Automatic detection of depression has attracted increasing attention from
      researchers in psychology, computer science, linguistics, and related
      disciplines. As a result, promising depression detection systems have been
      reported. This paper surveys these efforts by presenting the first
      cross-modal review of depression detection systems and discusses best
      practices and most promising approaches to this task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>morales-scherer-levitan:2017:CLPsych</bibkey>
  </paper>
  <paper id='3102'>
    <title>In your wildest dreams: the language and psychological features of dreams</title>
    <author>
      <first>Kate</first>
      <last>Niederhoffer</last>
    </author>
    <author>
      <first>Jonathan</first>
      <last>Schler</last>
    </author>
    <author>
      <first>Patrick</first>
      <last>Crutchley</last>
    </author>
    <author>
      <first>Kate</first>
      <last>Loveys</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Coppersmith</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–25</pages>
    <url>http://www.aclweb.org/anthology/W17-3102</url>
    <doi>10.18653/v1/W17-3102</doi>
    <abstract>
      In this paper, we provide the first quantified exploration of the
      structure of the language of dreams, their linguistic style and emotional
      content. We present a collection of digital dream logs as a viable corpus
      for the growing study of mental health through the lens of language,
      complementary to the work done examining more traditional social media.
      This paper is largely exploratory in nature to lay the groundwork for
      subsequent research in mental health, rather than optimizing a particular
      text classification task.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>niederhoffer-EtAl:2017:CLPsych</bibkey>
  </paper>
  <paper id='3103'>
    <title>
      A Corpus Analysis of Social Connections and Social Isolation in
      Adolescents Suffering from Depressive Disorders
    </title>
    <author>
      <first>Jia-Wen</first>
      <last>Guo</last>
    </author>
    <author>
      <first>Danielle L</first>
      <last>Mowery</last>
    </author>
    <author>
      <first>Djin</first>
      <last>Lai</last>
    </author>
    <author>
      <first>Katherine</first>
      <last>Sward</last>
    </author>
    <author>
      <first>Mike</first>
      <last>Conway</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26–31</pages>
    <url>http://www.aclweb.org/anthology/W17-3103</url>
    <doi>10.18653/v1/W17-3103</doi>
    <abstract>
      Social connection and social isolation are associated with depressive
      symptoms, particularly in adolescents and young adults, but how these
      concepts are documented in clinical notes is unknown. This pilot study
      aimed to identify the topics relevant to social connection and isolation
      by analyzing 145 clinical notes from patients with depression diagnosis.
      We found that providers, including physicians, nurses, social workers, and
      psychologists, document descriptions of both social connection and social
      isolation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>guo-EtAl:2017:CLPsych</bibkey>
  </paper>
  <paper id='3104'>
    <title>Monitoring Tweets for Depression to Detect At-risk Users</title>
    <author>
      <first>Zunaira</first>
      <last>Jamil</last>
    </author>
    <author>
      <first>Diana</first>
      <last>Inkpen</last>
    </author>
    <author>
      <first>Prasadith</first>
      <last>Buddhitha</last>
    </author>
    <author>
      <first>Kenton</first>
      <last>White</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–40</pages>
    <url>http://www.aclweb.org/anthology/W17-3104</url>
    <doi>10.18653/v1/W17-3104</doi>
    <abstract>
      We propose an automated system that can identify at-risk users from their
      public social media activity, more specifically, from Twitter. The data
      that we collected is from the #BellLetsTalk campaign, which is a
      wide-reaching, multi-year program designed to break the silence around
      mental illness and support mental health across Canada. To achieve our
      goal, we trained a user-level classifier that can detect at-risk users
      that achieves a reasonable precision and recall. We also trained a
      tweet-level classifier that predicts if a tweet indicates depression. This
      task was much more difficult due to the imbalanced data. In the dataset
      that we labeled, we came across 5% depression tweets and 95%
      non-depression tweets. To handle this class imbalance, we used
      undersampling methods. The resulting classifier had high recall, but low
      precision. Therefore, we only use this classifier to compute the estimated
      percentage of depressed tweets and to add this value as a feature for the
      user-level classifier.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jamil-EtAl:2017:CLPsych</bibkey>
  </paper>
  <paper id='3105'>
    <title>
      Investigating Patient Attitudes Towards the use of Social Media Data to
      Augment Depression Diagnosis and Treatment: a Qualitative Study
    </title>
    <author>
      <first>Jude</first>
      <last>Mikal</last>
    </author>
    <author>
      <first>Samantha</first>
      <last>Hurst</last>
    </author>
    <author>
      <first>Mike</first>
      <last>Conway</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41–47</pages>
    <url>http://www.aclweb.org/anthology/W17-3105</url>
    <doi>10.18653/v1/W17-3105</doi>
    <abstract>
      In this paper, we use qualitative research methods to investigate the
      attitudes of social media users towards the (opt-in) integration of social
      media data with routine mental health care and diagnosis. Our
      investigation was based on secondary analysis of a series of five focus
      groups with Twitter users, including three groups consisting of
      participants with a self-reported history of depression, and two groups
      consisting of participants without a self reported history of depression.
      Our results indicate that, overall, research participants were
      enthusiastic about the possibility of using social media (in conjunction
      with automated Natural Language Processing algorithms) for mood tracking
      under the supervision of a mental health practitioner. However, for at
      least some participants, there was skepticism related to how well social
      media represents the mental health of users, and hence its usefulness in
      the clinical context.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mikal-hurst-conway:2017:CLPsych</bibkey>
  </paper>
  <paper id='3106'>
    <title>
      Natural-language Interactive Narratives in Imaginal Exposure Therapy for
      Obsessive-Compulsive Disorder
    </title>
    <author>
      <first>Melissa</first>
      <last>Roemmele</last>
    </author>
    <author>
      <first>Paola</first>
      <last>Mardo</last>
    </author>
    <author>
      <first>Andrew</first>
      <last>Gordon</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48–57</pages>
    <url>http://www.aclweb.org/anthology/W17-3106</url>
    <doi>10.18653/v1/W17-3106</doi>
    <abstract>
      Obsessive-compulsive disorder (OCD) is an anxiety-based disorder that
      affects around 2.5% of the population. A common treatment for OCD is
      exposure therapy, where the patient repeatedly confronts a feared
      experience, which has the long-term effect of decreasing their anxiety.
      Some exposures consist of reading and writing stories about an imagined
      anxiety-provoking scenario. In this paper, we present a technology that
      enables patients to interactively contribute to exposure stories by
      supplying natural language input (typed or spoken) that advances a
      scenario. This interactivity could potentially increase the patient's
      sense of immersion in an exposure and contribute to its success. We
      introduce the NLP task behind processing inputs to predict new events in
      the scenario, and describe our initial approach. We then illustrate the
      future possibility of this work with an example of an exposure scenario
      authored with our application.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>roemmele-mardo-gordon:2017:CLPsych</bibkey>
  </paper>
  <paper id='3107'>
    <title>Detecting Anxiety through Reddit</title>
    <author>
      <first>Judy Hanwen</first>
      <last>Shen</last>
    </author>
    <author>
      <first>Frank</first>
      <last>Rudzicz</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>58–65</pages>
    <url>http://www.aclweb.org/anthology/W17-3107</url>
    <doi>10.18653/v1/W17-3107</doi>
    <abstract>
      Previous investigations into detecting mental illnesses through social
      media have predominately focused on detecting depression through Twitter
      corpora. In this paper, we study anxiety disorders through personal
      narratives collected through the popular social media website, Reddit. We
      build a substantial data set of typical and anxiety-related posts, and we
      apply N-gram language modeling, vector embeddings, topic analysis, and
      emotional norms to generate features that accurately classify posts
      related to binary levels of anxiety. We achieve an accuracy of 91% with
      vector-space word embeddings, and an accuracy of 98% when combined with
      lexicon-based features.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shen-rudzicz:2017:CLPsych</bibkey>
  </paper>
  <paper id='3108'>
    <title>Detecting and Explaining Crisis</title>
    <author>
      <first>Rohan</first>
      <last>Kshirsagar</last>
    </author>
    <author>
      <first>Robert</first>
      <last>Morris</last>
    </author>
    <author>
      <first>Samuel</first>
      <last>Bowman</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–73</pages>
    <url>http://www.aclweb.org/anthology/W17-3108</url>
    <doi>10.18653/v1/W17-3108</doi>
    <abstract>
      Individuals on social media may reveal themselves to be in various states
      of crisis (e.g. suicide, self-harm, abuse, or eating disorders). Detecting
      crisis from social media text automatically and accurately can have
      profound consequences. However, detecting a general state of crisis
      without explaining why has limited applications. An explanation in this
      context is a coherent, concise subset of the text that rationalizes the
      crisis detection. We explore several methods to detect and explain crisis
      using a combination of neural and non-neural techniques. We evaluate these
      techniques on a unique data set obtained from Koko, an anonymous emotional
      support network available through various messaging applications. We
      annotate a small subset of the samples labeled with crisis with
      corresponding explanations. Our best technique significantly outperforms
      the baseline for detection and explanation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kshirsagar-morris-bowman:2017:CLPsych</bibkey>
  </paper>
  <paper id='3109'>
    <title>
      A Dictionary-Based Comparison of Autobiographies by People and Murderous
      Monsters
    </title>
    <author>
      <first>Micah</first>
      <last>Iserman</last>
    </author>
    <author>
      <first>Molly</first>
      <last>Ireland</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74–84</pages>
    <url>http://www.aclweb.org/anthology/W17-3109</url>
    <doi>10.18653/v1/W17-3109</doi>
    <abstract>
      People typically assume that killers are mentally ill or fundamentally
      different from the rest of humanity. Similarly, people often associate
      mental health conditions (such as schizophrenia or autism) with violence
      and otherness - treatable perhaps, but not empathically understandable. We
      take a dictionary approach to explore word use in a set of
      autobiographies, comparing the narratives of 2 killers (Adolf Hitler and
      Elliot Rodger) and 39 non-killers. Although results suggest several
      dimensions that differentiate these autobiographies - such as sentiment,
      temporal orientation, and references to death - they appear to reflect
      subject matter rather than psychology per se. Additionally, the Rodger
      text shows roughly typical developmental arcs in its use of words relating
      to friends, family, sex, and affect. From these data, we discuss the
      challenges of understanding killers and people in general.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>iserman-ireland:2017:CLPsych</bibkey>
  </paper>
  <paper id='3110'>
    <title>
      Small but Mighty: Affective Micropatterns for Quantifying Mental Health
      from Social Media Language
    </title>
    <author>
      <first>Kate</first>
      <last>Loveys</last>
    </author>
    <author>
      <first>Patrick</first>
      <last>Crutchley</last>
    </author>
    <author>
      <first>Emily</first>
      <last>Wyatt</last>
    </author>
    <author>
      <first>Glen</first>
      <last>Coppersmith</last>
    </author>
    <booktitle>
      Proceedings of the Fourth Workshop on Computational Linguistics and
      Clinical Psychology –- From Linguistic Signal to Clinical Reality
    </booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver, BC</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>85–95</pages>
    <url>http://www.aclweb.org/anthology/W17-3110</url>
    <doi>10.18653/v1/W17-3110</doi>
    <abstract>
      Many psychological phenomena occur in small time windows, measured in
      minutes or hours. However, most computational linguistic techniques look
      at data on the order of weeks, months, or years. We explore micropatterns
      in sequences of messages occurring over a short time window for their
      prevalence and power for quantifying psychological phenomena,
      specifically, patterns in affect. We examine affective micropatterns in
      social media posts from users with anxiety, eating disorders, panic
      attacks, schizophrenia, suicidality, and matched controls.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>loveys-EtAl:2017:CLPsych</bibkey>
  </paper>
  <paper id='3200'>
    <title>Proceedings of the First Workshop on Neural Machine Translation</title>
    <editor>Thang Luong</editor>
    <editor>Alexandra Birch</editor>
    <editor>Graham Neubig</editor>
    <editor>Andrew Finch</editor>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-32</url>
    <doi>10.18653/v1/W17-32</doi>
    <bibtype>book</bibtype>
    <bibkey>NMT:2017</bibkey>
  </paper>
  <paper id='3201'>
    <title>
      An Empirical Study of Adequate Vision Span for Attention-Based Neural
      Machine Translation
    </title>
    <author>
      <first>Raphael</first>
      <last>Shu</last>
    </author>
    <author>
      <first>Hideki</first>
      <last>Nakayama</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-3201</url>
    <doi>10.18653/v1/W17-3201</doi>
    <abstract>
      Recently, the attention mechanism plays a key role to achieve high
      performance for Neural Machine Translation models. However, as it computes
      a score function for the encoder states in all positions at each decoding
      step, the attention model greatly increases the computational complexity.
      In this paper, we investigate the adequate vision span of attention models
      in the context of machine translation, by proposing a novel attention
      framework that is capable of reducing redundant score computation
      dynamically. The term "vision span"' means a window of the encoder states
      considered by the attention model in one step. In our experiments, we
      found that the average window size of vision span can be reduced by over
      50% with modest loss in accuracy on English-Japanese and German-English
      translation tasks.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>shu-nakayama:2017:NMT</bibkey>
  </paper>
  <paper id='3202'>
    <title>Analyzing Neural MT Search and Model Performance</title>
    <author>
      <first>Jan</first>
      <last>Niehues</last>
    </author>
    <author>
      <first>Eunah</first>
      <last>Cho</last>
    </author>
    <author>
      <first>Thanh-Le</first>
      <last>Ha</last>
    </author>
    <author>
      <first>Alex</first>
      <last>Waibel</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–17</pages>
    <url>http://www.aclweb.org/anthology/W17-3202</url>
    <doi>10.18653/v1/W17-3202</doi>
    <abstract>
      In this paper, we offer an in-depth analysis about the modeling and search
      performance. We address the question if a more complex search algorithm is
      necessary. Furthermore, we investigate the question if more complex models
      which might only be applicable during rescoring are promising. By
      separating the search space and the modeling using n-best list reranking,
      we analyze the influence of both parts of an NMT system independently. By
      comparing differently performing NMT systems, we show that the better
      translation is already in the search space of the translation systems with
      less performance. This results indicate that the current search algorithms
      are sufficient for the NMT systems. Furthermore, we could show that even a
      relatively small $n$-best list of $50$ hypotheses already contain notably
      better translations.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>niehues-EtAl:2017:NMT</bibkey>
  </paper>
  <paper id='3203'>
    <title>Stronger Baselines for Trustable Results in Neural Machine Translation</title>
    <author>
      <first>Michael</first>
      <last>Denkowski</last>
    </author>
    <author>
      <first>Graham</first>
      <last>Neubig</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18–27</pages>
    <url>http://www.aclweb.org/anthology/W17-3203</url>
    <doi>10.18653/v1/W17-3203</doi>
    <abstract>
      Interest in neural machine translation has grown rapidly as its
      effectiveness has been demonstrated across language and data scenarios.
      New research regularly introduces architectural and algorithmic
      improvements that lead to significant gains over “vanilla” NMT
      implementations. However, these new techniques are rarely evaluated in the
      context of previously published techniques, specifically those that are
      widely used in state-of-the-art production and shared-task systems. As a
      result, it is often difficult to determine whether improvements from
      research will carry over to systems deployed for real-world use. In this
      work, we recommend three specific methods that are relatively easy to
      implement and result in much stronger experimental systems. Beyond
      reporting significantly higher BLEU scores, we conduct an in-depth
      analysis of where improvements originate and what inherent weaknesses of
      basic NMT models are being addressed. We then compare the relative gains
      afforded by several other techniques proposed in the literature when
      starting with vanilla systems versus our stronger baselines, showing that
      experimental conclusions may change depending on the baseline chosen. This
      indicates that choosing a strong baseline is crucial for reporting
      reliable experimental results.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>denkowski-neubig:2017:NMT</bibkey>
  </paper>
  <paper id='3204'>
    <title>Six Challenges for Neural Machine Translation</title>
    <author>
      <first>Philipp</first>
      <last>Koehn</last>
    </author>
    <author>
      <first>Rebecca</first>
      <last>Knowles</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>28–39</pages>
    <url>http://www.aclweb.org/anthology/W17-3204</url>
    <doi>10.18653/v1/W17-3204</doi>
    <abstract>
      We explore six challenges for neural machine translation: domain mismatch,
      amount of training data, rare words, long sentences, word alignment, and
      beam search. We show both deficiencies and improvements over the quality
      of phrase-based statistical machine translation.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koehn-knowles:2017:NMT</bibkey>
  </paper>
  <paper id='3205'>
    <title>Cost Weighting for Neural Machine Translation Domain Adaptation</title>
    <author>
      <first>Boxing</first>
      <last>Chen</last>
    </author>
    <author>
      <first>Colin</first>
      <last>Cherry</last>
    </author>
    <author>
      <first>George</first>
      <last>Foster</last>
    </author>
    <author>
      <first>Samuel</first>
      <last>Larkin</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>40–46</pages>
    <url>http://www.aclweb.org/anthology/W17-3205</url>
    <doi>10.18653/v1/W17-3205</doi>
    <abstract>
      In this paper, we propose a new domain adaptation technique for neural
      machine translation called cost weighting, which is appropriate for
      adaptation scenarios in which a small in-domain data set and a large
      general-domain data set are available. Cost weighting incorporates a
      domain classifier into the neural machine translation training algorithm,
      using features derived from the encoder representation in order to
      distinguish in-domain from out-of-domain data. Classifier probabilities
      are used to weight sentences according to their domain similarity when
      updating the parameters of the neural translation model. We compare cost
      weighting to two traditional domain adaptation techniques developed for
      statistical machine translation: data selection and sub-corpus weighting.
      Experiments on two large-data tasks show that both the traditional
      techniques and our novel proposal lead to significant gains, with cost
      weighting outperforming the traditional methods.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chen-EtAl:2017:NMT</bibkey>
  </paper>
  <paper id='3206'>
    <title>Detecting Untranslated Content for Neural Machine Translation</title>
    <author>
      <first>Isao</first>
      <last>Goto</last>
    </author>
    <author>
      <first>Hideki</first>
      <last>Tanaka</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–55</pages>
    <url>http://www.aclweb.org/anthology/W17-3206</url>
    <doi>10.18653/v1/W17-3206</doi>
    <abstract>
      Despite its promise, neural machine translation (NMT) has a serious
      problem in that source content may be mistakenly left untranslated. The
      ability to detect untranslated content is important for the practical use
      of NMT. We evaluate two types of probability with which to detect
      untranslated content: the cumulative attention (ATN) probability and back
      translation (BT) probability from the target sentence to the source
      sentence. Experiments on detecting untranslated content in
      Japanese-English patent translations show that ATN and BT are each more
      effective than random choice, BT is more effective than ATN, and the
      combination of the two provides further improvements. We also confirmed
      the effectiveness of using ATN and BT to rerank the n-best NMT outputs.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>goto-tanaka:2017:NMT</bibkey>
  </paper>
  <paper id='3207'>
    <title>Beam Search Strategies for Neural Machine Translation</title>
    <author>
      <first>Markus</first>
      <last>Freitag</last>
    </author>
    <author>
      <first>Yaser</first>
      <last>Al-Onaizan</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56–60</pages>
    <url>http://www.aclweb.org/anthology/W17-3207</url>
    <doi>10.18653/v1/W17-3207</doi>
    <abstract>
      The basic concept in Neural Machine Translation (NMT) is to train a large
      Neural Network that maximizes the translation performance on a given
      parallel corpus. NMT is then using a simple left-to-right beam-search
      decoder to generate new translations that approximately maximize the
      trained conditional probability. The current beam search strategy
      generates the target sentence word by word from left-to-right while
      keeping a fixed amount of active candidates at each time step. First, this
      simple search is less adaptive as it also expands candidates whose scores
      are much worse than the current best. Secondly, it does not expand
      hypotheses if they are not within the best scoring candidates, even if
      their scores are close to the best one. The latter one can be avoided by
      increasing the beam size until no performance improvement can be observed.
      While you can reach better performance, this has the drawback of a slower
      decoding speed. In this paper, we concentrate on speeding up the decoder
      by applying a more flexible beam search strategy whose candidate size may
      vary at each time step depending on the candidate scores. We speed up the
      original decoder by up to 43% for the two language pairs German to English
      and Chinese to English without losing any translation quality.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>freitag-alonaizan:2017:NMT</bibkey>
  </paper>
  <paper id='3208'>
    <title>
      An Empirical Study of Mini-Batch Creation Strategies for Neural Machine
      Translation
    </title>
    <author>
      <first>Makoto</first>
      <last>Morishita</last>
    </author>
    <author>
      <first>Yusuke</first>
      <last>Oda</last>
    </author>
    <author>
      <first>Graham</first>
      <last>Neubig</last>
    </author>
    <author>
      <first>Koichiro</first>
      <last>Yoshino</last>
    </author>
    <author>
      <first>Katsuhito</first>
      <last>Sudoh</last>
    </author>
    <author>
      <first>Satoshi</first>
      <last>Nakamura</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61–68</pages>
    <url>http://www.aclweb.org/anthology/W17-3208</url>
    <doi>10.18653/v1/W17-3208</doi>
    <abstract>
      Training of neural machine translation (NMT) models usually uses
      mini-batches for efficiency purposes. During the mini-batched training
      process, it is necessary to pad shorter sentences in a mini-batch to be
      equal in length to the longest sentence therein for efficient computation.
      Previous work has noted that sorting the corpus based on the sentence
      length before making mini-batches reduces the amount of padding and
      increases the processing speed. However, despite the fact that mini-batch
      creation is an essential step in NMT training, widely used NMT toolkits
      implement disparate strategies for doing so, which have not been
      empirically validated or compared. This work investigates mini-batch
      creation strategies with experiments over two different datasets. Our
      results suggest that the choice of a mini-batch creation strategy has a
      large effect on NMT training and some length-based sorting strategies do
      not always work well compared with simple shuffling.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>morishita-EtAl:2017:NMT</bibkey>
  </paper>
  <paper id='3209'>
    <title>Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation</title>
    <author>
      <first>Marine</first>
      <last>Carpuat</last>
    </author>
    <author>
      <first>Yogarshi</first>
      <last>Vyas</last>
    </author>
    <author>
      <first>Xing</first>
      <last>Niu</last>
    </author>
    <booktitle>Proceedings of the First Workshop on Neural Machine Translation</booktitle>
    <month>August</month>
    <year>2017</year>
    <address>Vancouver</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69–79</pages>
    <url>http://www.aclweb.org/anthology/W17-3209</url>
    <doi>10.18653/v1/W17-3209</doi>
    <abstract>
      Parallel corpora are often not as parallel as one might assume:
      non-literal translations and noisy translations abound, even in curated
      corpora routinely used for training and evaluation. We use a cross-lingual
      textual entailment system to distinguish sentence pairs that are parallel
      in meaning from those that are not, and show that filtering out divergent
      examples from training improves translation quality.
    </abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>carpuat-vyas-niu:2017:NMT</bibkey>
  </paper>
  <paper id='3400'>
    <title>Proceedings of the 15th Meeting on the Mathematics of Language</title>
    <editor>
      <first>Makoto</first>
      <last>Kanazawa</last>
    </editor>
    <editor>
      <first>Philippe</first>
      <last>de Groote</last>
    </editor>
    <editor>
      <first>Mehrnoosh</first>
      <last>Sadrzadeh</last>
    </editor>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-34</url>
    <doi>10.18653/v1/W17-34</doi>
    <bibtype>book</bibtype>
    <bibkey>MOL:2017</bibkey>
  </paper>
  <paper id='3401'>
    <title>BE Is Not the Unique Homomorphism That Makes the Partee Triangle Commute</title>
    <author>
      <first>Junri</first>
      <last>Shimada</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–10</pages>
    <url>http://www.aclweb.org/anthology/W17-3401</url>
    <doi>10.18653/v1/W17-3401</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>shimada:2017:MOL</bibkey>
  </paper>
  <paper id='3402'>
    <title>How Many Stemmata with Root Degree k?</title>
    <author>
      <first>Armin</first>
      <last>Hoenen</last>
    </author>
    <author>
      <first>Steffen</first>
      <last>Eger</last>
    </author>
    <author>
      <first>Ralf</first>
      <last>Gehrke</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11–21</pages>
    <url>http://www.aclweb.org/anthology/W17-3402</url>
    <doi>10.18653/v1/W17-3402</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>hoenen-eger-gehrke:2017:MOL</bibkey>
  </paper>
  <paper id='3403'>
    <title>On the Logical Complexity of Autosegmental Representations</title>
    <author>
      <first>Adam</first>
      <last>Jardine</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22–35</pages>
    <url>http://www.aclweb.org/anthology/W17-3403</url>
    <doi>10.18653/v1/W17-3403</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>jardine:2017:MOL</bibkey>
  </paper>
  <paper id='3404'>
    <title>Extracting Forbidden Factors from Regular Stringsets</title>
    <author>
      <first>James</first>
      <last>Rogers</last>
    </author>
    <author>
      <first>Dakotah</first>
      <last>Lambert</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36–46</pages>
    <url>http://www.aclweb.org/anthology/W17-3404</url>
    <doi>10.18653/v1/W17-3404</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>rogers-lambert:2017:MOL</bibkey>
  </paper>
  <paper id='3405'>
    <title>Latent-Variable PCFGs: Background and Applications</title>
    <author>
      <first>Shay</first>
      <last>Cohen</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47–58</pages>
    <url>http://www.aclweb.org/anthology/W17-3405</url>
    <doi>10.18653/v1/W17-3405</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>cohen:2017:MOL</bibkey>
  </paper>
  <paper id='3406'>
    <title>A Proof-Theoretic Semantics for Transitive Verbs with an Implicit Object</title>
    <author>
      <first>Nissim</first>
      <last>Francez</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–67</pages>
    <url>http://www.aclweb.org/anthology/W17-3406</url>
    <doi>10.18653/v1/W17-3406</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>francez:2017:MOL</bibkey>
  </paper>
  <paper id='3407'>
    <title>Why We Speak</title>
    <author>
      <first>Rohit</first>
      <last>Parikh</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>68–74</pages>
    <url>http://www.aclweb.org/anthology/W17-3407</url>
    <doi>10.18653/v1/W17-3407</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>parikh:2017:MOL</bibkey>
  </paper>
  <paper id='3408'>
    <title>A Monotonicity Calculus and Its Completeness</title>
    <author>
      <first>Thomas</first>
      <last>Icard</last>
    </author>
    <author>
      <first>Lawrence</first>
      <last>Moss</last>
    </author>
    <author>
      <first>William</first>
      <last>Tune</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>75–87</pages>
    <url>http://www.aclweb.org/anthology/W17-3408</url>
    <doi>10.18653/v1/W17-3408</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>icard-moss-tune:2017:MOL</bibkey>
  </paper>
  <paper id='3409'>
    <title>DAG Automata for Meaning Representation</title>
    <author>
      <first>Frank</first>
      <last>Drewes</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>88–99</pages>
    <url>http://www.aclweb.org/anthology/W17-3409</url>
    <doi>10.18653/v1/W17-3409</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>drewes:2017:MOL</bibkey>
  </paper>
  <paper id='3410'>
    <title>(Re)introducing Regular Graph Languages</title>
    <author>
      <first>Sorcha</first>
      <last>Gilroy</last>
    </author>
    <author>
      <first>Adam</first>
      <last>Lopez</last>
    </author>
    <author>
      <first>Sebastian</first>
      <last>Maneth</last>
    </author>
    <author>
      <first>Pijus</first>
      <last>Simonaitis</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>100–113</pages>
    <url>http://www.aclweb.org/anthology/W17-3410</url>
    <doi>10.18653/v1/W17-3410</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>gilroy-EtAl:2017:MOL</bibkey>
  </paper>
  <paper id='3411'>
    <title>Graph Transductions and Typological Gaps in Morphological Paradigms</title>
    <author>
      <first>Thomas</first>
      <last>Graf</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>114–126</pages>
    <url>http://www.aclweb.org/anthology/W17-3411</url>
    <doi>10.18653/v1/W17-3411</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>graf:2017:MOL</bibkey>
  </paper>
  <paper id='3412'>
    <title>Introducing Structure into Neural Network-Based Semantic Models</title>
    <author>
      <first>Stephen</first>
      <last>Clark</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>127</pages>
    <url>http://www.aclweb.org/anthology/W17-3412</url>
    <doi>10.18653/v1/W17-3412</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>clark:2017:MOL</bibkey>
  </paper>
  <paper id='3413'>
    <title>Count-Invariance Including Exponentials</title>
    <author>
      <first>Stepan</first>
      <last>Kuznetsov</last>
    </author>
    <author>
      <first>Glyn</first>
      <last>Morrill</last>
    </author>
    <author>
      <first>Oriol</first>
      <last>Valentín</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>128–139</pages>
    <url>http://www.aclweb.org/anthology/W17-3413</url>
    <doi>10.18653/v1/W17-3413</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kuznetsov-morrill-valentin:2017:MOL</bibkey>
  </paper>
  <paper id='3414'>
    <title>Conjunctive Categorial Grammars</title>
    <author>
      <first>Stepan</first>
      <last>Kuznetsov</last>
    </author>
    <author>
      <first>Alexander</first>
      <last>Okhotin</last>
    </author>
    <booktitle>Proceedings of the 15th Meeting on the Mathematics of Language</booktitle>
    <month>July</month>
    <year>2017</year>
    <address>London, UK</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>140–151</pages>
    <url>http://www.aclweb.org/anthology/W17-3414</url>
    <doi>10.18653/v1/W17-3414</doi>
    <bibtype>inproceedings</bibtype>
    <bibkey>kuznetsov-okhotin:2017:MOL</bibkey>
  </paper>
</volume>