<?xml version="1.0" encoding="UTF-8" ?>
<volume id="W17">
  <paper id="0600">
    <title>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</title>
    <editor>Francis M. Tyers</editor>
    <editor>Michael Rießler</editor>
    <editor>Tommi A. Pirinen</editor>
    <editor>Trond Trosterud</editor>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-06</url>
    <bibtype>book</bibtype>
    <bibkey>IWCLUL:2017</bibkey>
  </paper>

  <paper id="0601">
    <title>Synchronized Mediawiki based analyzer dictionary development</title>
    <author><first>Jack</first><last>Rueter</last></author>
    <author><first>Mika</first><last>H&#228;m&#228;l&#228;inen</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;7</pages>
    <url>http://www.aclweb.org/anthology/W17-0601</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rueter-hamalainen:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0602">
    <title>DEMO: Giellatekno Open-source click-in-text dictionaries for bringing closely related languages into contact.</title>
    <author><first>Jack</first><last>Rueter</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8&#8211;9</pages>
    <url>http://www.aclweb.org/anthology/W17-0602</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>rueter:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0603">
    <title>Languages under the influence: Building a database of Uralic languages</title>
    <author><first>Eszter</first><last>Simon</last></author>
    <author><first>Nikolett</first><last>Mus</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10&#8211;24</pages>
    <url>http://www.aclweb.org/anthology/W17-0603</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>simon-mus:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0604">
    <title>Instant Annotations ėxtendash Applying NLP Methods to the Annotation of Spoken Language Documentation Corpora</title>
    <author><first>Ciprian</first><last>Gerstenberger</last></author>
    <author><first>Niko</first><last>Partanen</last></author>
    <author><first>Michael</first><last>Rie&#223;ler</last></author>
    <author><first>Joshua</first><last>Wilbur</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25&#8211;36</pages>
    <url>http://www.aclweb.org/anthology/W17-0604</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>gerstenberger-EtAl:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0605">
    <title>Preliminary Experiments concerning Verbal Predicative Structure Extraction from a Large Finnish Corpus</title>
    <author><first>Guersande</first><last>Chaminade</last></author>
    <author><first>Thierry</first><last>Poibeau</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37&#8211;55</pages>
    <url>http://www.aclweb.org/anthology/W17-0605</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>chaminade-poibeau:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0606">
    <title>Language technology resources and tools for Mansi: an overview</title>
    <author><first>Csilla</first><last>Horv&#225;th</last></author>
    <author><first>Norbert</first><last>Szil&#225;gyi</last></author>
    <author><first>Veronika</first><last>Vincze</last></author>
    <author><first>&#192;goston</first><last>Nagy</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56&#8211;65</pages>
    <url>http://www.aclweb.org/anthology/W17-0606</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>horvath-EtAl:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0607">
    <title>Annotation schemes in North S&#225;mi dependency parsing</title>
    <author><first>Francis</first><last>M. Tyers</last></author>
    <author><first>Mariya</first><last>Sheyanova</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66&#8211;75</pages>
    <url>http://www.aclweb.org/anthology/W17-0607</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>mtyers-sheyanova:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0608">
    <title>A morphological analyser for Kven</title>
    <author><first>Sindre</first><last>Reino Trosterud</last></author>
    <author><first>Trond</first><last>Trosterud</last></author>
    <author><first>Anna-Kaisa</first><last>R&#228;is&#228;nen</last></author>
    <author><first>Leena</first><last>Niiranen</last></author>
    <author><first>Mervi</first><last>Haavisto</last></author>
    <author><first>Kaisa</first><last>Maliniemi</last></author>
    <booktitle>Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2017</year>
    <address>St. Petersburg, Russia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76&#8211;88</pages>
    <url>http://www.aclweb.org/anthology/W17-0608</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>reinotrosterud-EtAl:2017:IWCLUL</bibkey>
  </paper>

  <paper id="0700">
    <title>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</title>
    <editor>Ted Gibson</editor>
    <editor>Tal Linzen</editor>
    <editor>Asad Sayeed</editor>
    <editor>Martin van Schijndel</editor>
    <editor>William Schuler</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-07</url>
    <bibtype>book</bibtype>
    <bibkey>CMCL:2017</bibkey>
  </paper>

  <paper id="0701">
    <title>Entropy Reduction correlates with temporal lobe activity</title>
    <author><first>Matthew</first><last>Nelson</last></author>
    <author><first>Stanislas</first><last>Dehaene</last></author>
    <author><first>Christophe</first><last>Pallier</last></author>
    <author><first>John</first><last>Hale</last></author>
    <booktitle>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;10</pages>
    <url>http://www.aclweb.org/anthology/W17-0701</url>
    <abstract>Using the Entropy Reduction incremental complexity metric,
	we relate high gamma power signals from the brains of epileptic patients
	to incremental stages of syntactic analysis in English and French.
	We find that signals recorded intracranially from the anterior Inferior
	Temporal Sulcus (aITS) and
	the posterior Inferior Temporal Gyrus (pITG) correlate with word-by-word
	Entropy Reduction values
	derived from phrase structure grammars for those languages.
	In the anterior region, this correlation persists even in combination with
	surprisal co-predictors
	from PCFG and ngram models.
	The result confirms the idea that the brain's temporal lobe houses a parsing
	function,
	one whose incremental processing difficulty profile reflects changes in
	grammatical uncertainty.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nelson-EtAl:2017:CMCL</bibkey>
  </paper>

  <paper id="0702">
    <title>Learning an Input Filter for Argument Structure Acquisition</title>
    <author><first>Laurel</first><last>Perkins</last></author>
    <author><first>Naomi</first><last>Feldman</last></author>
    <author><first>Jeffrey</first><last>Lidz</last></author>
    <booktitle>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11&#8211;19</pages>
    <url>http://www.aclweb.org/anthology/W17-0702</url>
    <abstract>How do children learn a verb’s argument structure when their input contains
	nonbasic clauses that obscure verb transitivity? Here we present a new model
	that infers verb transitivity by learning to filter out non-basic clauses that
	were likely parsed in error. In simulations with child-directed speech, we show
	that this model accurately categorizes the majority of 50 frequent transitive,
	intransitive and alternating verbs, and jointly learns appropriate parameters
	for filtering parsing errors. Our model is thus able to filter out problematic
	data for verb learning without knowing in advance which data need to be
	filtered.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>perkins-feldman-lidz:2017:CMCL</bibkey>
  </paper>

  <paper id="0703">
    <title>Grounding sound change in ideal observer models of perception</title>
    <author><first>Zachary</first><last>Burchill</last></author>
    <author><first>T. Florian</first><last>Jaeger</last></author>
    <booktitle>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20&#8211;28</pages>
    <url>http://www.aclweb.org/anthology/W17-0703</url>
    <abstract>An important predictor of historical sound change, functional load, fails to
	capture insights from speech perception. Building on ideal observer models of
	word recognition, we devise a new definition of functional load that
	incorporates both a priori predictability and perceptual information. We
	explore this new measure with a simple model and find that it outperforms
	traditional measures.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>burchill-jaeger:2017:CMCL</bibkey>
  </paper>

  <paper id="0704">
    <title>&#x201c;Oh, I've Heard That Before": Modelling Own-Dialect Bias After Perceptual Learning by Weighting Training Data</title>
    <author><first>Rachael</first><last>Tatman</last></author>
    <booktitle>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>29&#8211;34</pages>
    <url>http://www.aclweb.org/anthology/W17-0704</url>
    <abstract>Human listeners are able to quickly and robustly adapt to new accents and do so
	by using information about speaker's identities. This paper will present
	experimental evidence that, even considering information about speaker's
	identities, listeners retain a strong bias towards the acoustics of their own
	dialect after dialect learning. Participants' behaviour was accurately mimicked
	by a classifier which was trained on more cases from the base dialect and fewer
	from the target dialect. This suggests that imbalanced training data may result
	in automatic speech recognition errors consistent with those of speakers from
	populations over-represented in the training data.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tatman:2017:CMCL</bibkey>
  </paper>

  <paper id="0705">
    <title>Inherent Biases of Recurrent Neural Networks for Phonological Assimilation and Dissimilation</title>
    <author><first>Amanda</first><last>Doucette</last></author>
    <booktitle>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35&#8211;40</pages>
    <url>http://www.aclweb.org/anthology/W17-0705</url>
    <abstract>A recurrent neural network model of phonological pattern learning is proposed.
	The model is a relatively simple neural network with one recurrent layer, and
	displays biases in learning that mimic observed biases in human learning.
	Single-feature patterns are learned faster than two-feature patterns, and vowel
	or consonant-only patterns are learned faster than patterns involving vowels
	and consonants, mimicking the results of laboratory learning experiments. In
	non-recurrent models, capturing these biases requires the use of alpha features
	or some other representation of repeated features, but with a recurrent neural
	network, these elaborations are not necessary.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>doucette:2017:CMCL</bibkey>
  </paper>

  <paper id="0706">
    <title>Predicting Japanese scrambling in the wild</title>
    <author><first>Naho</first><last>Orita</last></author>
    <booktitle>Proceedings of the 7th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41&#8211;45</pages>
    <url>http://www.aclweb.org/anthology/W17-0706</url>
    <abstract>Japanese speakers have a choice between canonical SOV and scrambled OSV word
	order to express the same meaning. Although previous experiments examine the
	influence of one or two factors for scrambling in a controlled setting, it is
	not yet known what kinds of multiple effects contribute to scrambling. This
	study uses naturally distributed data to test the multiple effects on
	scrambling simultaneously. A regression analysis replicates the NP length
	effect and suggests the influence of noun types, but it provides no evidence
	for syntactic priming, given-new ordering, and the animacy effect. These
	findings only show evidence for sentence-internal factors, but we find no
	evidence that discourse level factors play a role.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>orita:2017:CMCL</bibkey>
  </paper>

  <paper id="0800">
    <title>Proceedings of the 11th Linguistic Annotation Workshop</title>
    <editor>Nathan Schneider</editor>
    <editor>Nianwen Xue</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-08</url>
    <bibtype>book</bibtype>
    <bibkey>LAW:2017</bibkey>
  </paper>

  <paper id="0801">
    <title>Readers vs. Writers vs. Texts: Coping with Different Perspectives of Text Understanding in Emotion Annotation</title>
    <author><first>Sven</first><last>Buechel</last></author>
    <author><first>Udo</first><last>Hahn</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;12</pages>
    <url>http://www.aclweb.org/anthology/W17-0801</url>
    <abstract>We here examine how different perspectives of understanding written discourse,
	like the reader's, the writer's or the text's point of view, affect the quality
	of emotion annotations. We conducted a series of annotation experiments on two
	corpora, a popular movie review corpus and a genre- and domain-balanced corpus
	of standard English. We found statistical evidence that the writer's
	perspective yields superior annotation quality overall. However, the quality
	one perspective yields compared to the other(s) seems to depend on the domain
	the utterance originates from. Our data further suggest that the popular movie
	review data set suffers from an atypical bimodal distribution which may
	decrease model performance when used as a training resource.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>buechel-hahn:2017:LAW</bibkey>
  </paper>

  <paper id="0802">
    <title>Finding Good Conversations Online: The Yahoo News Annotated Comments Corpus</title>
    <author><first>Courtney</first><last>Napoles</last></author>
    <author><first>Joel</first><last>Tetreault</last></author>
    <author><first>Aasish</first><last>Pappu</last></author>
    <author><first>Enrica</first><last>Rosato</last></author>
    <author><first>Brian</first><last>Provenzale</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13&#8211;23</pages>
    <url>http://www.aclweb.org/anthology/W17-0802</url>
    <abstract>This work presents a dataset and annotation scheme for the new task of
	identifying "good" conversations that occur online, which we call ERICs:
	Engaging, Respectful, and/or Informative Conversations. We develop a taxonomy
	to reflect features of entire threads and individual comments which we believe
	contribute to identifying ERICs; code a novel dataset of Yahoo News comment
	threads (2.4k threads and 10k comments) and 1k threads from the Internet
	Argument Corpus; and analyze the features characteristic of ERICs. This is one
	of the largest annotated corpora of online human dialogues, with the most
	detailed set of annotations. It will be valuable for identifying ERICs and
	other aspects of argumentation, dialogue, and discourse.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>napoles-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0803">
    <title>Crowdsourcing discourse interpretations: On the influence of context and the reliability of a connective insertion task</title>
    <author><first>Merel</first><last>Scholman</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24&#8211;33</pages>
    <url>http://www.aclweb.org/anthology/W17-0803</url>
    <abstract>Traditional discourse annotation tasks are considered costly and
	time-consuming, and the reliability and validity of these tasks is in question.
	In this paper, we investigate whether crowdsourcing can be used to obtain
	reliable discourse relation annotations. We also examine the influence of
	context on the reliability of the data. The results of a crowdsourced
	connective insertion task showed that the method can be used to obtain reliable
	annotations: The majority of the inserted connectives converged with the
	original label. Further, the method is sensitive to the fact that multiple
	senses can often be inferred for a single relation. Regarding the presence of
	context, the results show no significant difference in distributions of
	insertions between conditions overall. However, a by-item comparison revealed
	several characteristics of segments that determine whether the presence of
	context makes a difference in annotations. The findings discussed in this paper
	can be taken as evidence that crowdsourcing can be used as a valuable method to
	obtain insights into the sense(s) of relations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>scholman-demberg:2017:LAW</bibkey>
  </paper>

  <paper id="0804">
    <title>A Code-Switching Corpus of Turkish-German Conversations</title>
    <author><first>&#214;zlem</first><last>&#199;etino&#287;lu</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34&#8211;40</pages>
    <url>http://www.aclweb.org/anthology/W17-0804</url>
    <abstract>We present a code-switching corpus of Turkish-German that is collected by
	recording 
	conversations of bilinguals. The recordings are then transcribed in two layers
	following speech and orthography conventions, and annotated with sentence
	boundaries and intersentential, intrasentential, and intra-word switch points. 
	The total amount of data is 5 hours of speech which corresponds to 3614
	sentences. 
	The corpus aims at serving as a resource for speech or text analysis, as well
	as a
	collection for linguistic inquiries.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ccetinouglu:2017:LAW</bibkey>
  </paper>

  <paper id="0805">
    <title>Annotating omission in statement pairs</title>
    <author><first>H&#233;ctor</first><last>Mart&#237;nez Alonso</last></author>
    <author><first>Amaury</first><last>Delamaire</last></author>
    <author><first>Beno&#238;t</first><last>Sagot</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41&#8211;45</pages>
    <url>http://www.aclweb.org/anthology/W17-0805</url>
    <abstract>We focus on the identification of omission in statement pairs. We compare three
	annotation schemes, namely two different crowdsourcing schemes and manual
	expert annotation. We show that the simplest of the two crowdsourcing
	approaches yields a better annotation quality than the more complex one. We use
	a dedicated classifier to assess whether the annotators' behavior can be
	explained by straightforward linguistic features. The classifier benefits from
	a modeling that uses lexical information beyond length and overlap measures.
	However, for our  task, we argue that expert and not crowdsourcing-based
	annotation is the best compromise between annotation cost and quality.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>martinezalonso-delamaire-sagot:2017:LAW</bibkey>
  </paper>

  <paper id="0806">
    <title>Annotating Speech, Attitude and Perception Reports</title>
    <author><first>Corien</first><last>Bary</last></author>
    <author><first>Leopold</first><last>Hess</last></author>
    <author><first>Kees</first><last>Thijs</last></author>
    <author><first>Peter</first><last>Berck</last></author>
    <author><first>Iris</first><last>Hendrickx</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46&#8211;56</pages>
    <url>http://www.aclweb.org/anthology/W17-0806</url>
    <abstract>We present REPORTS, an annotation scheme for the annotation of speech, attitude
	and perception reports. Such a scheme makes it possible to annotate the various
	text elements involved in such reports (e.g. embedding entity, complement,
	complement head) and their relations in a uniform way, which in turn
	facilitates the automatic extraction of information on, for example,
	complementation and vocabulary distribution. We also present the Ancient Greek
	corpus RAG (Thucydides’ History of the Peloponnesian War), to which we have
	applied this scheme using the annotation tool BRAT. We discuss some of the
	issues, both theoretical and practical, that we encountered, show how the
	corpus helps in answering specific questions, and conclude that REPORTS fitted
	in well with our needs.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bary-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0807">
    <title>Consistent Classification of Translation Revisions: A Case Study of English-Japanese Student Translations</title>
    <author><first>Atsushi</first><last>Fujita</last></author>
    <author><first>Kikuko</first><last>Tanabe</last></author>
    <author><first>Chiho</first><last>Toyoshima</last></author>
    <author><first>Mayuka</first><last>Yamamoto</last></author>
    <author><first>Kyo</first><last>Kageura</last></author>
    <author><first>Anthony</first><last>Hartley</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>57&#8211;66</pages>
    <url>http://www.aclweb.org/anthology/W17-0807</url>
    <abstract>Consistency is a crucial requirement in text annotation.  It is especially
	important in educational applications, as lack of consistency directly affects
	learners' motivation and learning performance.                          This paper
	presents
	a
	quality
	assessment scheme for English-to-Japanese translations produced by learner
	translators at university.  We constructed a revision typology and a decision
	tree manually through an application of the OntoNotes method, i.e., an
	iteration of assessing learners' translations and hypothesizing the conditions
	for consistent decision making, as well as re-organizing the typology. 
	Intrinsic evaluation of the created scheme confirmed its potential contribution
	to the consistent classification of identified erroneous text spans, achieving
	visibly higher Cohen's kappa values, up to 0.831, than previous work.  This
	paper also describes an application of our scheme to an English-to-Japanese
	translation exercise course for undergraduate students at a university in
	Japan.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fujita-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0808">
    <title>Representation and Interchange of Linguistic Annotation. An In-Depth, Side-by-Side Comparison of Three Designs</title>
    <author><first>Richard</first><last>Eckart de Castilho</last></author>
    <author><first>Nancy</first><last>Ide</last></author>
    <author><first>Emanuele</first><last>Lapponi</last></author>
    <author><first>Stephan</first><last>Oepen</last></author>
    <author><first>Keith</first><last>Suderman</last></author>
    <author><first>Erik</first><last>Velldal</last></author>
    <author><first>Marc</first><last>Verhagen</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>67&#8211;75</pages>
    <url>http://www.aclweb.org/anthology/W17-0808</url>
    <abstract>For decades, most self-respecting linguistic engineering initiatives have
	designed and implemented custom representations for various layers of, for
	example, morphological, syntactic, and semantic analysis. Despite occasional
	efforts at harmonization or even standardization, our field today is blessed
	with a multitude of ways of encoding and exchanging linguistic annotations of
	these types, both at the levels of ‘abstract syntax’, naming choices, and
	of
	course file formats. To a large degree, it is possible to work within and
	across design plurality by conversion, and often there may be good reasons for
	divergent design reflecting differences in use. However, it is likely that some
	abstract commonalities across choices of representation are obscured by more
	superficial differences, and conversely there is no obvious procedure to tease
	apart what actually constitute contentful vs. mere technical divergences. In
	this study, we seek to conceptually align three representations for common
	types of morpho-syntactic analysis, pinpoint what in our view constitute
	contentful differences, and reflect on the underlying principles and specific
	requirements that led to individual choices. We expect that a more in-depth
	understanding of these choices across designs may led to increased
	harmonization, or at least to more informed design of future representations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>eckartdecastilho-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0809">
    <title>TDB 1.1: Extensions on Turkish Discourse Bank</title>
    <author><first>Deniz</first><last>Zeyrek</last></author>
    <author><first>Murathan</first><last>Kurfalı</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76&#8211;81</pages>
    <url>http://www.aclweb.org/anthology/W17-0809</url>
    <abstract>This paper presents the recent developments on Turkish Discourse Bank (TDB).
	First, the resource is summarized and an evaluation is presented. Then, TDB
	1.1, i.e. enrichments on 10% of the corpus are described (namely, senses for
	explicit discourse connectives, and new annotations for three discourse
	relation types - implicit relations, entity relations and alternative
	lexicalizations). The method of annotation is explained and the data are
	evaluated.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zeyrek-kurfal:2017:LAW</bibkey>
  </paper>

  <paper id="0810">
    <title>Two Layers of Annotation for Representing Event Mentions in News Stories</title>
    <author><first>Maria Pia</first><last>di Buono</last></author>
    <author><first>Martin</first><last>Tutek</last></author>
    <author><first>Jan</first><last>&#x160;najder</last></author>
    <author><first>Goran</first><last>Glava&#x161;</last></author>
    <author><first>Bojana</first><last>Dalbelo Ba&#x161;i&#x107;</last></author>
    <author><first>Natasa</first><last>Milic-Frayling</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>82&#8211;90</pages>
    <url>http://www.aclweb.org/anthology/W17-0810</url>
    <abstract>In this paper, we describe our preliminary study on annotating event mention as
	a part of our research on high-precision news event extraction models. To this
	end, we propose a two-layer annotation scheme, designed to separately capture
	the functional and conceptual aspects of event mentions. We hypothesize that
	the precision of models can be improved by modeling and extracting separately
	the different aspects of news events, and then combining the extracted
	information by leveraging the complementarities of the models. In addition, we
	carry out a preliminary annotation using the proposed scheme and analyze the
	annotation quality in terms of inter-annotator agreement.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dibuono-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0811">
    <title>Word Similarity Datasets for Indian Languages: Annotation and Baseline Systems</title>
    <author><first>Syed Sarfaraz</first><last>Akhtar</last></author>
    <author><first>Arihant</first><last>Gupta</last></author>
    <author><first>Avijit</first><last>Vajpayee</last></author>
    <author><first>Arjit</first><last>Srivastava</last></author>
    <author><first>Manish</first><last>Shrivastava</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91&#8211;94</pages>
    <url>http://www.aclweb.org/anthology/W17-0811</url>
    <abstract>With the advent of word representations, word similarity tasks are becoming
	increasing popular as an evaluation metric for the quality of the
	representations. In this paper, we present manually annotated monolingual word
	similarity datasets of six Indian languages - Urdu, Telugu, Marathi, Punjabi,
	Tamil and Gujarati. These languages are most spoken Indian languages worldwide
	after Hindi and Bengali. For the construction of these datasets, our approach
	relies on translation and re-annotation of word similarity datasets of English.
	We also present baseline scores for word representation models using
	state-of-the-art techniques for Urdu, Telugu and Marathi by evaluating them on
	newly created word similarity datasets.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>akhtar-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0812">
    <title>The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations</title>
    <author><first>Jesse</first><last>Dunietz</last></author>
    <author><first>Lori</first><last>Levin</last></author>
    <author><first>Jaime</first><last>Carbonell</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>95&#8211;104</pages>
    <url>http://www.aclweb.org/anthology/W17-0812</url>
    <abstract>Language of cause and effect captures an essential component of the semantics
	of a text. However, causal language is also intertwined with other semantic
	relations, such as temporal precedence and correlation. This makes it difficult
	to determine when causation is the primary intended meaning. This paper
	presents BECauSE 2.0, a new version of the BECauSE corpus with exhaustively
	annotated expressions of causal language, but also seven semantic relations
	that are frequently co-present with causation. The new corpus shows high
	inter-annotator agreement, and yields insights both about the linguistic
	expressions of causation and about the process of annotating co-present
	semantic relations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dunietz-levin-carbonell:2017:LAW</bibkey>
  </paper>

  <paper id="0813">
    <title>Catching the Common Cause: Extraction and Annotation of Causal Relations and their Participants</title>
    <author><first>Ines</first><last>Rehbein</last></author>
    <author><first>Josef</first><last>Ruppenhofer</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>105&#8211;114</pages>
    <url>http://www.aclweb.org/anthology/W17-0813</url>
    <abstract>In this paper, we present a simple, yet effective method for the automatic
	identification and extraction of causal relations from text, based on a large
	English-German parallel corpus. The goal of this effort is to create a lexical
	resource for German causal relations. The resource will consist of a lexicon
	that describes constructions that trigger causality as well as the participants
	of the causal event, and will be augmented by a corpus with annotated instances
	for each entry, that can be used as training data to develop a system for
	automatic classification of causal relations. Focusing on verbs, our method
	harvested a set of 100 different lexical triggers of causality, including
	support verb constructions. At the moment, our corpus includes over 1,000
	annotated instances. The lexicon and the annotated data will be made available
	to the research community.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rehbein-ruppenhofer:2017:LAW</bibkey>
  </paper>

  <paper id="0814">
    <title>Assessing SRL Frameworks with Automatic Training Data Expansion</title>
    <author><first>Silvana</first><last>Hartmann</last></author>
    <author><first>&#201;va</first><last>M&#250;jdricza-Maydt</last></author>
    <author><first>Ilia</first><last>Kuznetsov</last></author>
    <author><first>Iryna</first><last>Gurevych</last></author>
    <author><first>Anette</first><last>Frank</last></author>
    <booktitle>Proceedings of the 11th Linguistic Annotation Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115&#8211;121</pages>
    <url>http://www.aclweb.org/anthology/W17-0814</url>
    <abstract>We present the first experiment-based study that explicitly contrasts the three
	major semantic role labeling frameworks.
	As a prerequisite, we create a dataset labeled with parallel FrameNet-,
	PropBank-, and VerbNet-style labels for German.  
	We train a state-of-the-art SRL tool for German for the different annotation
	styles and provide a comparative analysis across frameworks.
	We further explore the behavior of the frameworks with automatic training data
	generation. 
	VerbNet provides larger semantic expressivity than PropBank, and we find that
	its generalization capacity approaches PropBank in SRL training, 
	but it benefits less from training data expansion than the sparse-data affected
	FrameNet.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hartmann-EtAl:2017:LAW</bibkey>
  </paper>

  <paper id="0900">
    <title>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
    <editor>Michael Roth</editor>
    <editor>Nasrin Mostafazadeh</editor>
    <editor>Nathanael Chambers</editor>
    <editor>Annie Louis</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://aclweb.org/anthology/W/W17/W17-09</url>
    <bibtype>book</bibtype>
    <bibkey>LSDSem:2017</bibkey>
  </paper>

  <paper id="0901">
    <title>Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering</title>
    <author><first>Lilian</first><last>Wanzare</last></author>
    <author><first>Alessandra</first><last>Zarcone</last></author>
    <author><first>Stefan</first><last>Thater</last></author>
    <author><first>Manfred</first><last>Pinkal</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;11</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0901</url>
    <abstract>We present a semi-supervised clustering approach to induce script structure
	from crowdsourced descriptions of event sequences by grouping event
	descriptions into paraphrase sets (representing event types) and inducing their
	temporal order. Our approach exploits semantic and positional similarity and
	allows for flexible event order, thus overcoming the rigidity of previous
	approaches. We incorporate crowdsourced alignments as prior knowledge and show
	that exploiting a small number of alignments results in a substantial
	improvement in cluster quality over state-of-the-art models and provides an
	appropriate basis for the induction of temporal order. We also show a coverage
	study to demonstrate the scalability of our approach.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wanzare-EtAl:2017:LSDSem</bibkey>
  </paper>

  <paper id="0902">
    <title>A Consolidated Open Knowledge Representation for Multiple Texts</title>
    <author><first>Rachel</first><last>Wities</last></author>
    <author><first>Vered</first><last>Shwartz</last></author>
    <author><first>Gabriel</first><last>Stanovsky</last></author>
    <author><first>Meni</first><last>Adler</last></author>
    <author><first>Ori</first><last>Shapira</last></author>
    <author><first>Shyam</first><last>Upadhyay</last></author>
    <author><first>Dan</first><last>Roth</last></author>
    <author><first>Eugenio</first><last>Mart&#237;nez-C&#225;mara</last></author>
    <author><first>Iryna</first><last>Gurevych</last></author>
    <author><first>Ido</first><last>Dagan</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12&#8211;24</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0902</url>
    <abstract>We propose to move from Open Information Extraction (OIE) ahead to Open
	Knowledge Representation (OKR), aiming to represent information conveyed
	jointly in a set of texts in an open text- based manner. We do so by
	consolidating OIE extractions using entity and predicate coreference, while
	modeling information containment between coreferring elements via lexical
	entailment. We suggest that generating OKR structures can be a useful step in
	the NLP pipeline, to give semantic applications an easy handle on consolidated
	information across multiple texts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wities-EtAl:2017:LSDSem</bibkey>
  </paper>

  <paper id="0903">
    <title>Event-Related Features in Feedforward Neural Networks Contribute to Identifying Causal Relations in Discourse</title>
    <author><first>Edoardo Maria</first><last>Ponti</last></author>
    <author><first>Anna</first><last>Korhonen</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25&#8211;30</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0903</url>
    <abstract>Causal relations play a key role in information extraction and reasoning. Most
	of the times, their expression is ambiguous or implicit, i.e. without signals
	in the text. This makes their identification challenging. We aim to improve
	their identification by implementing a Feedforward Neural Network with a novel
	set of features for this task. In particular, these are based on the position
	of event mentions and the semantics of events and participants. The resulting
	classifier outperforms strong baselines on two datasets (the Penn Discourse
	Treebank and the CSTNews corpus) annotated with different schemes and
	containing examples in two languages, English and Portuguese. This result
	demonstrates the importance of events for identifying discourse relations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ponti-korhonen:2017:LSDSem</bibkey>
  </paper>

  <paper id="0904">
    <title>Stance Detection in Facebook Posts of a German Right-wing Party</title>
    <author><first>Manfred</first><last>Klenner</last></author>
    <author><first>Don</first><last>Tuggener</last></author>
    <author><first>Simon</first><last>Clematide</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31&#8211;40</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0904</url>
    <abstract>We argue that in order to detect stance, not only the explicit attitudes
	of the stance holder towards the targets are crucial. It is the whole narrative
	the writer drafts that counts, including the way  he hypostasizes the discourse
	referents: as benefactors or villains, as victims or beneficiaries. 
	We exemplify the ability of our system to identify targets and detect 
	the writer's stance towards them on the basis of about 100 000 Facebook posts
	of a German right-wing party.
	A reader and writer model on top of our verb-based attitude extraction 
	directly reveal stance conflicts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>klenner-tuggener-clematide:2017:LSDSem</bibkey>
  </paper>

  <paper id="0905">
    <title>Behind the Scenes of an Evolving Event Cloze Test</title>
    <author><first>Nathanael</first><last>Chambers</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41&#8211;45</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0905</url>
    <abstract>This paper analyzes the narrative event cloze test and its recent evolution.
	The test removes one event from a document's chain of events, and systems
	predict the missing event.
	Originally proposed to evaluate learned knowledge of event scenarios (e.g.,
	scripts and frames), most recent work now builds ngram-like language models
	(LM) to beat the test.
	This paper argues that the test has slowly/unknowingly been altered to
	accommodate LMs.5
	Most notably, tests are auto-generated rather than by hand, and no effort is
	taken to include core script events.
	Recent work is not clear on evaluation goals and contains contradictory
	results.
	We implement several models, and show that the test's bias to high-frequency
	events explains the inconsistencies.
	We conclude with recommendations on how to return to the test's original
	intent, and offer brief suggestions on a path forward.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chambers:2017:LSDSem</bibkey>
  </paper>

  <paper id="0906">
    <title>LSDSem 2017 Shared Task: The Story Cloze Test</title>
    <author><first>Nasrin</first><last>Mostafazadeh</last></author>
    <author><first>Michael</first><last>Roth</last></author>
    <author><first>Annie</first><last>Louis</last></author>
    <author><first>Nathanael</first><last>Chambers</last></author>
    <author><first>James</first><last>Allen</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46&#8211;51</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0906</url>
    <abstract>The LSDSem'17 shared task is the Story Cloze Test, a new evaluation for story
	understanding and script learning. This test provides a system with a
	four-sentence story and two possible endings, and the system must choose the
	correct ending to the story. Successful narrative understanding (getting closer
	to human performance of 100%) requires systems to link various levels of
	semantics to commonsense knowledge. A total of eight systems participated in
	the shared task, with a variety of approaches including.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mostafazadeh-EtAl:2017:LSDSem</bibkey>
  </paper>

  <paper id="0907">
    <title>Story Cloze Task: UW NLP System</title>
    <author><first>Roy</first><last>Schwartz</last></author>
    <author><first>Maarten</first><last>Sap</last></author>
    <author><first>Ioannis</first><last>Konstas</last></author>
    <author><first>Leila</first><last>Zilles</last></author>
    <author><first>Yejin</first><last>Choi</last></author>
    <author><first>Noah A.</first><last>Smith</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52&#8211;55</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0907</url>
    <abstract>This paper describes University of Washington NLP’s submission for the
	Linking Models of Lexical, Sentential and Discourse-level Semantics (LSDSem
	2017) shared task—the Story Cloze Task. Our system is a linear classifier
	with a variety of features, including both the scores of a neural language
	model and style features. We report 75.2% accuracy on the task. A further
	discussion of our results can be found in Schwartz et al. (2017).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schwartz-EtAl:2017:LSDSem</bibkey>
  </paper>

  <paper id="0908">
    <title>LSDSem 2017: Exploring Data Generation Methods for the Story Cloze Test</title>
    <author><first>Michael</first><last>Bugert</last></author>
    <author><first>Yevgeniy</first><last>Puzikov</last></author>
    <author><first>Andreas</first><last>R&#252;ckl&#233;</last></author>
    <author><first>Judith</first><last>Eckle-Kohler</last></author>
    <author><first>Teresa</first><last>Martin</last></author>
    <author><first>Eugenio</first><last>Mart&#237;nez-C&#225;mara</last></author>
    <author><first>Daniil</first><last>Sorokin</last></author>
    <author><first>Maxime</first><last>Peyrard</last></author>
    <author><first>Iryna</first><last>Gurevych</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56&#8211;61</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0908</url>
    <abstract>The Story Cloze test is a recent effort in providing a common test scenario for
	text understanding systems.
	As part of the LSDSem 2017 shared task, we present a system based on a deep
	learning architecture combined with a rich set of manually-crafted linguistic
	features. The system outperforms all known baselines for the task, suggesting
	that the chosen approach is promising. We additionally present two methods for
	generating further training data based on stories from the ROCStories corpus.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bugert-EtAl:2017:LSDSem</bibkey>
  </paper>

  <paper id="0909">
    <title>Sentiment Analysis and Lexical Cohesion for the Story Cloze Task</title>
    <author><first>Michael</first><last>Flor</last></author>
    <author><first>Swapna</first><last>Somasundaran</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>62&#8211;67</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0909</url>
    <abstract>We present two NLP components for the Story Cloze Task &#8211; dictionary-based
	sentiment analysis and lexical cohesion. While previous research found no
	contribution from sentiment analysis to the accuracy on this task, we
	demonstrate that sentiment is an important aspect. We describe a new approach,
	using a rule that estimates sentiment congruence in a story. Our
	sentiment-based system achieves strong results on this task. Our lexical
	cohesion system achieves accuracy comparable to previously published baseline
	results. A combination of the two systems achieves better accuracy than
	published baselines. We argue that sentiment analysis should be considered an
	integral part of narrative comprehension.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>flor-somasundaran:2017:LSDSem</bibkey>
  </paper>

  <paper id="0910">
    <title>Resource-Lean Modeling of Coherence in Commonsense Stories</title>
    <author><first>Niko</first><last>Schenk</last></author>
    <author><first>Christian</first><last>Chiarcos</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>68&#8211;73</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0910</url>
    <abstract>We present a resource-lean neural recognizer for modeling coherence in
	commonsense stories. Our lightweight system is inspired by successful attempts
	to modeling discourse relations and stands out due to its simplicity and easy
	optimization compared to prior approaches to narrative script learning. 
	We evaluate our approach in the Story Cloze Test demonstrating an absolute
	improvement in accuracy of 4.7% over state-of-the-art implementations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schenk-chiarcos:2017:LSDSem</bibkey>
  </paper>

  <paper id="0911">
    <title>An RNN-based Binary Classifier for the Story Cloze Test</title>
    <author><first>Melissa</first><last>Roemmele</last></author>
    <author><first>Sosuke</first><last>Kobayashi</last></author>
    <author><first>Naoya</first><last>Inoue</last></author>
    <author><first>Andrew</first><last>Gordon</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74&#8211;80</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0911</url>
    <abstract>The Story Cloze Test consists of choosing a sentence that best completes a
	story given two choices. In this paper we present a system that performs this
	task using a supervised binary classifier on top of a recurrent neural network
	to predict the probability that a given story ending is correct. The classifier
	is trained to distinguish correct story endings given in the training data from
	incorrect ones that we artificially generate. Our experiments evaluate
	different methods for generating these negative examples, as well as different
	embedding-based representations of the stories. Our best result obtains 67.2%
	accuracy on the test set, outperforming the existing top baseline of 58.5%.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>roemmele-EtAl:2017:LSDSem</bibkey>
  </paper>

  <paper id="0912">
    <title>IIT (BHU): System Description for LSDSem'17 Shared Task</title>
    <author><first>Pranav</first><last>Goel</last></author>
    <author><first>Anil Kumar</first><last>Singh</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>81&#8211;86</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0912</url>
    <abstract>This paper describes an ensemble system submitted as part of the LSDSem Shared
	Task 2017 - the Story Cloze Test. The main conclusion from our results is that
	an approach based on semantic similarity alone may not be enough for this task.
	We test various approaches and compare them with two ensemble systems. One is
	based on voting and the other on logistic regression based classifier. Our
	final system is able to outperform the previous state of the art for the Story
	Cloze test. Another very interesting observation is the performance of 
	sentiment based approach which works almost as well on its own as our final
	ensemble system.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>goel-singh:2017:LSDSem</bibkey>
  </paper>

  <paper id="0913">
    <title>Story Cloze Ending Selection Baselines and Data Examination</title>
    <author><first>Todor</first><last>Mihaylov</last></author>
    <author><first>Anette</first><last>Frank</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>87&#8211;92</pages>
    <url>http://aclweb.org/anthology/W/W17/W17-0913</url>
    <abstract>This paper describes two supervised baseline systems for the Story Cloze Test
	Shared Task (Mostafazadeh et al., 2016a). We first build a classifier using
	features based on word embeddings and semantic similarity computation. We
	further implement a neural LSTM system with different encoding strategies that
	try to model the relation between the story and the
	provided endings. Our experiments show that a model using representation
	features based on average word embedding vectors over the given story words and
	the candidate ending sentences words, joint with similarity features between
	the story and candidate ending representations performed better than the neural
	models. Our best model based on achieves an accuracy
	of 72.42, ranking 3rd in the official evaluation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mihaylov-frank:2017:LSDSem</bibkey>
  </paper>

  <paper id="1000">
    <title>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</title>
    <editor>George Giannakopoulos</editor>
    <editor>Elena Lloret</editor>
    <editor>John M. Conroy</editor>
    <editor>Josef Steinberger</editor>
    <editor>Marina Litvak</editor>
    <editor>Peter Rankel</editor>
    <editor>Benoit Favre</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-10</url>
    <bibtype>book</bibtype>
    <bibkey>MultiLing2017:2017</bibkey>
  </paper>

  <paper id="1001">
    <title>MultiLing 2017 Overview</title>
    <author><first>George</first><last>Giannakopoulos</last></author>
    <author><first>John</first><last>Conroy</last></author>
    <author><first>Jeff</first><last>Kubina</last></author>
    <author><first>Peter A.</first><last>Rankel</last></author>
    <author><first>Elena</first><last>Lloret</last></author>
    <author><first>Josef</first><last>Steinberger</last></author>
    <author><first>Marina</first><last>Litvak</last></author>
    <author><first>Benoit</first><last>Favre</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;6</pages>
    <url>http://www.aclweb.org/anthology/W17-1001</url>
    <abstract>In this brief report we present an overview of the MultiLing 2017 effort and
	workshop, as implemented within EACL 2017.
	MultiLing is a community-driven initiative that pushes the state-of-the-art in
	Automatic Summarization by providing data sets and fostering further research
	and development of summarization systems.
	This year the scope of the workshop was widened, bringing together researchers
	that work on summarization across sources, languages and genres. We summarize
	the main tasks planned and implemented this year, the contributions received,
	and we also provide insights on next steps.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>giannakopoulos-EtAl:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1002">
    <title>Decoupling Encoder and Decoder Networks for Abstractive Document Summarization</title>
    <author><first>Ying</first><last>Xu</last></author>
    <author><first>Jey Han</first><last>Lau</last></author>
    <author><first>Timothy</first><last>Baldwin</last></author>
    <author><first>Trevor</first><last>Cohn</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>7&#8211;11</pages>
    <url>http://www.aclweb.org/anthology/W17-1002</url>
    <abstract>Abstractive document summarization seeks to automatically generate a summary
	for a document, based on some abstract &#x201d;understanding&#x201d; of the original
	document. State-of-the-art techniques traditionally use
	attentive encoder&#8211;decoder architectures.  However, due to the large number of
	parameters in these models, they require large training datasets and long
	training times. In this paper, we propose decoupling the encoder and decoder
	networks, and training them separately.  We encode documents using an
	unsupervised document encoder, and then feed the document vector to a recurrent
	neural network decoder. With this decoupled architecture, we decrease the
	number of parameters in the decoder substantially, and shorten its training
	time.  Experiments show that the decoupled model achieves comparable
	performance with state-of-the-art models for in-domain documents, but less well
	for out-of-domain documents.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>xu-EtAl:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1003">
    <title>Centroid-based Text Summarization through Compositionality of Word Embeddings</title>
    <author><first>Gaetano</first><last>Rossiello</last></author>
    <author><first>Pierpaolo</first><last>Basile</last></author>
    <author><first>Giovanni</first><last>Semeraro</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12&#8211;21</pages>
    <url>http://www.aclweb.org/anthology/W17-1003</url>
    <abstract>The textual similarity is a crucial aspect for many extractive text
	summarization methods. A bag-of-words representation does not allow to grasp
	the semantic relationships between concepts when comparing strongly related
	sentences with no words in common. To overcome this issue, in this paper we
	propose a centroid-based method for text summarization that exploits the
	compositional capabilities of word embeddings. The evaluations on
	multi-document and multilingual datasets prove the effectiveness of the
	continuous vector representation of words compared to the bag-of-words model.
	Despite its simplicity, our method achieves good performance even in comparison
	to more complex deep learning models. Our method is unsupervised and it can be
	adopted in other summarization tasks.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rossiello-basile-semeraro:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1004">
    <title>Query-based summarization using MDL principle</title>
    <author><first>Marina</first><last>Litvak</last></author>
    <author><first>Natalia</first><last>Vanetik</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22&#8211;31</pages>
    <url>http://www.aclweb.org/anthology/W17-1004</url>
    <abstract>Query-based text summarization is aimed at extracting essential information
	that answers the query from original text. The answer is presented  
	in a minimal, often predefined, number of words. In this paper we introduce a
	new unsupervised approach for query-based extractive summarization, based on
	the minimum description length (MDL) principle that employs Krimp compression
	algorithm (Vreeken et al., 2011). The key idea of our approach is to select
	frequent word sets related to a given query that compress document sentences
	better and therefore describe the document better.
	A summary is extracted by selecting sentences that best cover query-related
	frequent word sets.
	The approach is evaluated based on the DUC 2005 and DUC 2006 datasets which are
	specifically designed for query-based summarization (DUC, 2005 2006). It
	competes with the best results.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>litvak-vanetik:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1005">
    <title>Word Embedding and Topic Modeling Enhanced Multiple Features for Content Linking and Argument / Sentiment Labeling in Online Forums</title>
    <author><first>Lei</first><last>Li</last></author>
    <author><first>Liyuan</first><last>Mao</last></author>
    <author><first>Moye</first><last>Chen</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32&#8211;36</pages>
    <url>http://www.aclweb.org/anthology/W17-1005</url>
    <abstract>Multiple grammatical and semantic features are adopted in content linking and
	argument/sentiment labeling for online forums in this paper. There are mainly
	two different methods for content linking. First, we utilize the deep feature
	obtained from Word Embedding Model in deep learning and compute sentence
	similarity. Second, we use multiple traditional features to locate candidate
	linking sentences, and then adopt a voting method to obtain the final result.
	LDA topic modeling is used to mine latent semantic feature and K-means
	clustering is implemented for argument labeling, while features from sentiment
	dictionaries and rule-based sentiment analysis are integrated for sentiment
	labeling. Experimental results have shown that our methods are valid.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>li-mao-chen:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1006">
    <title>Ultra-Concise Multi-genre Summarisation of Web2.0: towards Intelligent Content Generation</title>
    <author><first>Elena</first><last>Lloret</last></author>
    <author><first>Ester</first><last>Boldrini</last></author>
    <author><first>Patricio</first><last>Martinez-Barco</last></author>
    <author><first>Manuel</first><last>Palomar</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37&#8211;46</pages>
    <url>http://www.aclweb.org/anthology/W17-1006</url>
    <abstract>The electronic Word of Mouth has become the most powerful communication channel
	thanks to the wide usage of the Social Media. Our research proposes an approach
	towards the production of automatic ultra-concise summaries from multiple Web
	2.0
	sources. We exploit user-generated content from reviews and microblogs in dif-
	ferent domains, and compile and analyse four types of ultra-concise summaries:
	a)positive information, b) negative information; c) both or d) objective
	information. The appropriateness and usefulness of our model is demonstrated by
	its successful results and great potential in real-life applications, thus
	meaning a relevant advancement of the state-of-the-art approaches.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lloret-EtAl:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1007">
    <title>Machine Learning Approach to Evaluate MultiLingual Summaries</title>
    <author><first>Samira</first><last>Ellouze</last></author>
    <author><first>Maher</first><last>Jaoua</last></author>
    <author><first>Lamia</first><last>Hadrich Belguith</last></author>
    <booktitle>Proceedings of the MultiLing 2017 Workshop on Summarization and Summary Evaluation Across Source Types and Genres</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47&#8211;54</pages>
    <url>http://www.aclweb.org/anthology/W17-1007</url>
    <abstract>The present paper introduces a new MultiLing text summary evaluation method.
	This method relies on machine learning approach which operates by combining
	multiple features to build models that predict the human score (overall
	responsiveness) of a new summary. We have tried several single and &#x201c;ensemble
	learning&#x201d; classifiers to build the best model. We have experimented our
	method
	in summary level evaluation where we evaluate each text summary separately. The
	correlation between built models and human score is better than the correlation
	between baselines and manual score.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ellouze-jaoua-hadrichbelguith:2017:MultiLing2017</bibkey>
  </paper>

  <paper id="1100">
    <title>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</title>
    <editor>Lun-Wei Ku</editor>
    <editor>Cheng-Te Li</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-11</url>
    <bibtype>book</bibtype>
    <bibkey>SocialNLP2017:2017</bibkey>
  </paper>

  <paper id="1101">
    <title>A Survey on Hate Speech Detection using Natural Language Processing</title>
    <author><first>Anna</first><last>Schmidt</last></author>
    <author><first>Michael</first><last>Wiegand</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;10</pages>
    <url>http://www.aclweb.org/anthology/W17-1101</url>
    <abstract>This paper presents a survey on hate speech detection. Given the steadily
	growing body of social media content, the amount of online hate speech is also
	increasing. Due to the massive scale of the web, methods that automatically
	detect hate speech are required. Our survey describes key areas that have been
	explored to automatically recognize these types of utterances using natural
	language processing. We also discuss limits of those approaches.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schmidt-wiegand:2017:SocialNLP2017</bibkey>
  </paper>

  <paper id="1102">
    <title>Facebook sentiment: Reactions and Emojis</title>
    <author><first>Ye</first><last>Tian</last></author>
    <author><first>Thiago</first><last>Galery</last></author>
    <author><first>Giulio</first><last>Dulcinati</last></author>
    <author><first>Emilia</first><last>Molimpakis</last></author>
    <author><first>Chao</first><last>Sun</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11&#8211;16</pages>
    <url>http://www.aclweb.org/anthology/W17-1102</url>
    <abstract>Emojis are used frequently in social media. A widely assumed view is that
	emojis express the emotional state of the user, which has led to research
	focusing on the expressiveness of emojis independent from the linguistic
	context. We argue that emojis and the linguistic texts can modify the meaning
	of each other. The overall communicated meaning is not a simple sum of the two
	channels. 
	In order to study the meaning interplay, we need data indicating the overall
	sentiment of the entire message as well as the sentiment of the emojis
	stand-alone. We propose that Facebook Reactions are a good data source for such
	a purpose. FB reactions (e.g. &#x201c;Love&#x201d; and &#x201c;Angry&#x201d;) indicate the readers'
	overall sentiment, against which we can investigate the types of emojis used
	the comments under different reaction profiles. We present a data set of 21,000
	FB posts (57 million reactions and 8 million comments) from public media pages
	across four countries.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tian-EtAl:2017:SocialNLP2017</bibkey>
  </paper>

  <paper id="1103">
    <title>Potential and Limitations of Cross-Domain Sentiment Classification</title>
    <author><first>Jan Milan</first><last>Deriu</last></author>
    <author><first>Martin</first><last>Weilenmann</last></author>
    <author><first>Dirk</first><last>Von Gruenigen</last></author>
    <author><first>Mark</first><last>Cieliebak</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17&#8211;24</pages>
    <url>http://www.aclweb.org/anthology/W17-1103</url>
    <abstract>In this paper we investigate the cross-domain performance of a current
	state-of-the-art sentiment analysis systems. For this purpose we train a
	convolutional neural network (CNN) on data from different domains and evaluate
	its performance on other domains. Furthermore, we evaluate the usefulness of
	combining a large amount of different smaller annotated corpora to a large
	corpus. Our results show that more sophisticated approaches are required to
	train a system that works equally well on various domains.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>deriu-EtAl:2017:SocialNLP2017</bibkey>
  </paper>

  <paper id="1104">
    <title>Aligning Entity Names with Online Aliases on Twitter</title>
    <author><first>Kevin</first><last>McKelvey</last></author>
    <author><first>Peter</first><last>Goutzounis</last></author>
    <author><first>Stephen</first><last>da Cruz</last></author>
    <author><first>Nathanael</first><last>Chambers</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25&#8211;35</pages>
    <url>http://www.aclweb.org/anthology/W17-1104</url>
    <abstract>This paper presents new models that automatically align online aliases with
	their real entity names. Many research applications rely on identifying entity
	names in text, but people often refer to entities with unexpected nicknames and
	aliases. For example, The King and King James are aliases for Lebron James, a
	professional basketball player. Recent work on entity linking attempts to
	resolve mentions to knowledge base entries, like a wikipedia page, but linking
	is unfortunately limited to well-known entities with pre-built pages. This
	paper asks a more basic question: can aliases be aligned without background
	knowledge of the entity? Further, can the semantics surrounding alias mentions
	be used to inform alignments? We describe statistical models that make
	decisions based on the lexicographic properties of the aliases with their
	semantic context in a large corpus of tweets. We experiment on a database of
	Twitter users and their usernames, and present the first human evaluation for
	this task. Alignment accuracy approaches human performance at 81%, and we show
	that while lexicographic features are most important, the semantic context of
	an alias further improves classification accuracy.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mckelvey-EtAl:2017:SocialNLP2017</bibkey>
  </paper>

  <paper id="1105">
    <title>Character-based Neural Embeddings for Tweet Clustering</title>
    <author><first>Svitlana</first><last>Vakulenko</last></author>
    <author><first>Lyndon</first><last>Nixon</last></author>
    <author><first>Mihai</first><last>Lupu</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36&#8211;44</pages>
    <url>http://www.aclweb.org/anthology/W17-1105</url>
    <abstract>In this paper we show how the performance of tweet clustering can be improved
	by leveraging character-based neural networks. The proposed approach overcomes
	the limitations related to the vocabulary explosion in the word-based models
	and allows for the seamless processing of the multilingual content. Our
	evaluation results and code are available on-line:
	https://github.com/vendi12/tweet2vec_clustering.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vakulenko-nixon-lupu:2017:SocialNLP2017</bibkey>
  </paper>

  <paper id="1106">
    <title>A Twitter Corpus and Benchmark Resources for German Sentiment Analysis</title>
    <author><first>Mark</first><last>Cieliebak</last></author>
    <author><first>Jan Milan</first><last>Deriu</last></author>
    <author><first>Dominic</first><last>Egger</last></author>
    <author><first>Fatih</first><last>Uzdilli</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45&#8211;51</pages>
    <url>http://www.aclweb.org/anthology/W17-1106</url>
    <abstract>In this paper we present SB10k, a new corpus for sentiment analysis with
	approx. 10,000 German tweets. 
	We use this new corpus and two existing corpora to provide state-of-the-art
	benchmarks for sentiment analysis in German: we implemented a CNN (based on the
	winning system of SemEval-2016) and a feature-based SVM and compare their
	performance on all three corpora. 
	For the CNN, we also created German word embeddings trained on 300M tweets.
	These word embeddings were then optimized for sentiment analysis using
	distant-supervised
	learning. 
	The new corpus, the German word embeddings (plain and optimized), and 
	source code to re-run the benchmarks are publicly available.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cieliebak-EtAl:2017:SocialNLP2017</bibkey>
  </paper>

  <paper id="1200">
    <title>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</title>
    <editor>Preslav Nakov</editor>
    <editor>Marcos Zampieri</editor>
    <editor>Nikola Ljube&#x161;i&#x107;</editor>
    <editor>J&#246;rg Tiedemann</editor>
    <editor>Shevin Malmasi</editor>
    <editor>Ahmed Ali</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-12</url>
    <bibtype>book</bibtype>
    <bibkey>VarDial:2017</bibkey>
  </paper>

  <paper id="1201">
    <title>Findings of the VarDial Evaluation Campaign 2017</title>
    <author><first>Marcos</first><last>Zampieri</last></author>
    <author><first>Shervin</first><last>Malmasi</last></author>
    <author><first>Nikola</first><last>Ljube&#x161;i&#x107;</last></author>
    <author><first>Preslav</first><last>Nakov</last></author>
    <author><first>Ahmed</first><last>Ali</last></author>
    <author><first>J&#246;rg</first><last>Tiedemann</last></author>
    <author><first>Yves</first><last>Scherrer</last></author>
    <author><first>No&#235;mi</first><last>Aepli</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;15</pages>
    <url>http://www.aclweb.org/anthology/W17-1201</url>
    <abstract>We present the results of the VarDial Evaluation Campaign on Natural Language
	Processing (NLP) for Similar Languages, Varieties and Dialects, which we
	organized as part of the fourth edition of the VarDial workshop at EACL'2017.
	This year, we included four shared tasks: Discriminating between Similar
	Languages (DSL), Arabic Dialect Identification (ADI), German Dialect
	Identification (GDI), and Cross-lingual Dependency Parsing (CLP). A total of 19
	teams submitted runs across the four tasks, and 15 of them wrote system
	description papers.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zampieri-EtAl:2017:VarDial</bibkey>
  </paper>

  <paper id="1202">
    <title>Dialectometric analysis of language variation in Twitter</title>
    <author><first>Gonzalo</first><last>Donoso</last></author>
    <author><first>David</first><last>Sanchez</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16&#8211;25</pages>
    <url>http://www.aclweb.org/anthology/W17-1202</url>
    <abstract>In the last few years, microblogging platforms such as Twitter have given rise
	to a deluge of textual data that can be used for the analysis of informal
	communication between millions of individuals. In this work, we propose an
	information-theoretic approach to geographic language variation using a corpus
	based on Twitter. We test our models with tens of concepts and their associated
	keywords detected in Spanish tweets geolocated in Spain. We employ
	dialectometric measures (cosine similarity and Jensen-Shannon divergence) to
	quantify the linguistic distance on the lexical level between cells created in
	a uniform grid over the map. This can be done for a single concept or in the
	general case taking into account an average of the considered variants. The
	latter permits an analysis of the dialects that naturally emerge from the data.
	Interestingly, our results reveal the existence of two dialect macrovarieties.
	The first group includes a region-specific speech spoken in small towns and
	rural areas whereas the second cluster encompasses cities that tend to use a
	more uniform variety. Since the results obtained with the two different metrics
	qualitatively agree, our work suggests that social media corpora can be
	efficiently used for dialectometric analyses.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>donoso-sanchez:2017:VarDial</bibkey>
  </paper>

  <paper id="1203">
    <title>Computational analysis of Gondi dialects</title>
    <author><first>Taraka</first><last>Rama</last></author>
    <author><first>&#199;a&#287;rı</first><last>&#199;&#246;ltekin</last></author>
    <author><first>Pavel</first><last>Sofroniev</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>26&#8211;35</pages>
    <url>http://www.aclweb.org/anthology/W17-1203</url>
    <abstract>This paper presents a computational analysis of Gondi dialects spoken in
	central India. We present a digitized data set of the dialect area, and analyze
	the data using different techniques from dialectometry, deep learning, and
	computational biology. We show that the methods largely agree with each other
	and with the earlier non-computational analyses of the language group.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rama-ccoltekin-sofroniev:2017:VarDial</bibkey>
  </paper>

  <paper id="1204">
    <title>Investigating Diatopic Variation in a Historical Corpus</title>
    <author><first>Stefanie</first><last>Dipper</last></author>
    <author><first>Sandra</first><last>Waldenberger</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>36&#8211;45</pages>
    <url>http://www.aclweb.org/anthology/W17-1204</url>
    <abstract>This paper investigates diatopic variation in a historical corpus of German.
	Based on equivalent word forms from different language areas, replacement rules
	and mappings are derived which describe the relations between these word forms.
	These rules and mappings are then interpreted as reflections of morphological,
	phonological or graphemic variation. Based on sample rules and mappings, we
	show that our approach can replicate results from historical linguistics. While
	previous studies were restricted to predefined word lists, or confined to
	single authors or texts, our approach uses a much wider range of data available
	in historical corpora.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dipper-waldenberger:2017:VarDial</bibkey>
  </paper>

  <paper id="1205">
    <title>Author Profiling at PAN: from Age and Gender Identification to Language Variety Identification (invited talk)</title>
    <author><first>Paolo</first><last>Rosso</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46</pages>
    <url>http://www.aclweb.org/anthology/W17-1205</url>
    <abstract>Author profiling is the study of how language is shared by people, a problem of
	growing importance in applications dealing with security, in order to
	understand who could be behind an anonymous threat message, and marketing,
	where companies may be interested in knowing the demographics of people that in
	online reviews liked or disliked their products. In this talk we will give an
	overview of the PAN shared tasks that
	since 2013 have been organised at CLEF and FIRE evaluation forums, mainly on
	age and gender identification in social media, although also personality
	recognition in Twitter as well as in code sources was also addressed.
	In 2017 the PAN author profiling shared task addresses jointly gender and
	language variety identification in Twitter where tweets have been annotated 
	with authors' gender and their specific variation of their native language:
	English (Australia, Canada, Great Britain, Ireland, New  Zealand, United
	States), Spanish (Argentina, Chile, Colombia, Mexico,  Peru, Spain, Venezuela),
	Portuguese (Brazil, Portugal), and Arabic  (Egypt, Gulf, Levantine, Maghrebi).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rosso:2017:VarDial</bibkey>
  </paper>

  <paper id="1206">
    <title>The similarity and Mutual Intelligibility between Amharic and Tigrigna Varieties</title>
    <author><first>Tekabe Legesse</first><last>Feleke</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47&#8211;54</pages>
    <url>http://www.aclweb.org/anthology/W17-1206</url>
    <abstract>The present study has examined the similarity and the mutual intelligibility
	between Amharic and Tigrigna using three tools namely Levenshtein distance,
	intelligibility test  and questionnaires. The study has shown that both
	Tigrigna varieties have almost equal phonetic and lexical distances from
	Amharic. The study also indicated that Amharic speakers understand less than
	50% of the two varieties. Furthermore, the study showed that Amharic speakers
	are more positive about the Ethiopian Tigrigna variety than the Eritrean
	Variety. However, their attitude towards the two varieties does not have an
	impact on their intelligibility. The Amharic speakers’ familiarity to the
	Tigrigna varieties is largely dependent on the genealogical relation between
	Amharic and the two Tigrigna varieties.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>feleke:2017:VarDial</bibkey>
  </paper>

  <paper id="1207">
    <title>Why Catalan-Spanish Neural Machine Translation? Analysis, comparison and combination with standard Rule and Phrase-based technologies</title>
    <author><first>Marta R.</first><last>Costa-juss&#224;</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55&#8211;62</pages>
    <url>http://www.aclweb.org/anthology/W17-1207</url>
    <abstract>Catalan and Spanish are two related languages given that both derive from
	Latin.
	They share similarities in several linguistic levels including morphology,
	syntax and semantics. This makes them particularly interesting for the MT task.
	Given the recent appearance and popularity of neural MT, this paper analyzes
	the
	performance of this new approach compared to the well-established rule-based
	and phrase-based MT systems.
	Experiments are reported on a large database of 180 million words. Results,
	in terms of standard automatic measures, show that neural MT clearly
	outperforms
	the rule-based and phrase-based MT system on in-domain test set, but it is
	worst in the out-of-domain test set. A naive system combination specially works
	for the latter.
	In-domain manual analysis shows that neural MT tends to improve both adequacy
	and fluency, for example, by being able to generate more natural translations
	instead of literal ones, choosing to the adequate target word when the source
	word has several translations and improving gender agreement. However,
	out-of-domain manual analysis shows how neural MT is more affected by unknown
	words or contexts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>costajussa:2017:VarDial</bibkey>
  </paper>

  <paper id="1208">
    <title>Kurdish Interdialect Machine Translation</title>
    <author><first>Hossein</first><last>Hassani</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>63&#8211;72</pages>
    <url>http://www.aclweb.org/anthology/W17-1208</url>
    <abstract>This research suggests a method for machine translation among two Kurdish
	dialects. We chose the two widely spoken dialects, Kurmanji and Sorani, which
	are considered to be mutually unintelligible. Also, despite being spoken by
	about 30 million people in different countries, Kurdish is among less-resourced
	languages. The research used bi-dialectal dictionaries and showed that the lack
	of parallel corpora is not a major obstacle in machine translation between the
	two dialects. The experiments showed that the machine translated texts are
	comprehensible to those who do not speak the dialect. The research is the first
	attempt for inter-dialect machine translation in Kurdish and particularly could
	help in making online texts in one dialect comprehensible to those who only
	speak the target dialect. The results showed that the translated texts are in
	71%  and 79% cases rated as understandable for Kurmanji and Sorani
	respectively. They are rated as slightly-understandable in 29% cases for
	Kurmanji and 21% for Sorani.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hassani:2017:VarDial</bibkey>
  </paper>

  <paper id="1209">
    <title>Twitter Language Identification Of Similar Languages And Dialects Without Ground Truth</title>
    <author><first>Jennifer</first><last>Williams</last></author>
    <author><first>Charlie</first><last>Dagli</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73&#8211;83</pages>
    <url>http://www.aclweb.org/anthology/W17-1209</url>
    <abstract>We present a new method to bootstrap filter Twitter language ID labels in our
	dataset for automatic language identification (LID). Our method combines
	geo-location, original Twitter LID labels, and Amazon Mechanical Turk to
	resolve missing and unreliable labels. We are the first to compare LID
	classification performance using the MIRA algorithm and langid.py. We show
	classifier performance on different versions of our dataset with high accuracy
	using only Twitter data, without ground truth, and very few training examples.
	We also show how Platt Scaling can be use to calibrate MIRA classifier output
	values into a probability distribution over candidate classes, making the
	output more intuitive. Our method allows for fine-grained distinctions between
	similar languages and dialects and allows us to rediscover the language
	composition of our Twitter dataset.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>williams-dagli:2017:VarDial</bibkey>
  </paper>

  <paper id="1210">
    <title>Multi-source morphosyntactic tagging for spoken Rusyn</title>
    <author><first>Yves</first><last>Scherrer</last></author>
    <author><first>Achim</first><last>Rabus</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>84&#8211;92</pages>
    <url>http://www.aclweb.org/anthology/W17-1210</url>
    <abstract>This paper deals with the development of morphosyntactic taggers for spoken
	varieties of the Slavic minority language Rusyn. As neither annotated corpora
	nor parallel corpora are electronically available for Rusyn, we propose to
	combine existing resources from the etymologically close Slavic languages
	Russian, Ukrainian, Slovak, and Polish and adapt them to Rusyn. Using MarMoT as
	tagging toolkit, we show that a tagger trained on a balanced set of the four
	source languages outperforms single language taggers by about 9%, and that
	additional automatically induced morphosyntactic lexicons lead to further
	improvements. The best observed accuracies for Rusyn are 82.4% for
	part-of-speech tagging and 75.5% for full morphological tagging.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>scherrer-rabus:2017:VarDial</bibkey>
  </paper>

  <paper id="1211">
    <title>Identifying dialects with textual and acoustic cues</title>
    <author><first>Abualsoud</first><last>Hanani</last></author>
    <author><first>Aziz</first><last>Qaroush</last></author>
    <author><first>Stephen</first><last>Taylor</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>93&#8211;101</pages>
    <url>http://www.aclweb.org/anthology/W17-1211</url>
    <abstract>We describe several systems for identifying short samples of Arabic or
	Swiss-German dialects, which were prepared for the shared task of the 2017 DSL
	Workshop.  The Arabic data comprises both text and
	acoustic files, and our best run combined both.  The Swiss-German data
	is text-only.  Coincidently, our best runs  achieved a
	accuracy of nearly 63% on both the Swiss-German and Arabic dialects tasks.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>hanani-qaroush-taylor:2017:VarDial</bibkey>
  </paper>

  <paper id="1212">
    <title>Evaluating HeLI with Non-Linear Mappings</title>
    <author><first>Tommi</first><last>Jauhiainen</last></author>
    <author><first>Krister</first><last>Lind&#233;n</last></author>
    <author><first>Heidi</first><last>Jauhiainen</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102&#8211;108</pages>
    <url>http://www.aclweb.org/anthology/W17-1212</url>
    <abstract>In this paper we describe the non-linear mappings we used with the Helsinki
	language identification method, HeLI, in the 4th edition of the Discriminating
	between Similar Languages (DSL) shared task, which was organized as part of the
	VarDial 2017 workshop. Our SUKI team participated on the closed track together
	with 10 other teams. Our system reached the 7th position in the track. We
	describe the HeLI method and the non-linear mappings in mathematical notation.
	The HeLI method uses a probabilistic model with character n-grams and
	word-based backoff. We also describe our trials using the non-linear mappings
	instead of relative frequencies and we present statistics about the back-off
	function of the HeLI method.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jauhiainen-linden-jauhiainen:2017:VarDial</bibkey>
  </paper>

  <paper id="1213">
    <title>A Perplexity-Based Method for Similar Languages Discrimination</title>
    <author><first>Pablo</first><last>Gamallo</last></author>
    <author><first>Jose Ramom</first><last>Pichel</last></author>
    <author><first>I&#241;aki</first><last>Alegria</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>109&#8211;114</pages>
    <url>http://www.aclweb.org/anthology/W17-1213</url>
    <abstract>This article describes the system submitted by the Citius_Ixa_Imaxin team to
	the VarDial 2017 (DSL and GDI tasks). The strategy underlying our system is
	based on a language distance computed by means of model perplexity. The best
	model configuration we have tested is a voting system making use of several
	$n$-grams models of both words and characters, even if word unigrams turned out
	to be a very competitive model with reasonable results in the tasks we have
	participated. An error analysis has been performed in which we identified many
	test examples with no linguistic evidences to distinguish among the variants.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gamallo-pichel-alegria:2017:VarDial</bibkey>
  </paper>

  <paper id="1214">
    <title>Improving the Character Ngram Model for the DSL Task with BM25 Weighting and Less Frequently Used Feature Sets</title>
    <author><first>Yves</first><last>Bestgen</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115&#8211;123</pages>
    <url>http://www.aclweb.org/anthology/W17-1214</url>
    <abstract>This paper describes the system developed by the Centre for English Corpus
	Linguistics (CECL) to discriminating similar languages, language varieties and
	dialects. Based on a SVM with character and POStag n-grams as features and the
	BM25 weighting scheme, it achieved 92.7% accuracy in the Discriminating
	between Similar Languages (DSL) task, ranking first among eleven systems but
	with a lead over the next three teams of only 0.2%. A simpler version of the
	system ranked second in the German Dialect Identification (GDI) task thanks to
	several ad hoc postprocessing steps. Complementary analyses carried out by a
	cross-validation procedure suggest that the BM25 weighting scheme could be
	competitive in this type of tasks, at least in comparison with the sublinear
	TF-IDF. POStag n-grams also improved the system performance.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bestgen:2017:VarDial</bibkey>
  </paper>

  <paper id="1215">
    <title>Discriminating between Similar Languages with Word-level Convolutional Neural Networks</title>
    <author><first>Marcelo</first><last>Criscuolo</last></author>
    <author><first>Sandra Maria</first><last>Aluisio</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>124&#8211;130</pages>
    <url>http://www.aclweb.org/anthology/W17-1215</url>
    <abstract>Discriminating between Similar Languages (DSL) is a challenging task addressed
	at the VarDial Workshop series. We report on our participation in the DSL
	shared task with a two-stage system. In the first stage, character n-grams are
	used to separate language groups, then specialized classifiers distinguish
	similar language varieties. We have conducted experiments with three system
	configurations and submitted one run for each. Our main approach is a
	word-level convolutional neural network (CNN) that learns task-specific vectors
	with minimal text preprocessing. We also experiment with multi-layer perceptron
	(MLP) networks and another hybrid configuration. Our best run achieved an
	accuracy of 90.76%, ranking 8th among 11 participants and getting very close to
	the system that ranked first (less than 2 points). Even though the CNN model
	could not achieve the best results, it still makes a viable approach to
	discriminating between similar languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>criscuolo-aluisio:2017:VarDial</bibkey>
  </paper>

  <paper id="1216">
    <title>Cross-lingual dependency parsing for closely related languages - Helsinki's submission to VarDial 2017</title>
    <author><first>J&#246;rg</first><last>Tiedemann</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>131&#8211;136</pages>
    <url>http://www.aclweb.org/anthology/W17-1216</url>
    <abstract>This paper describes the submission from the University of Helsinki to the
	shared task on cross-lingual dependency parsing at VarDial 2017. We present
	work on annotation projection and treebank translation that gave good results
	for all three target languages in the test set. In particular, Slovak seems to
	work well with information coming from the Czech treebank, which is in line
	with related work. The attachment scores for cross-lingual models even surpass
	the fully supervised models trained on the target language treebank. Croatian
	is the most difficult language in the test set and the improvements over the
	baseline are rather modest. Norwegian works best with information coming from
	Swedish whereas Danish contributes surprisingly little.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tiedemann:2017:VarDial</bibkey>
  </paper>

  <paper id="1217">
    <title>Discriminating between Similar Languages Using a Combination of Typed and Untyped Character N-grams and Words</title>
    <author><first>Helena</first><last>Gomez</last></author>
    <author><first>Ilia</first><last>Markov</last></author>
    <author><first>Jorge</first><last>Baptista</last></author>
    <author><first>Grigori</first><last>Sidorov</last></author>
    <author><first>David</first><last>Pinto</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>137&#8211;145</pages>
    <url>http://www.aclweb.org/anthology/W17-1217</url>
    <abstract>This paper presents the cic_ualg's system that took part in the Discriminating
	between Similar Languages (DSL) shared task, held at the VarDial 2017 Workshop.
	This year's task aims at identifying 14 languages across 6 language groups
	using a corpus of excerpts of journalistic texts. Two classification approaches
	were compared: a single-step (all languages) approach and a two-step (language
	group and then languages within the group) approach. Features exploited include
	lexical features (unigrams of words) and character n-grams. Besides traditional
	(untyped) character n-grams, we introduce typed character n-grams in the DSL
	task. Experiments were carried out with different feature representation
	methods (binary and raw term frequency), frequency threshold values, and
	machine-learning algorithms &#8211; Support Vector Machines (SVM) and Multinomial
	Naive Bayes (MNB). Our best run in the DSL task achieved 91.46% accuracy.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gomez-EtAl:2017:VarDial</bibkey>
  </paper>

  <paper id="1218">
    <title>T&#252;bingen system in VarDial 2017 shared task: experiments with language identification and cross-lingual parsing</title>
    <author><first>&#199;a&#287;rı</first><last>&#199;&#246;ltekin</last></author>
    <author><first>Taraka</first><last>Rama</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>146&#8211;155</pages>
    <url>http://www.aclweb.org/anthology/W17-1218</url>
    <abstract>This paper describes our systems and results on VarDial 2017 shared
	tasks.                    Besides three language/dialect discrimination tasks, we
	also
	participated in the cross-lingual dependency parsing (CLP) task using
	a simple methodology which we also briefly describe in this paper.
	For all the discrimination tasks, we used linear SVMs with character
	and word features.  The system achieves competitive results among
	other systems in the shared task.  We also report additional
	experiments with neural network models. The performance of neural
	network models was close but always below the corresponding SVM
	classifiers in the discrimination tasks.
	For the cross-lingual parsing task, we experimented with an approach
	based on automatically translating the source treebank to the target
	language, and training a parser on the translated treebank.  We used
	off-the-shelf tools for both translation and parsing.  Despite
	achieving better-than-baseline results, our scores in CLP tasks were
	substantially lower than the scores of the other participants.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ccoltekin-rama:2017:VarDial</bibkey>
  </paper>

  <paper id="1219">
    <title>When Sparse Traditional Models Outperform Dense Neural Networks: the Curious Case of Discriminating between Similar Languages</title>
    <author><first>Maria</first><last>Medvedeva</last></author>
    <author><first>Martin</first><last>Kroon</last></author>
    <author><first>Barbara</first><last>Plank</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>156&#8211;163</pages>
    <url>http://www.aclweb.org/anthology/W17-1219</url>
    <abstract>We present the results of our participation in the VarDial 4 shared task on
	discriminating closely related languages. Our submission includes simple
	traditional models using linear support vector machines (SVMs) and a neural
	network (NN). The main idea was to leverage language group information. We did
	so with a two-layer approach in the traditional model and a multi-task
	objective in the neural network case. Our results confirm earlier findings:
	simple traditional models outperform neural networks consistently for this
	task, at least given the amount of systems we could examine in the available
	time. Our two-layer linear SVM ranked 2nd in the shared task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>medvedeva-kroon-plank:2017:VarDial</bibkey>
  </paper>

  <paper id="1220">
    <title>German Dialect Identification in Interview Transcriptions</title>
    <author><first>Shervin</first><last>Malmasi</last></author>
    <author><first>Marcos</first><last>Zampieri</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>164&#8211;169</pages>
    <url>http://www.aclweb.org/anthology/W17-1220</url>
    <abstract>This paper presents three systems submitted to the German Dialect
	Identification (GDI) task at the VarDial Evaluation Campaign 2017. The task
	consists of training models to identify the dialect of Swiss- German speech
	transcripts. The dialects included in the GDI dataset are Basel, Bern, Lucerne,
	and Zurich. The three systems we submitted are based on: a plurality ensemble,
	a mean probability ensemble, and a meta-classifier trained on character and
	word n-grams. The best results were obtained by the meta-classifier achieving
	68.1% accuracy and 66.2% F1-score, ranking first among the 10 teams which
	participated in the GDI shared task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>malmasi-zampieri:2017:VarDial1</bibkey>
  </paper>

  <paper id="1221">
    <title>CLUZH at VarDial GDI 2017: Testing a Variety of Machine Learning Tools for the Classification of Swiss German Dialects</title>
    <author><first>Simon</first><last>Clematide</last></author>
    <author><first>Peter</first><last>Makarov</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>170&#8211;177</pages>
    <url>http://www.aclweb.org/anthology/W17-1221</url>
    <abstract>Our submissions for the GDI 2017 Shared Task are the results from three
	different types of classifiers: Naı̈ve Bayes, Conditional Random Fields
	(CRF), and Support Vector Machine (SVM). Our CRF-based run achieves a weighted
	F1 score of 65% (third rank) being beaten by the best system by 0.9%. Measured
	by classification accuracy, our ensemble run (Naı̈ve Bayes, CRF, SVM) reaches
	67% (second rank) being 1% lower than the best system. We also describe our
	experiments with Recurrent Neural Network (RNN) architectures. Since they
	performed worse than our non-neural approaches we did not include them in the
	submission.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>clematide-makarov:2017:VarDial</bibkey>
  </paper>

  <paper id="1222">
    <title>Arabic Dialect Identification Using iVectors and ASR Transcripts</title>
    <author><first>Shervin</first><last>Malmasi</last></author>
    <author><first>Marcos</first><last>Zampieri</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>178&#8211;183</pages>
    <url>http://www.aclweb.org/anthology/W17-1222</url>
    <abstract>This paper presents the systems submitted by the MAZA team to the Arabic
	Dialect Identification (ADI) shared task at the VarDial Evaluation Campaign
	2017. The goal of the task is to evaluate computational models to identify the
	dialect of Arabic utterances using both audio and text transcriptions. The ADI
	shared task dataset included Modern Standard Arabic (MSA) and four Arabic
	dialects: Egyptian, Gulf, Levantine, and North-African. The three systems
	submitted by MAZA are based on combinations of multiple machine learning
	classifiers arranged as (1) voting ensemble; (2) mean probability ensemble; (3)
	meta-classifier. The best results were obtained by the meta-classifier
	achieving 71.7% accuracy, ranking second among the six teams which participated
	in the ADI shared task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>malmasi-zampieri:2017:VarDial2</bibkey>
  </paper>

  <paper id="1223">
    <title>Discriminating between Similar Languages using Weighted Subword Features</title>
    <author><first>Adrien</first><last>Barbaresi</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184&#8211;189</pages>
    <url>http://www.aclweb.org/anthology/W17-1223</url>
    <abstract>The present contribution revolves around a contrastive subword n-gram model
	which has been tested in the Discriminating between Similar Languages shared
	task. I present and discuss the method used in this 14-way language
	identification task comprising varieties of 6 main language groups. It features
	the following characteristics: (1) the preprocessing and conversion of a
	collection of documents to sparse features; (2) weighted character n-gram
	profiles; (3) a multinomial Bayesian classifier. Meaningful bag-of-n-grams
	features can be used as a system in a straightforward way, my approach
	outperforms most of the systems used in the DSL shared task (3rd rank).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>barbaresi:2017:VarDial</bibkey>
  </paper>

  <paper id="1224">
    <title>Exploring Lexical and Syntactic Features for Language Variety Identification</title>
    <author><first>Chris</first><last>van der Lee</last></author>
    <author><first>Antal</first><last>van den Bosch</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>190&#8211;199</pages>
    <url>http://www.aclweb.org/anthology/W17-1224</url>
    <abstract>We present a method to discriminate between texts written in either the
	Netherlandic or the Flemish variant of the Dutch language. The method draws on
	a feature bundle representing text statistics, syntactic features, and word
	$n$-grams. Text statistics include average word length and sentence length,
	while syntactic features include ratios of function words and part-of-speech
	$n$-grams.
	        The effectiveness of the classifier was measured by classifying Dutch
	subtitles developed for either Dutch or Flemish television. Several machine
	learning algorithms were compared as well as feature combination methods in
	order to find the optimal generalization performance. A machine-learning meta
	classifier based on AdaBoost attained the best F-score of 0.92.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vanderlee-vandenbosch:2017:VarDial</bibkey>
  </paper>

  <paper id="1225">
    <title>Learning to Identify Arabic and German Dialects using Multiple Kernels</title>
    <author><first>Radu Tudor</first><last>Ionescu</last></author>
    <author><first>Andrei</first><last>Butnaru</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200&#8211;209</pages>
    <url>http://www.aclweb.org/anthology/W17-1225</url>
    <abstract>We present a machine learning approach for the Arabic Dialect Identification
	(ADI) and the German Dialect Identification (GDI) Closed Shared Tasks of the
	DSL 2017 Challenge. The proposed approach combines several kernels using
	multiple kernel learning. While most of our kernels are based on character
	p-grams (also known as n-grams) extracted from speech transcripts, we also use
	a kernel based on i-vectors, a low-dimensional representation of audio
	recordings, provided only for the Arabic data. In the learning stage, we
	independently employ Kernel Discriminant Analysis (KDA) and Kernel Ridge
	Regression (KRR). Our approach is shallow and simple, but the empirical results
	obtained in the shared tasks prove that it achieves very good results. Indeed,
	we ranked on the first place in the ADI Shared Task with a weighted F1 score of
	76.32% (4.62% above the second place) and on the fifth place in the GDI Shared
	Task with a weighted F1 score of 63.67% (2.57% below the first place).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ionescu-butnaru:2017:VarDial</bibkey>
  </paper>

  <paper id="1226">
    <title>Slavic Forest, Norwegian Wood</title>
    <author><first>Rudolf</first><last>Rosa</last></author>
    <author><first>Daniel</first><last>Zeman</last></author>
    <author><first>David</first><last>Mare&#x10D;ek</last></author>
    <author><first>Zdeněk</first><last>&#x17D;abokrtsk&#253;</last></author>
    <booktitle>Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>210&#8211;219</pages>
    <url>http://www.aclweb.org/anthology/W17-1226</url>
    <abstract>We once had a corp,
	or should we say, it once had us
	They showed us its tags,
	isn't it great, unified tags
	They asked us to parse
	and they told us to use everything
	So we looked around
	and we noticed there was near nothing
	We took other langs,
	bitext aligned: words one-to-one
	We played for two weeks,
	and then they said, here is the test
	The parser kept training till morning,
	just until deadline
	So we had to wait and hope what we get
	would be just fine
	And, when we awoke,
	the results were done, we saw we'd won
	So, we wrote this paper,
	isn't it good, Norwegian wood.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rosa-EtAl:2017:VarDial</bibkey>
  </paper>

  <paper id="1300">
    <title>Proceedings of the Third Arabic Natural Language Processing Workshop</title>
    <editor>Nizar Habash</editor>
    <editor>Mona Diab</editor>
    <editor>Kareem Darwish</editor>
    <editor>Wassim El-Hajj</editor>
    <editor>Hend Al-Khalifa</editor>
    <editor>Houda Bouamor</editor>
    <editor>Nadi Tomeh</editor>
    <editor>Mahmoud El-Haj</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-13</url>
    <bibtype>book</bibtype>
    <bibkey>W17-13:2017</bibkey>
  </paper>

  <paper id="1301">
    <title>Identification of Languages in Algerian Arabic Multilingual Documents</title>
    <author><first>Wafia</first><last>Adouane</last></author>
    <author><first>Simon</first><last>Dobnik</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;8</pages>
    <url>http://www.aclweb.org/anthology/W17-1301</url>
    <abstract>This paper presents a language identification system designed to detect the
	language of each word, in its context, in a multilingual documents as generated
	in social media by bilingual/multilingual communities, in our case speakers of
	Algerian Arabic. We frame the task as a sequence tagging problem and use
	supervised machine learning with standard methods like HMM and Ngram
	classification tagging. We also experiment with a lexicon-based method.
	Combining all the methods in a fall-back mechanism and introducing some
	linguistic rules, to deal with unseen tokens and ambiguous words, gives an
	overall accuracy of 93.14%. Finally, we introduced rules for language
	identification from sequences of recognised words.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>adouane-dobnik:2017:W17-13</bibkey>
  </paper>

  <paper id="1302">
    <title>Arabic Diacritization: Stats, Rules, and Hacks</title>
    <author><first>Kareem</first><last>Darwish</last></author>
    <author><first>Hamdy</first><last>Mubarak</last></author>
    <author><first>Ahmed</first><last>Abdelali</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>9&#8211;17</pages>
    <url>http://www.aclweb.org/anthology/W17-1302</url>
    <abstract>In this paper, we present a new and fast state-of-the-art Arabic diacritizer
	that guesses the diacritics of words and then their case endings.  We employ a
	Viterbi decoder at word-level with back-off to stem, morphological patterns,
	and transliteration and sequence labeling based diacritization of named
	entities.  For case endings, we use Support Vector Machine (SVM) based ranking
	coupled with morphological patterns and linguistic rules to properly guess case
	endings. We achieve a low word level diacritization error of 3.29% and 12.77%
	without and with case endings respectively on a new multi-genre free of
	copyright test set. We are making the diacritizer available for free for
	research purposes.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>darwish-mubarak-abdelali:2017:W17-13</bibkey>
  </paper>

  <paper id="1303">
    <title>Semantic Similarity of Arabic Sentences with Word Embeddings</title>
    <author><first>El Moatez Billah</first><last>Nagoudi</last></author>
    <author><first>Didier</first><last>Schwab</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>18&#8211;24</pages>
    <url>http://www.aclweb.org/anthology/W17-1303</url>
    <abstract>Semantic textual similarity is the basis of
	countless applications and plays an important
	role in diverse areas, such as information
	retrieval, plagiarism detection, information
	extraction and machine translation.
	This article proposes an innovative
	word embedding-based system devoted to
	calculate the semantic similarity in Arabic
	sentences. The main idea is to exploit vectors
	as word representations in a multidimensional
	space in order to capture the semantic
	and syntactic properties of words.
	IDF weighting and Part-of-Speech tagging
	are applied on the examined sentences to
	support the identification of words that are
	highly descriptive in each sentence. The
	performance of our proposed system is
	confirmed through the Pearson correlation
	between our assigned semantic similarity
	scores and human judgments.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nagoudi-schwab:2017:W17-13</bibkey>
  </paper>

  <paper id="1304">
    <title>Morphological Analysis for the Maltese Language: The challenges of a hybrid system</title>
    <author><first>Claudia</first><last>Borg</last></author>
    <author><first>Albert</first><last>Gatt</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>25&#8211;34</pages>
    <url>http://www.aclweb.org/anthology/W17-1304</url>
    <abstract>Maltese is a morphologically rich language with a hybrid morphological system
	which features both concatenative and non-concatenative processes. This paper
	analyses the impact of this hybridity on the performance of machine learning
	techniques for morphological labelling and clustering. In particular, we
	analyse a dataset of morphologically related word clusters to evaluate the
	difference in results for concatenative and non-concatenative clusters. We also
	describe research carried out in morphological labelling, with a particular
	focus on the verb category. Two evaluations were carried out, one using an
	unseen dataset, and another one using a gold standard dataset which was
	manually labelled. The gold standard dataset was split into concatenative and
	non-concatenative to analyse the difference in results between the two
	morphological systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>borg-gatt:2017:W17-13</bibkey>
  </paper>

  <paper id="1305">
    <title>A Morphological Analyzer for Gulf Arabic Verbs</title>
    <author><first>Salam</first><last>Khalifa</last></author>
    <author><first>Sara</first><last>Hassan</last></author>
    <author><first>Nizar</first><last>Habash</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>35&#8211;45</pages>
    <url>http://www.aclweb.org/anthology/W17-1305</url>
    <abstract>We present CALIMAGLF, a Gulf Arabic morphological analyzer currently covering
	over 2,600 verbal lemmas. We describe in detail the process of building the
	analyzer starting from phonetic dictionary entries to fully inflected
	orthographic paradigms and associated lexicon and orthographic variants. We
	evaluate the coverage of CALIMA-GLF against Modern Standard Arabic and Egyptian
	Arabic analyzers on part of a Gulf Arabic novel. CALIMA-GLF verb analysis token
	recall for identifying correct POS tag outperforms both the Modern Standard
	Arabic and Egyptian Arabic analyzers by over 27.4% and 16.9% absolute,
	respectively.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>khalifa-hassan-habash:2017:W17-13</bibkey>
  </paper>

  <paper id="1306">
    <title>A Neural Architecture for Dialectal Arabic Segmentation</title>
    <author><first>Younes</first><last>Samih</last></author>
    <author><first>Mohammed</first><last>Attia</last></author>
    <author><first>Mohamed</first><last>Eldesouki</last></author>
    <author><first>Ahmed</first><last>Abdelali</last></author>
    <author><first>Hamdy</first><last>Mubarak</last></author>
    <author><first>Laura</first><last>Kallmeyer</last></author>
    <author><first>Kareem</first><last>Darwish</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>46&#8211;54</pages>
    <url>http://www.aclweb.org/anthology/W17-1306</url>
    <abstract>The automated processing of Arabic Dialects is challenging due to the lack of
	spelling standards and to the scarcity of annotated data and resources in
	general. Segmentation of words into its constituent parts is an important
	processing building block. In this paper, we show how a segmenter can be
	trained using only 350 annotated tweets using neural networks without any
	normalization or use of lexical features or lexical resources. We deal with
	segmentation as a sequence labeling problem at the character level. We show
	experimentally that our model can rival state-of-the-art methods that rely on
	additional resources.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>samih-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1307">
    <title>Sentiment Analysis of Tunisian Dialects: Linguistic Ressources and Experiments</title>
    <author><first>Salima</first><last>Medhaffar</last></author>
    <author><first>Fethi</first><last>Bougares</last></author>
    <author><first>Yannick</first><last>Est&#232;ve</last></author>
    <author><first>Lamia</first><last>Hadrich-Belguith</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55&#8211;61</pages>
    <url>http://www.aclweb.org/anthology/W17-1307</url>
    <abstract>Dialectal Arabic (DA) is significantly different from the Arabic
	language taught in schools and used in written communication and formal speech
	(broadcast news, religion, politics, etc.). There are many existing researches
	in the field of Arabic language Sentiment Analysis (SA); however, they are
	generally restricted to Modern Standard Arabic (MSA) or some dialects of
	economic or political interest. In this paper we are interested in the SA of
	the Tunisian Dialect. We utilize Machine Learning techniques to determine the
	polarity of comments written in Tunisian Dialect. First, we evaluate the SA
	systems performances with models trained using freely available MSA and
	Multi-dialectal data sets. We then collect and annotate a Tunisian Dialect
	corpus of 17.000 comments from Facebook. This corpus allows us a significant
	accuracy improvement compared to the best model trained on other Arabic
	dialects or MSA data.
	We believe that this first freely available corpus will be valuable to
	researchers working in the field of Tunisian Sentiment Analysis and similar
	areas.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>medhaffar-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1308">
    <title>CAT: Credibility Analysis of Arabic Content on Twitter</title>
    <author><first>Rim</first><last>El Ballouli</last></author>
    <author><first>Wassim</first><last>El-Hajj</last></author>
    <author><first>Ahmad</first><last>Ghandour</last></author>
    <author><first>Shady</first><last>Elbassuoni</last></author>
    <author><first>Hazem</first><last>Hajj</last></author>
    <author><first>Khaled</first><last>Shaban</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>62&#8211;71</pages>
    <url>http://www.aclweb.org/anthology/W17-1308</url>
    <abstract>Data generated on Twitter has become a rich source for various data mining
	tasks. 
	Those data analysis tasks that are dependent on the tweet semantics, such as
	sentiment 
	analysis, emotion mining, and rumor detection among others, suffer considerably
	if 
	the tweet is not credible, not real, or spam. In this paper, we perform an
	extensive 
	analysis on credibility of Arabic content on Twitter. We also build a
	classification 
	model (CAT) to automatically predict the credibility of a given Arabic tweet.
	Of particular originality is the inclusion of features extracted directly 
	or indirectly from the author's profile and timeline. To train and test CAT, we
	annotated for credibility a data set of 9,000 Arabic tweets that are 
	topic independent. CAT achieved consistent improvements 
	in predicting the credibility of the tweets when compared to several baselines
	and when 
	compared to the state-of-the-art approach with an improvement of 21% in
	weighted 
	average F-measure. We also conducted experiments to highlight the importance of
	the 
	user-based features as opposed to the content-based 
	features. We conclude our work with a feature reduction 
	experiment that highlights the best indicative features of credibility.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>elballouli-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1309">
    <title>A New Error Annotation for Dyslexic texts in Arabic</title>
    <author><first>Maha</first><last>Alamri</last></author>
    <author><first>William J.</first><last>Teahan</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72&#8211;78</pages>
    <url>http://www.aclweb.org/anthology/W17-1309</url>
    <abstract>This paper aims to develop a new classification of errors made in Arabic by
	those suffering from dyslexia to be used in the annotation of the Arabic
	dyslexia corpus (BDAC). The dyslexic error classification for Arabic texts
	(DECA) comprises a list of spelling errors extracted from previous studies and
	a collection of texts written by people with dyslexia that can provide a
	framework to help analyse specific errors committed by dyslexic writers. The
	classification comprises 37 types of errors, grouped into nine categories. The
	paper also discusses building a corpus of dyslexic Arabic texts that uses the
	error annotation scheme and provides an analysis of the errors that were found
	in the texts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alamri-teahan:2017:W17-13</bibkey>
  </paper>

  <paper id="1310">
    <title>An Unsupervised Speaker Clustering Technique based on SOM and I-vectors for Speech Recognition Systems</title>
    <author><first>Hany</first><last>Ahmed</last></author>
    <author><first>Mohamed</first><last>Elaraby</last></author>
    <author><first>Abdullah</first><last>M. Mousa</last></author>
    <author><first>Mostafa</first><last>Elhosiny</last></author>
    <author><first>Sherif</first><last>Abdou</last></author>
    <author><first>Mohsen</first><last>Rashwan</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79&#8211;83</pages>
    <url>http://www.aclweb.org/anthology/W17-1310</url>
    <abstract>In this paper, we introduce an enhancement for speech recognition systems using
	an unsupervised speaker clustering technique. The proposed technique is mainly
	based on I-vectors and Self-Organizing Map Neural Network(SOM).The input to the
	proposed algorithm is a set of speech utterances. For each utterance, we
	extract 100-dimensional I-vector and then SOM is used to group the utterances
	to different speakers. In our experiments, we compared our technique with
	Normalized Cross Likelihood ratio Clustering (NCLR). Results show that the
	proposed technique reduces the speaker error rate in comparison with NCLR.
	Finally, we have experimented the effect of speaker clustering on Speaker
	Adaptive Training (SAT) in a speech recognition system implemented to test the
	performance of the proposed technique. It was noted that the proposed technique
	reduced the WER over clustering speakers with NCLR.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ahmed-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1311">
    <title>SHAKKIL: An Automatic Diacritization System for Modern Standard Arabic Texts</title>
    <author><first>Amany</first><last>Fashwan</last></author>
    <author><first>Sameh</first><last>Alansary</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>84&#8211;93</pages>
    <url>http://www.aclweb.org/anthology/W17-1311</url>
    <abstract>This paper sheds light on a system that would be able to diacritize Arabic
	texts automatically (SHAKKIL). In this system, the diacritization problem will
	be handled through two levels; morphological and syntactic processing levels.
	The adopted morphological disambiguation algorithm depends on four layers;
	Uni-morphological form layer, rule-based morphological disambiguation layer,
	statistical-based disambiguation layer and Out Of  Vocabulary (OOV) layer. The
	adopted syntactic disambiguation algorithms is concerned with detecting the
	case ending diacritics depending on a rule based approach simulating the
	shallow parsing technique. This will be achieved using an annotated corpus for
	extracting the Arabic linguistic rules, building the language models and
	testing the system output. This system is considered as a good trial of the
	interaction between rule-based approach and statistical approach, where the
	rules can help the statistics in detecting the right diacritization and vice
	versa. At this point, the morphological Word Error Rate (WER) is 4.56% while
	the morphological Diacritic Error Rate (DER) is 1.88% and the syntactic WER is
	9.36%. The best WER is 14.78% compared to the best-published results, of
	(Abandah, 2015); 11.68%, (Rashwan, et al., 2015); 12.90% and (Metwally,
	Rashwan, &#38; Atiya, 2016); 13.70%.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fashwan-alansary:2017:W17-13</bibkey>
  </paper>

  <paper id="1312">
    <title>Arabic Tweets Treebanking and Parsing: A Bootstrapping Approach</title>
    <author><first>Fahad</first><last>Albogamy</last></author>
    <author><first>Allan</first><last>Ramsay</last></author>
    <author><first>Hanady</first><last>Ahmed</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>94&#8211;99</pages>
    <url>http://www.aclweb.org/anthology/W17-1312</url>
    <abstract>In this paper, we propose using a "bootstrapping" method for constructing a
	dependency treebank of Arabic tweets. This method uses a rule-based parser to
	create a small treebank of one thousand Arabic tweets and a data-driven parser
	to create a larger treebank by using the small treebank as a seed training set.
	We are able to create a dependency treebank from unlabelled tweets without any
	manual intervention. Experiments results show that this method can improve the
	speed of training the parser and the accuracy of the resulting parsers.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>albogamy-ramsay-ahmed:2017:W17-13</bibkey>
  </paper>

  <paper id="1313">
    <title>Identifying Effective Translations for Cross-lingual Arabic-to-English User-generated Speech Search</title>
    <author><first>Ahmad</first><last>Khwileh</last></author>
    <author><first>Haithem</first><last>Afli</last></author>
    <author><first>Gareth</first><last>Jones</last></author>
    <author><first>Andy</first><last>Way</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>100&#8211;109</pages>
    <url>http://www.aclweb.org/anthology/W17-1313</url>
    <abstract>Cross Language Information Retrieval (CLIR) systems are a valuable tool to
	enable speakers of one language to search for content of interest expressed in
	a different language. A group for whom this is of particular interest is
	bilingual Arabic speakers who wish to search for English language content using
	information needs expressed in Arabic queries. A key challenge in CLIR is
	crossing the language barrier between the query and the documents. The most
	common approach to bridging this gap is automated query translation, which can
	be unreliable for vague or short queries. In this work, we examine the
	potential for improving CLIR effectiveness by predicting the translation
	effectiveness using Query Performance Prediction (QPP) techniques. We propose a
	novel QPP method to estimate the quality of translation for an Arabic-English
	Cross-lingual User-generated Speech Search (CLUGS) task. We present an
	empirical evaluation that demonstrates the quality of our method on alternative
	translation outputs extracted from an Arabic-to-English Machine Translation
	system developed for this task. Finally, we show how this framework can be
	integrated in CLUGS to find relevant translations for improved retrieval
	performance.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>khwileh-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1314">
    <title>A Characterization Study of Arabic Twitter Data with a Benchmarking for State-of-the-Art Opinion Mining Models</title>
    <author><first>Ramy</first><last>Baly</last></author>
    <author><first>Gilbert</first><last>Badaro</last></author>
    <author><first>Georges</first><last>El-Khoury</last></author>
    <author><first>Rawan</first><last>Moukalled</last></author>
    <author><first>Rita</first><last>Aoun</last></author>
    <author><first>Hazem</first><last>Hajj</last></author>
    <author><first>Wassim</first><last>El-Hajj</last></author>
    <author><first>Nizar</first><last>Habash</last></author>
    <author><first>Khaled</first><last>Shaban</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110&#8211;118</pages>
    <url>http://www.aclweb.org/anthology/W17-1314</url>
    <abstract>Opinion mining in Arabic is a challenging task given the rich morphology of
	the language. The task becomes more challenging when it is applied to Twitter
	data, which contains additional sources of noise, such as the use of
	unstandardized dialectal variations, the nonconformation to grammatical rules,
	the use of Arabizi and code-switching, and the use of non-text objects such as
	images and URLs to express opinion. In this paper, we perform an analytical
	study to observe how such linguistic phenomena
	vary across different Arab regions. This study of Arabic Twitter
	characterization aims at providing better understanding of Arabic Tweets, and
	fostering advanced research on the topic. Furthermore, we explore the
	performance of the two schools of machine learning on Arabic Twitter, namely
	the feature engineering approach and the deep learning approach. We consider
	models that have achieved state-of-the-art performance for opinion mining in
	English. Results highlight the advantages of using deep learning-based models,
	and confirm the importance of using morphological abstractions to address
	Arabic’s complex morphology.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>baly-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1315">
    <title>Robust Dictionary Lookup in Multiple Noisy Orthographies</title>
    <author><first>Lingliang</first><last>Zhang</last></author>
    <author><first>Nizar</first><last>Habash</last></author>
    <author><first>Godfried</first><last>Toussaint</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119&#8211;129</pages>
    <url>http://www.aclweb.org/anthology/W17-1315</url>
    <abstract>We present the MultiScript Phonetic Search algorithm to address the problem of
	language learners looking up unfamiliar words that they heard. We apply it to
	Arabic dictionary lookup with noisy queries done using both the Arabic and
	Roman scripts. Our algorithm is based on a computational phonetic distance
	metric that can be optionally machine learned. To benchmark our performance, we
	created the ArabScribe dataset, containing 10,000 noisy transcriptions of
	random Arabic dictionary words. Our algorithm outperforms Google Translate's
	&#x201c;did you mean" feature, as well as the Yamli smart Arabic keyboard.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>zhang-habash-toussaint:2017:W17-13</bibkey>
  </paper>

  <paper id="1316">
    <title>Arabic POS Tagging: Don't Abandon Feature Engineering Just Yet</title>
    <author><first>Kareem</first><last>Darwish</last></author>
    <author><first>Hamdy</first><last>Mubarak</last></author>
    <author><first>Ahmed</first><last>Abdelali</last></author>
    <author><first>Mohamed</first><last>Eldesouki</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>130&#8211;137</pages>
    <url>http://www.aclweb.org/anthology/W17-1316</url>
    <abstract>This paper focuses on comparing between using Support Vector Machine based
	ranking (SVM-Rank) and Bidirectional Long-Short-Term-Memory (bi-LSTM)
	neural-network based sequence labeling in building a state-of-the-art Arabic
	part-of-speech tagging system. Using SVM-Rank leads to state-of-the-art
	results, but with a fair amount of feature engineering. Using bi-LSTM,
	particularly when combined with word embeddings, may lead to competitive
	POS-tagging results by automatically deducing latent linguistic features.
	However, we show that augmenting bi-LSTM sequence labeling with some of the
	features that we used for the SVM-Rank based tagger yields to further
	improvements. We also show that gains that realized by using embeddings may not
	be additive with the gains achieved by the features. We are open-sourcing both
	the SVM-Rank and the bi-LSTM based systems for free.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>darwish-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1317">
    <title>Toward a Web-based Speech Corpus for Algerian Dialectal Arabic Varieties</title>
    <author><first>Soumia</first><last>Bougrine</last></author>
    <author><first>Aicha</first><last>Chorana</last></author>
    <author><first>Abdallah</first><last>Lakhdari</last></author>
    <author><first>Hadda</first><last>Cherroun</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>138&#8211;146</pages>
    <url>http://www.aclweb.org/anthology/W17-1317</url>
    <abstract>The success of machine learning for automatic
	speech processing has raised the
	need for large scale datasets. However,
	collecting such data is often a challenging
	task as it implies significant investment involving
	time and money cost. In this paper,
	we devise a recipe for building largescale
	Speech Corpora by harnessing Web
	resources namely YouTube, other Social
	Media, Online Radio and TV. We illustrate
	our methodology by building KALAM’DZ,
	An Arabic Spoken corpus dedicated to Algerian
	dialectal varieties. The preliminary
	version of our dataset covers all major Algerian
	dialects. In addition, we make sure
	that this material takes into account numerous
	aspects that foster its richness. In
	fact, we have targeted various speech topics.
	Some automatic and manual annotations
	are provided. They gather useful
	information related to the speakers and
	sub-dialect information at the utterance
	level. Our corpus encompasses the 8 major
	Algerian Arabic sub-dialects with 4881
	speakers and more than 104.4 hours segmented
	in utterances of at least 6 s.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bougrine-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1318">
    <title>Not All Segments are Created Equal: Syntactically Motivated Sentiment Analysis in Lexical Space</title>
    <author><first>Muhammad</first><last>Abdul-Mageed</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>147&#8211;156</pages>
    <url>http://www.aclweb.org/anthology/W17-1318</url>
    <abstract>Although there is by now a considerable amount of research on subjectivity and
	sentiment analysis on morphologically-rich languages, it is still unclear how
	lexical information can best be modeled in these languages. To bridge this gap,
	we build effective models exploiting exclusively gold- and machine-segmented
	lexical input and successfully employ syntactically motivated feature selection
	to improve classification. Our best models achieve significantly above the
	baselines, with 67.93% and 69.37% accuracies for subjectivity and sentiment
	classification respectively.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>abdulmageed:2017:W17-13</bibkey>
  </paper>

  <paper id="1319">
    <title>An enhanced automatic speech recognition system for Arabic</title>
    <author><first>Mohamed Amine</first><last>Menacer</last></author>
    <author><first>Odile</first><last>Mella</last></author>
    <author><first>Dominique</first><last>Fohr</last></author>
    <author><first>Denis</first><last>Jouvet</last></author>
    <author><first>David</first><last>Langlois</last></author>
    <author><first>Kamel</first><last>Smaili</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>157&#8211;165</pages>
    <url>http://www.aclweb.org/anthology/W17-1319</url>
    <abstract>Automatic speech recognition for Arabic
	is a very challenging task. Despite all the
	classical techniques for Automatic Speech
	Recognition (ASR), which can be efficiently applied to Arabic speech
	recognition, it is essential to take into consideration
	the language specificities to improve
	the system performance. In this article, we
	focus on Modern Standard Arabic (MSA)
	speech recognition. We introduce the challenges
	related to Arabic language, namely
	the complex morphology nature of the language
	and the absence of the short vowels
	in written text, which leads to several potential
	vowelization for each graphemes,
	which is often conflicting. We develop
	an ASR system for MSA by using Kaldi
	toolkit. Several acoustic and language
	models are trained. We obtain a Word Error
	Rate (WER) of 14.42 for the baseline
	system and 12.2 relative improvement by
	rescoring the lattice and by rewriting the
	output with the right Z hamoza above or
	below Alif.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>menacer-EtAl:2017:W17-13</bibkey>
  </paper>

  <paper id="1320">
    <title>Universal Dependencies for Arabic</title>
    <author><first>Dima</first><last>Taji</last></author>
    <author><first>Nizar</first><last>Habash</last></author>
    <author><first>Daniel</first><last>Zeman</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>166&#8211;176</pages>
    <url>http://www.aclweb.org/anthology/W17-1320</url>
    <abstract>We describe the process of creating NUDAR, a Universal Dependency treebank for
	Arabic. We present the conversion from the Penn Arabic Tree- bank to the
	Universal Dependency syntactic representation through an intermediate
	dependency representation. We discuss the challenges faced in the conversion of
	the trees, the decisions we made to solve them, and the validation of our
	conversion. We also present initial parsing results on NUDAR.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>taji-habash-zeman:2017:W17-13</bibkey>
  </paper>

  <paper id="1321">
    <title>A Layered Language Model based Hybrid Approach to Automatic Full Diacritization of Arabic</title>
    <author><first>Mohamed</first><last>Al-Badrashiny</last></author>
    <author><first>Abdelati</first><last>Hawwari</last></author>
    <author><first>Mona</first><last>Diab</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>177&#8211;184</pages>
    <url>http://www.aclweb.org/anthology/W17-1321</url>
    <abstract>In this paper we  present a system for automatic Arabic text diacritization
	using three levels of analysis granularity in a layered back off manner. We
	build and exploit diacritized language models (LM)  for each of three different
	levels of granularity: surface form, morphologically segmented into
	prefix/stem/suffix, and character level.  For each of the passes, we use
	Viterbi search to pick the most probable diacritization per word in the input.
	We start with the surface form LM, followed by the morphological level, then
	finally we leverage the character level LM. Our system outperforms all of the
	published systems evaluated against the same training and test data. It
	achieves a 10.87% WER for complete full diacritization including lexical and
	syntactic diacritization, and 3.0% WER for lexical diacritization, ignoring
	syntactic diacritization.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>albadrashiny-hawwari-diab:2017:W17-13</bibkey>
  </paper>

  <paper id="1322">
    <title>Arabic Textual Entailment with Word Embeddings</title>
    <author><first>Nada</first><last>Almarwani</last></author>
    <author><first>Mona</first><last>Diab</last></author>
    <booktitle>Proceedings of the Third Arabic Natural Language Processing Workshop</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>185&#8211;190</pages>
    <url>http://www.aclweb.org/anthology/W17-1322</url>
    <abstract>Determining the textual entailment be- tween texts is important in many NLP
	tasks, such as summarization, question answering, and information extraction
	and retrieval. Various methods have been suggested based on external knowledge
	sources; however, such resources are not always available in all languages and
	their acquisition is typically laborious and very costly. Distributional word
	representations such as word embeddings learned over large corpora have been
	shown to capture syntactic and semantic word relationships. Such models have
	contributed to improv- ing the performance of several NLP tasks. In this paper,
	we address the problem of textual entailment in Arabic. We employ both
	traditional features and distributional representations. Crucially, we do not
	de- pend on any external resources in the pro- cess. Our suggested approach
	yields state of the art performance on a standard data set, ArbTE, achieving an
	accuracy of 76.2 % compared to state of the art of 69.3 %.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>almarwani-diab:2017:W17-13</bibkey>
  </paper>

  <paper id="1400">
    <title>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</title>
    <editor>Toma&#x17E; Erjavec</editor>
    <editor>Jakub Piskorski</editor>
    <editor>Lidia Pivovarova</editor>
    <editor>Jan &#x160;najder</editor>
    <editor>Josef Steinberger</editor>
    <editor>Roman Yangarber</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-14</url>
    <bibtype>book</bibtype>
    <bibkey>BSNLP:2017</bibkey>
  </paper>

  <paper id="1401">
    <title>Toward Pan-Slavic NLP: Some Experiments with Language Adaptation</title>
    <author><first>Serge</first><last>Sharoff</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;2</pages>
    <url>http://www.aclweb.org/anthology/W17-1401</url>
    <abstract>There is great variation in the amount of NLP resources available for Slavonic
	languages. For example, the Universal Dependency treebank (Nivre et al., 2016)
	has about 2 MW of training resources for Czech, more than 1 MW for Russian,
	while only 950 words for Ukrainian and nothing for Belorussian, Bosnian or
	Macedonian. Similarly, the Autodesk Machine Translation dataset only covers
	three Slavonic languages (Czech, Polish and Russian). In this talk I will
	discuss a general approach, which can be called Language Adaptation, similarly
	to Domain Adaptation. In this approach, a model for a particular language
	processing task is built by lexical transfer of cognate words and by learning a
	new feature representation for a lesser-resourced (recipient) language starting
	from a better-resourced (donor) language. More specifically, I will demonstrate
	how language adaptation works in such training scenarios as Translation Quality
	Estimation, Part-of-Speech tagging and Named Entity Recognition.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sharoff:2017:BSNLP</bibkey>
  </paper>

  <paper id="1402">
    <title>Clustering of Russian Adjective-Noun Constructions using Word Embeddings</title>
    <author><first>Andrey</first><last>Kutuzov</last></author>
    <author><first>Elizaveta</first><last>Kuzmenko</last></author>
    <author><first>Lidia</first><last>Pivovarova</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>3&#8211;13</pages>
    <url>http://www.aclweb.org/anthology/W17-1402</url>
    <abstract>This paper presents a method of automatic construction extraction from a large
	corpus of Russian. The term `construction' here means a multi-word expression
	in which a variable can be replaced with another word from the same semantic
	class, for example, `a glass of [water/juice/milk]'. We deal with constructions
	that consist of a noun and its adjective modifier. We propose a method of
	grouping such constructions into semantic classes via 2-step clustering of word
	vectors in distributional models. We compare it with other clustering
	techniques and evaluate it against A Russian-English Collocational Dictionary
	of the Human Body that contains manually annotated groups of constructions with
	nouns meaning human body parts.
	The best performing method is used to cluster all adjective-noun bigrams in the
	Russian National Corpus. Results of this procedure are publicly available and
	can be used for building Russian construction dictionary as well as to
	accelerate theoretical studies of constructions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kutuzov-kuzmenko-pivovarova:2017:BSNLP</bibkey>
  </paper>

  <paper id="1403">
    <title>A Preliminary Study of Croatian Lexical Substitution</title>
    <author><first>Domagoj</first><last>Alagi&#x107;</last></author>
    <author><first>Jan</first><last>&#x160;najder</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>14&#8211;19</pages>
    <url>http://www.aclweb.org/anthology/W17-1403</url>
    <abstract>Lexical substitution is a task of determining a meaning-preserving replacement
	for a word in context. We report on a preliminary study of this task for the
	Croatian language on a small-scale lexical sample dataset, manually annotated
	using three different annotation schemes. We compare the annotations, analyze
	the inter-annotator agreement, and observe a number of interesting language
	specific details in the obtained lexical substitutes. Furthermore, we apply a
	recently-proposed, dependency-based lexical substitution model to our dataset. 
	The model achieves a P$@$3 score of 0.35, which indicates the difficulty of the
	task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alagic-vsnajder:2017:BSNLP</bibkey>
  </paper>

  <paper id="1404">
    <title>Projecting Multiword Expression Resources on a Polish Treebank</title>
    <author><first>Agata</first><last>Savary</last></author>
    <author><first>Jakub</first><last>Waszczuk</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>20&#8211;26</pages>
    <url>http://www.aclweb.org/anthology/W17-1404</url>
    <abstract>Multiword expressions (MWEs) are linguistic objects containing two or more
	words and showing idiosyncratic behavior at different levels. Treebanks with
	annotated MWEs enable studies of such properties, as well as training and
	evaluation of MWE-aware parsers. However, few treebanks contain full-fledged
	MWE annotations. We show how this gap can be bridged in Polish by projecting 3
	MWE resources on a constituency treebank.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>savary-waszczuk:2017:BSNLP</bibkey>
  </paper>

  <paper id="1405">
    <title>Lexicon Induction for Spoken Rusyn &#8211; Challenges and Results</title>
    <author><first>Achim</first><last>Rabus</last></author>
    <author><first>Yves</first><last>Scherrer</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27&#8211;32</pages>
    <url>http://www.aclweb.org/anthology/W17-1405</url>
    <abstract>This paper reports on challenges and results in developing NLP resources for
	spoken Rusyn. Being a Slavic minority language, Rusyn does not have any
	resources to make use of. We propose to build a morphosyntactic dictionary for
	Rusyn, combining existing resources from the etymologically close Slavic
	languages Russian, Ukrainian, Slovak, and Polish. We adapt these resources to
	Rusyn by using vowel-sensitive Levenshtein distance, hand-written
	language-specific transformation rules, and combinations of the two. Compared
	to an exact match baseline, we increase the coverage of the resulting
	morphological dictionary by up to 77.4% relative (42.9% absolute), which
	results in a tagging recall increased by 11.6% relative (9.1% absolute). Our
	research confirms and expands the results of previous studies showing the
	efficiency of using NLP resources from neighboring languages for low-resourced
	languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rabus-scherrer:2017:BSNLP</bibkey>
  </paper>

  <paper id="1406">
    <title>The Universal Dependencies Treebank for Slovenian</title>
    <author><first>Kaja</first><last>Dobrovoljc</last></author>
    <author><first>Toma&#x17E;</first><last>Erjavec</last></author>
    <author><first>Simon</first><last>Krek</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33&#8211;38</pages>
    <url>http://www.aclweb.org/anthology/W17-1406</url>
    <abstract>This paper introduces the Universal Dependencies Treebank for Slovenian. We
	overview the existing dependency treebanks for Slovenian and then detail the
	conversion of the ssj200k treebank to the framework of Universal Dependencies
	version 2. We explain the mapping of part-of-speech categories, morphosyntactic
	features, and the dependency relations, focusing on the more problematic
	language-specific issues. We conclude with a quantitative overview of the
	treebank and directions for further work.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>dobrovoljc-erjavec-krek:2017:BSNLP</bibkey>
  </paper>

  <paper id="1407">
    <title>Universal Dependencies for Serbian in Comparison with Croatian and Other Slavic Languages</title>
    <author><first>Tanja</first><last>Samard&#x17E;i&#x107;</last></author>
    <author><first>Mirjana</first><last>Starovi&#x107;</last></author>
    <author><first>&#x17D;eljko</first><last>Agi&#x107;</last></author>
    <author><first>Nikola</first><last>Ljube&#x161;i&#x107;</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>39&#8211;44</pages>
    <url>http://www.aclweb.org/anthology/W17-1407</url>
    <abstract>The paper documents the procedure of building a new Universal Dependencies
	(UDv2) treebank for Serbian starting from an existing Croatian UDv1 treebank
	and taking into account the other Slavic UD annotation guidelines. We describe
	the automatic and manual annotation procedures, discuss the annotation of
	Slavic-specific categories (case governing quantifiers, reflexive pronouns,
	question particles) and propose an approach to handling deverbal nouns in
	Slavic languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>samardvzic-EtAl:2017:BSNLP</bibkey>
  </paper>

  <paper id="1408">
    <title>Spelling Correction for Morphologically Rich Language: a Case Study of Russian</title>
    <author><first>Alexey</first><last>Sorokin</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>45&#8211;53</pages>
    <url>http://www.aclweb.org/anthology/W17-1408</url>
    <abstract>We present an algorithm for automatic correction of spelling errors on the
	sentence level, which uses noisy channel model and feature-based reranking of
	hypotheses. Our system is designed for Russian and clearly outperforms the
	winner of SpellRuEval-2016 competition. We show that language model size has
	the greatest influence on spelling correction quality. We also experiment with
	different types of features and show that morphological and semantic
	information also improves the accuracy of spellchecking.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>sorokin:2017:BSNLP</bibkey>
  </paper>

  <paper id="1409">
    <title>Debunking Sentiment Lexicons: A Case of Domain-Specific Sentiment Classification for Croatian</title>
    <author><first>Paula</first><last>Gombar</last></author>
    <author><first>Zoran</first><last>Medi&#x107;</last></author>
    <author><first>Domagoj</first><last>Alagi&#x107;</last></author>
    <author><first>Jan</first><last>&#x160;najder</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54&#8211;59</pages>
    <url>http://www.aclweb.org/anthology/W17-1409</url>
    <abstract>Sentiment lexicons are widely used as an intuitive and inexpensive way of
	tackling sentiment classification, often within a simple lexicon word-counting
	approach or as part of a  supervised model.  However, it is an open question
	whether these approaches can compete with supervised models that use only
	word-representation features.  We address this question in the context of
	domain-specific sentiment classification for Croatian. We experiment with the
	graph-based acquisition of sentiment lexicons, analyze their quality, and
	investigate how effectively they can be used in sentiment classification.  Our
	results indicate that, even with as few as 500 labeled instances, a supervised
	model substantially outperforms a word-counting model. We also observe that
	adding lexicon-based features does not significantly improve supervised
	sentiment classification.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gombar-EtAl:2017:BSNLP</bibkey>
  </paper>

  <paper id="1410">
    <title>Adapting a State-of-the-Art Tagger for South Slavic Languages to Non-Standard Text</title>
    <author><first>Nikola</first><last>Ljube&#x161;i&#x107;</last></author>
    <author><first>Toma&#x17E;</first><last>Erjavec</last></author>
    <author><first>Darja</first><last>Fi&#x161;er</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60&#8211;68</pages>
    <url>http://www.aclweb.org/anthology/W17-1410</url>
    <abstract>In this paper we present the adaptations of a state-of-the-art tagger for South
	Slavic languages to non-standard texts on the example of the Slovene language.
	We investigate the impact of introducing in-domain training data as well as
	additional supervision through external resources or tools like word clusters
	and word normalization. We remove more than half of the error of the standard
	tagger when applied to non-standard texts by training it on a combination of
	standard and non-standard training data, while enriching the data
	representation with external resources removes additional 11 percent of the
	error. The final configuration achieves tagging accuracy of 87.41% on the full
	morphosyntactic description, which is, nevertheless, still quite far from the
	accuracy of 94.27% achieved on standard text.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ljubevsic-erjavec-fivser:2017:BSNLP</bibkey>
  </paper>

  <paper id="1411">
    <title>Comparison of Short-Text Sentiment Analysis Methods for Croatian</title>
    <author><first>Leon</first><last>Rotim</last></author>
    <author><first>Jan</first><last>&#x160;najder</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>69&#8211;75</pages>
    <url>http://www.aclweb.org/anthology/W17-1411</url>
    <abstract>We focus on the task of supervised sentiment classification of short and
	informal texts in Croatian, using two simple yet effective methods: word
	embeddings and string kernels. We investigate whether word embeddings offer any
	advantage over corpus- and preprocessing-free string kernels, and how these
	compare to bag-of-words baselines. We conduct a comparison on three different
	datasets, using different preprocessing methods and kernel functions. Results
	show that, on two out of three datasets, word embeddings outperform string
	kernels, which in turn outperform word and n-gram bag-of-words baselines.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rotim-vsnajder:2017:BSNLP</bibkey>
  </paper>

  <paper id="1412">
    <title>The First Cross-Lingual Challenge on Recognition, Normalization, and Matching of Named Entities in Slavic Languages</title>
    <author><first>Jakub</first><last>Piskorski</last></author>
    <author><first>Lidia</first><last>Pivovarova</last></author>
    <author><first>Jan</first><last>&#x160;najder</last></author>
    <author><first>Josef</first><last>Steinberger</last></author>
    <author><first>Roman</first><last>Yangarber</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>76&#8211;85</pages>
    <url>http://www.aclweb.org/anthology/W17-1412</url>
    <abstract>This paper describes the outcomes of the first challenge on multilingual named
	entity recognition that aimed at recognizing mentions of named entities in web
	documents in Slavic languages, their normalization/lemmatization, and
	cross-language matching. It was organised in the context of the 6th
	Balto-Slavic Natural Language Processing Workshop, co-located with the EACL
	2017 conference. Although eleven teams signed up for the evaluation, due to the
	complexity of the task(s) and short time available for elaborating a solution,
	only two teams submitted results on time. The reported evaluation figures
	reflect the relatively higher level of complexity of named entity-related tasks
	in the context of processing texts in Slavic languages. Since the duration of
	the challenge goes beyond the date of the publication of this paper and updated
	picture of the participating systems and their corresponding performance can be
	found on the web page of the challenge.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>piskorski-EtAl:2017:BSNLP</bibkey>
  </paper>

  <paper id="1413">
    <title>Liner2 — a Generic Framework for Named Entity Recognition</title>
    <author><first>Micha&#x142;</first><last>Marci&#x144;czuk</last></author>
    <author><first>Jan</first><last>Koco&#x144;</last></author>
    <author><first>Marcin</first><last>Oleksy</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>86&#8211;91</pages>
    <url>http://www.aclweb.org/anthology/W17-1413</url>
    <abstract>In the paper we present an adaptation of Liner2 framework to solve the BSNLP
	2017 shared task on multilingual named entity recognition. The tool is tuned to
	recognize and lemmatize named entities for Polish.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marcinczuk-kocon-oleksy:2017:BSNLP</bibkey>
  </paper>

  <paper id="1414">
    <title>Language-Independent Named Entity Analysis Using Parallel Projection and Rule-Based Disambiguation</title>
    <author><first>James</first><last>Mayfield</last></author>
    <author><first>Paul</first><last>McNamee</last></author>
    <author><first>Cash</first><last>Costello</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>92&#8211;96</pages>
    <url>http://www.aclweb.org/anthology/W17-1414</url>
    <abstract>The 2017 shared task at the Balto-Slavic NLP workshop requires identifying
	coarse-grained named entities in seven languages, identifying each entity’s
	base form, and clustering name mentions across the multilingual set of
	documents. The fact that no training data is provided to systems for building
	supervised classifiers further adds to the complexity. To complete the task we
	first use publicly available parallel texts to project named entity recognition
	capability from English to each evaluation language. We ignore entirely the
	subtask of identifying non-inflected forms of names. Finally, we create
	cross-document entity identifiers by clustering named mentions using a
	procedure-based approach.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mayfield-mcnamee-costello:2017:BSNLP</bibkey>
  </paper>

  <paper id="1415">
    <title>Comparison of String Similarity Measures for Obscenity Filtering</title>
    <author><first>Ekaterina</first><last>Chernyak</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>97&#8211;101</pages>
    <url>http://www.aclweb.org/anthology/W17-1415</url>
    <abstract>In this paper we address the problem of filtering obscene lexis in Russian
	texts. We use string similarity measures to find words similar or identical to
	words from a stop list and establish both a test collection and a baseline for
	the task. Our experiments show that a novel string similarity measure based on
	the notion of an annotated suffix tree outperforms some of the other well known
	measures.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chernyak:2017:BSNLP</bibkey>
  </paper>

  <paper id="1416">
    <title>Stylometric Analysis of Parliamentary Speeches: Gender Dimension</title>
    <author><first>Justina</first><last>Mandravickaite</last></author>
    <author><first>Tomas</first><last>Krilavi&#x10D;ius</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102&#8211;107</pages>
    <url>http://www.aclweb.org/anthology/W17-1416</url>
    <abstract>Relation between gender and language has been studied by many authors, however,
	there is still some uncertainty left regarding gender influence on language
	usage in the professional environment. Often, the studied data sets are too
	small or texts of individual authors are too short in order to capture
	differences of language usage wrt gender successfully. This study draws from a
	larger corpus of speeches transcripts of the Lithuanian Parliament (1990-2013)
	to explore language differences of political debates by gender via stylometric
	analysis. Experimental set up consists of stylistic features that indicate
	lexical style and do not require external linguistic tools, namely the most
	frequent words, in combination with unsupervised machine learning algorithms.
	Results show that gender differences in the language use remain in professional
	environment not only in usage of function words, preferred linguistic
	constructions, but in the presented topics as well.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mandravickaite-krilavivcius:2017:BSNLP</bibkey>
  </paper>

  <paper id="1417">
    <title>Towards Never Ending Language Learning for Morphologically Rich Languages</title>
    <author><first>Kseniya</first><last>Buraya</last></author>
    <author><first>Lidia</first><last>Pivovarova</last></author>
    <author><first>Sergey</first><last>Budkov</last></author>
    <author><first>Andrey</first><last>Filchenkov</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>108&#8211;118</pages>
    <url>http://www.aclweb.org/anthology/W17-1417</url>
    <abstract>This work deals with ontology learning from unstructured Russian text. We
	implement one of components Never Ending Language Learner and introduce the
	algorithm extensions aimed to gather specificity of morphologicaly rich
	free-word-order language. We demonstrate that this method may be successfully
	applied to Russian data. In addition we perform several additional experiments
	comparing different settings of the training process. We demonstrate that
	utilizing of morphological features significantly improves the system precision
	while using of seed patterns helps to improve the coverage.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>buraya-EtAl:2017:BSNLP</bibkey>
  </paper>

  <paper id="1418">
    <title>Gender Profiling for Slovene Twitter communication: the Influence of Gender Marking, Content and Style</title>
    <author><first>Ben</first><last>Verhoeven</last></author>
    <author><first>Iza</first><last>&#x160;krjanec</last></author>
    <author><first>Senja</first><last>Pollak</last></author>
    <booktitle>Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>119&#8211;125</pages>
    <url>http://www.aclweb.org/anthology/W17-1418</url>
    <abstract>We present results of the first gender classification experiments on Slovene
	text to our knowledge. Inspired by the TwiSty corpus and experiments (Verhoeven
	et al., 2016), we employed the Janes corpus (Erjavec et al., 2016) and its
	gender annotations to perform gender classification experiments on Twitter text
	comparing a token-based and a lemma-based approach. We find that the
	token-based approach (92.6% accuracy), containing gender markings related to
	the author, outperforms the lemma-based approach by about 5%. Especially in the
	lemmatized version, we also observe stylistic and content-based differences in
	writing between men (e.g. more profane language, numerals and beer mentions)
	and women (e.g. more pronouns, emoticons and character flooding). Many of our
	findings corroborate previous research on other languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>verhoeven-vskrjanec-pollak:2017:BSNLP</bibkey>
  </paper>

  <paper id="1500">
    <title>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</title>
    <editor>Maciej Ogrodniczuk</editor>
    <editor>Vincent Ng</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-15</url>
    <bibtype>book</bibtype>
    <bibkey>CORBON:2017</bibkey>
  </paper>

  <paper id="1501">
    <title>Use Generalized Representations, But Do Not Forget Surface Features</title>
    <author><first>Nafise Sadat</first><last>Moosavi</last></author>
    <author><first>Michael</first><last>Strube</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;7</pages>
    <url>http://www.aclweb.org/anthology/W17-1501</url>
    <abstract>Only a year ago, all state-of-the-art coreference resolvers were using an
	extensive amount of surface features. Recently, there was a paradigm shift
	towards using word embeddings and deep neural networks, where the use of
	surface features is very limited. In this paper, we show that a simple SVM
	model with surface features outperforms more complex neural models for
	detecting anaphoric mentions. Our analysis suggests that using generalized
	representations and surface features have different strength that should be
	both taken into account for improving coreference resolution.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>moosavi-strube:2017:CORBON</bibkey>
  </paper>

  <paper id="1502">
    <title>Enriching Basque Coreference Resolution System using Semantic Knowledge sources</title>
    <author><first>Ander</first><last>Soraluze</last></author>
    <author><first>Olatz</first><last>Arregi</last></author>
    <author><first>Xabier</first><last>Arregi</last></author>
    <author><first>Arantza</first><last>D&#237;az de Ilarraza</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8&#8211;16</pages>
    <url>http://www.aclweb.org/anthology/W17-1502</url>
    <abstract>In this paper we present a Basque coreference resolution system enriched with
	semantic knowledge. An error analysis carried out revealed the deficiencies
	that the system had in resolving coreference cases in which semantic or world
	knowledge is needed. We attempt to improve the deficiencies using two semantic
	knowledge sources, specifically Wikipedia and WordNet.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>soraluze-EtAl:2017:CORBON</bibkey>
  </paper>

  <paper id="1503">
    <title>Improving Polish Mention Detection with Valency Dictionary</title>
    <author><first>Maciej</first><last>Ogrodniczuk</last></author>
    <author><first>Bart&#x142;omiej</first><last>Nito&#x144;</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>17&#8211;23</pages>
    <url>http://www.aclweb.org/anthology/W17-1503</url>
    <abstract>This paper presents results of an experiment integrating information from
	valency dictionary of Polish into a mention detection system. Two types of
	information is acquired: positions of syntactic schemata for nominal and verbal
	constructs and secondary prepositions present in schemata. The syntactic
	schemata are used to prevent (for verbal realizations) or encourage (for
	nominal groups) constructing mentions from phrases filling multiple schema
	positions, the secondary prepositions &#8211; to filter out artificial mentions
	created from their nominal components. Mention detection is evaluated against
	the manual annotation of the Polish Coreference Corpus in two settings: taking
	into account only mention heads or exact borders.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ogrodniczuk-niton:2017:CORBON</bibkey>
  </paper>

  <paper id="1504">
    <title>A Google-Proof Collection of French Winograd Schemas</title>
    <author><first>Pascal</first><last>Amsili</last></author>
    <author><first>Olga</first><last>Seminck</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24&#8211;29</pages>
    <url>http://www.aclweb.org/anthology/W17-1504</url>
    <abstract>This article presents the first collection of French Winograd Schemas. Winograd
	Schemas form anaphora resolution problems that can only be resolved with
	extensive world knowledge. For this reason the Winograd Schema Challenge has
	been proposed as an alternative to the Turing Test. A very important feature of
	Winograd Schemas is that it should be impossible to resolve them with
	statistical information about word co-occurrences: they should be Google-proof.
	We propose a measure of Google-proofness based on  Mutual Information, and
	demonstrate the method on our collection of French Winograd Schemas.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>amsili-seminck:2017:CORBON</bibkey>
  </paper>

  <paper id="1505">
    <title>Using Coreference Links to Improve Spanish-to-English Machine Translation</title>
    <author><first>Lesly</first><last>Miculicich Werlen</last></author>
    <author><first>Andrei</first><last>Popescu-Belis</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>30&#8211;40</pages>
    <url>http://www.aclweb.org/anthology/W17-1505</url>
    <abstract>In this paper, we present a proof-of-concept implementation of a
	coreference-aware decoder for document-level machine translation.  We consider
	that better translations should have coreference links that are closer to those
	in the source text, and implement this criterion in two ways.  First, we define
	a similarity measure between source and target coreference structures, by
	projecting the target ones onto the source and reusing existing coreference
	metrics.  Based on this similarity measure, we re-rank the translation
	hypotheses of a baseline system for each sentence.  Alternatively, to address
	the lack of diversity of mentions in the MT hypotheses, we focus on mention
	pairs and integrate their coreference scores with MT ones, resulting in
	post-editing decisions for mentions. The experimental results for Spanish to
	English MT on the AnCora-ES corpus show that the second approach yields a
	substantial increase in the accuracy of pronoun translation, with BLEU scores
	remaining constant.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>miculicichwerlen-popescubelis:2017:CORBON</bibkey>
  </paper>

  <paper id="1506">
    <title>Multi-source annotation projection of coreference chains: assessing strategies and testing opportunities</title>
    <author><first>Yulia</first><last>Grishina</last></author>
    <author><first>Manfred</first><last>Stede</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41&#8211;50</pages>
    <url>http://www.aclweb.org/anthology/W17-1506</url>
    <abstract>In this paper, we examine the possibility of using annotation projection from
	multiple sources for automatically obtaining coreference annotations in the
	target language. We implement a multi-source annotation projection algorithm
	and apply it on an English-German-Russian parallel corpus in order to transfer
	coreference chains from two sources to the target side. Operating in two
	settings &#8211; a low-resource and a more linguistically-informed one &#8211; we show
	that automatic coreference transfer
	could benefit from combining information from multiple languages, and assess
	the quality of both the extraction and the linking of target coreference
	mentions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grishina-stede:2017:CORBON</bibkey>
  </paper>

  <paper id="1507">
    <title>CORBON 2017 Shared Task: Projection-Based Coreference Resolution</title>
    <author><first>Yulia</first><last>Grishina</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>51&#8211;55</pages>
    <url>http://www.aclweb.org/anthology/W17-1507</url>
    <abstract>The CORBON 2017 Shared Task, organised as part of the Coreference Resolution
	Beyond OntoNotes workshop at EACL 2017, presented a new challenge for
	multilingual coreference resolution: we offer a projection-based setting in
	which one is supposed to build a coreference resolver for a new language
	exploiting little or even no knowledge of it, with our languages of interest
	being German and Russian. We additionally offer a more traditional setting,
	targeting the development of a multilingual coreference resolver without any
	restrictions on the resources and methods used. In this paper, we describe the
	task setting and provide the results of one participant who successfully
	completed the task, comparing their results to the closely related previous
	research. Analysing the task setting and the results, we discuss the major
	challenges and make suggestions on the future directions of coreference
	evaluation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>grishina:2017:CORBON</bibkey>
  </paper>

  <paper id="1508">
    <title>Projection-based Coreference Resolution Using Deep Syntax</title>
    <author><first>Michal</first><last>Nov&#225;k</last></author>
    <author><first>Anna</first><last>Nedoluzhko</last></author>
    <author><first>Zdeněk</first><last>&#x17D;abokrtsk&#253;</last></author>
    <booktitle>Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>56&#8211;64</pages>
    <url>http://www.aclweb.org/anthology/W17-1508</url>
    <abstract>The paper describes the system for coreference resolution in German and
	Russian, trained exclusively on coreference relations project
	ed through a parallel corpus from English. 
	The resolver operates on the level of deep syntax and makes use of multiple
	specialized models.
	It achieves 32 and 22 points in terms of CoNLL score for Russian and German,
	respectively.
	Analysis of the evaluation results show that the resolver for Russian is able
	to preserve 66% of the English resolver's quality in terms of CoNLL score.
	The system was submitted to the Closed track of the CORBON 2017 Shared task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>novak-nedoluzhko-vzabokrtsky:2017:CORBON</bibkey>
  </paper>

  <paper id="1600">
    <title>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</title>
    <editor>Dirk Hovy</editor>
    <editor>Shannon Spruit</editor>
    <editor>Margaret Mitchell</editor>
    <editor>Emily M. Bender</editor>
    <editor>Michael Strube</editor>
    <editor>Hanna Wallach</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-16</url>
    <bibtype>book</bibtype>
    <bibkey>EthNLP:2017</bibkey>
  </paper>

  <paper id="1601">
    <title>Gender as a Variable in Natural-Language Processing: Ethical Considerations</title>
    <author><first>Brian</first><last>Larson</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;11</pages>
    <url>http://www.aclweb.org/anthology/W17-1601</url>
    <abstract>Researchers and practitioners in natural-language processing (NLP) and related
	fields should attend to ethical principles in study design, ascription of
	categories/variables to study participants, and reporting of findings or
	results. This paper discusses theoretical and ethical frameworks for using
	gender as a variable in NLP studies and proposes four guidelines for
	researchers and practitioners. The principles outlined here should guide
	practitioners, researchers, and peer reviewers, and they may be applicable to
	other social categories, such as race, applied to human beings connected to NLP
	research.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>larson:2017:EthNLP</bibkey>
  </paper>

  <paper id="1602">
    <title>These are not the Stereotypes You are Looking For: Bias and Fairness in Authorial Gender Attribution</title>
    <author><first>Corina</first><last>Koolen</last></author>
    <author><first>Andreas</first><last>van Cranenburgh</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12&#8211;22</pages>
    <url>http://www.aclweb.org/anthology/W17-1602</url>
    <abstract>Stylometric and text categorization results show that author gender can be
	discerned in texts with relatively high accuracy. However, it is difficult to
	explain what gives rise to these results and there are many possible
	confounding factors, such as the domain, genre, and target audience of a text.
	More fundamentally, such classification efforts risk invoking stereotyping and
	essentialism. We explore this issue in two datasets of Dutch literary novels,
	using commonly used descriptive (LIWC, topic modeling) and predictive (machine
	learning) methods. Our results show the importance of controlling for variables
	in the corpus and we argue for taking care not to overgeneralize from the
	results.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koolen-vancranenburgh:2017:EthNLP</bibkey>
  </paper>

  <paper id="1603">
    <title>A Quantitative Study of Data in the NLP community</title>
    <author><first>Margot</first><last>Mieskes</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>23&#8211;29</pages>
    <url>http://www.aclweb.org/anthology/W17-1603</url>
    <abstract>We present results on a quantitative analysis of publications in the NLP domain
	on collecting, publishing and availability of research data. We find that a
	wide range of publications rely on data crawled from the web, but few give
	details on how potentially sensitive data was treated. Additionally, we find
	that while links to repositories of data are given, they often do not work even
	a short time after publication. We put together several suggestions on how to
	improve this situation based on publications from the NLP domain, but also
	other research areas.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mieskes:2017:EthNLP</bibkey>
  </paper>

  <paper id="1604">
    <title>Ethical by Design: Ethics Best Practices for Natural Language Processing</title>
    <author><first>Jochen L.</first><last>Leidner</last></author>
    <author><first>Vassilis</first><last>Plachouras</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>30&#8211;40</pages>
    <url>http://www.aclweb.org/anthology/W17-1604</url>
    <abstract>Natural language processing (NLP) systems analyze and/or generate human
	language, typically on users’ behalf. One natural and necessary question that
	needs to be addressed in this context, both in research projects and in
	production settings, is the question how ethical the work is, both regarding
	the process and its outcome.
	    Towards this end, we articulate a set of issues, propose a set of best
	practices, notably a process featuring an ethics review board, and sketch and
	how they could be meaningfully applied. Our main argument is that ethical
	outcomes ought to be achieved by design, i.e. by following a process aligned
	by ethical values. We also offer some response options for those facing ethics
	issues.
	    While a number of previous works exist that discuss ethical issues, in
	particular around big data and machine learning, to the authors’ knowledge
	this is the first account of NLP and ethics from the perspective of a
	principled process.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>leidner-plachouras:2017:EthNLP</bibkey>
  </paper>

  <paper id="1605">
    <title>Building Better Open-Source Tools to Support Fairness in Automated Scoring</title>
    <author><first>Nitin</first><last>Madnani</last></author>
    <author><first>Anastassia</first><last>Loukina</last></author>
    <author><first>Alina</first><last>von Davier</last></author>
    <author><first>Jill</first><last>Burstein</last></author>
    <author><first>Aoife</first><last>Cahill</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>41&#8211;52</pages>
    <url>http://www.aclweb.org/anthology/W17-1605</url>
    <abstract>Automated scoring of written and spoken responses is an NLP application that
	can significantly impact lives especially when deployed as part of high-stakes
	tests such as the GREėxtregistered~ and the TOEFLėxtregistered~. Ethical considerations require that
	automated scoring algorithms treat all test- takers fairly. The educational
	measurement community has done significant research on fairness in assessments
	and automated scoring systems must incorporate their recommendations. The best
	way to do that is by making available automated, non-proprietary tools to NLP
	researchers that directly incorporate these recommendations and generate the
	analyses needed to help identify and resolve biases in their scoring systems.
	In this paper, we attempt to provide such a solution.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>madnani-EtAl:2017:EthNLP</bibkey>
  </paper>

  <paper id="1606">
    <title>Gender and Dialect Bias in YouTube's Automatic Captions</title>
    <author><first>Rachael</first><last>Tatman</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53&#8211;59</pages>
    <url>http://www.aclweb.org/anthology/W17-1606</url>
    <abstract>This project evaluates the accuracy of YouTube's automatically-generated
	captions across two genders and five dialect groups. Speakers' dialect and
	gender was controlled for by using videos uploaded as part of the &#x201c;accent tag
	challenge", where speakers explicitly identify their language background. The
	results show robust differences in accuracy across both gender and dialect,
	with lower accuracy for 1) women and 2) speakers from Scotland. This finding
	builds on earlier research finding that speaker's sociolinguistic identity may
	negatively impact their ability to use automatic speech recognition, and
	demonstrates the need for sociolinguistically-stratified validation of systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tatman:2017:EthNLP</bibkey>
  </paper>

  <paper id="1607">
    <title>Integrating the Management of Personal Data Protection and Open Science with Research Ethics</title>
    <author><first>Dave</first><last>Lewis</last></author>
    <author><first>Joss</first><last>Moorkens</last></author>
    <author><first>Kaniz</first><last>Fatema</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60&#8211;65</pages>
    <url>http://www.aclweb.org/anthology/W17-1607</url>
    <abstract>We examine the impact of the EU General
	Data Protection Regulation and the push
	from research funders to provide open access
	research data on the current practices
	in Language Technology Research.
	We analyse the challenges that arise and
	the opportunities to address many of them
	through the use of existing open data practices.
	We discuss the impact of this also on
	current practice in research ethics.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>lewis-moorkens-fatema:2017:EthNLP</bibkey>
  </paper>

  <paper id="1608">
    <title>Ethical Considerations in NLP Shared Tasks</title>
    <author><first>Carla</first><last>Parra Escart&#237;n</last></author>
    <author><first>Wessel</first><last>Reijers</last></author>
    <author><first>Teresa</first><last>Lynn</last></author>
    <author><first>Joss</first><last>Moorkens</last></author>
    <author><first>Andy</first><last>Way</last></author>
    <author><first>Chao-Hong</first><last>Liu</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66&#8211;73</pages>
    <url>http://www.aclweb.org/anthology/W17-1608</url>
    <abstract>Shared tasks are increasingly common in our field, and new challenges are
	suggested at almost every conference and  workshop. However, as this has become
	an established way of pushing research forward, it is important to discuss how
	we researchers organise and participate in shared tasks, and make that
	information available to the  community to allow further research improvements.
	In this paper, we present a number of ethical issues along with other areas of
	concern that are related to the competitive nature of shared tasks. As such
	issues could potentially impact on research ethics in the Natural Language
	Processing community, we also propose the development of a framework for the
	organisation of and participation in shared tasks that can help mitigate
	against these issues arising.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>parraescartin-EtAl:2017:EthNLP</bibkey>
  </paper>

  <paper id="1609">
    <title>Social Bias in Elicited Natural Language Inferences</title>
    <author><first>Rachel</first><last>Rudinger</last></author>
    <author><first>Chandler</first><last>May</last></author>
    <author><first>Benjamin</first><last>Van Durme</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>74&#8211;79</pages>
    <url>http://www.aclweb.org/anthology/W17-1609</url>
    <abstract>We analyze the Stanford Natural Language Inference (SNLI) corpus in an
	investigation of bias and stereotyping in NLP data. The SNLI human-elicitation
	protocol makes it prone to amplifying bias and stereotypical associations,
	which we demonstrate statistically (using pointwise mutual information) and
	with qualitative examples.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rudinger-may-vandurme:2017:EthNLP</bibkey>
  </paper>

  <paper id="1610">
    <title>A Short Review of Ethical Challenges in Clinical Natural Language Processing</title>
    <author><first>Simon</first><last>Suster</last></author>
    <author><first>Stephan</first><last>Tulkens</last></author>
    <author><first>Walter</first><last>Daelemans</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>80&#8211;87</pages>
    <url>http://www.aclweb.org/anthology/W17-1610</url>
    <abstract>Clinical NLP has an immense potential in contributing to how clinical practice
	will be revolutionized by the advent of large scale processing of clinical
	records. However, this potential has remained largely untapped due to slow
	progress primarily caused by strict data access policies for researchers. In
	this paper, we discuss the concern for privacy and the measures it entails. We
	also suggest sources of less sensitive data. Finally, we draw attention to
	biases that can compromise the validity of empirical research and lead to
	socially harmful applications.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>suster-tulkens-daelemans:2017:EthNLP</bibkey>
  </paper>

  <paper id="1611">
    <title>Goal-Oriented Design for Ethical Machine Learning and NLP</title>
    <author><first>Tyler</first><last>Schnoebelen</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>88&#8211;93</pages>
    <url>http://www.aclweb.org/anthology/W17-1611</url>
    <abstract>The argument made in this paper is that to act ethically in machine learning
	and NLP requires focusing on goals. NLP projects are often classificatory
	systems that deal with human subjects, which means that goals from people
	affected by the systems should be included. The paper takes as its core example
	a model that detects criminality, showing the problems of training data,
	categories, and outcomes. The paper is oriented to the kinds of critiques on
	power and the reproduction of inequality that are found in social theory, but
	it also includes concrete suggestions on how to put goal-oriented design into
	practice.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>schnoebelen:2017:EthNLP</bibkey>
  </paper>

  <paper id="1612">
    <title>Ethical Research Protocols for Social Media Health Research</title>
    <author><first>Adrian</first><last>Benton</last></author>
    <author><first>Glen</first><last>Coppersmith</last></author>
    <author><first>Mark</first><last>Dredze</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>94&#8211;102</pages>
    <url>http://www.aclweb.org/anthology/W17-1612</url>
    <abstract>Social media have transformed data-driven research in political science, the
	social sciences, health, and medicine. Since health research often touches on
	sensitive topics that relate to ethics of treatment and patient privacy,
	similar ethical considerations should be acknowledged when using social media
	data in health research.  While much has been said regarding the ethical
	considerations of social media research, health research leads to an additional
	set of concerns.  We provide practical suggestions in the form of guidelines
	for researchers working with social media data in health research.  These
	guidelines can inform an IRB proposal for researchers new to social media
	health research.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>benton-coppersmith-dredze:2017:EthNLP</bibkey>
  </paper>

  <paper id="1613">
    <title>Say the Right Thing Right: Ethics Issues in Natural Language Generation Systems</title>
    <author><first>Charese</first><last>Smiley</last></author>
    <author><first>Frank</first><last>Schilder</last></author>
    <author><first>Vassilis</first><last>Plachouras</last></author>
    <author><first>Jochen L.</first><last>Leidner</last></author>
    <booktitle>Proceedings of the First ACL Workshop on Ethics in Natural Language Processing</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>103&#8211;108</pages>
    <url>http://www.aclweb.org/anthology/W17-1613</url>
    <abstract>We discuss the ethical implications of Natural Language Generation systems.
	We use one particular system as a case study to identify and classify issues,
	and we provide an ethics checklist, in the hope that future system designers
	may benefit from conducting their own ethics reviews based on our checklist.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>smiley-EtAl:2017:EthNLP</bibkey>
  </paper>

  <paper id="1700">
    <title>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</title>
    <editor>Stella Markantonatou</editor>
    <editor>Carlos Ramisch</editor>
    <editor>Agata Savary</editor>
    <editor>Veronika Vincze</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-17</url>
    <bibtype>book</bibtype>
    <bibkey>MWE2017:2017</bibkey>
  </paper>

  <paper id="1701">
    <title>ParaDi: Dictionary of Paraphrases of Czech Complex Predicates with Light Verbs</title>
    <author><first>Petra</first><last>Barancikova</last></author>
    <author><first>V&#225;clava</first><last>Kettnerov&#225;</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;10</pages>
    <url>http://www.aclweb.org/anthology/W17-1701</url>
    <abstract>We present a new freely available dictionary of paraphrases of Czech complex
	predicates with light verbs, ParaDi. Candidates for single predicative
	paraphrases of selected complex predicates have been extracted automatically
	from large monolingual data using word2vec. They have been manually verified
	and further refined. We demonstrate one of many possible applications of ParaDi
	in an experiment with improving machine translation quality.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>barancikova-kettnerova:2017:MWE2017</bibkey>
  </paper>

  <paper id="1702">
    <title>Multi-word Entity Classification in a Highly Multilingual Environment</title>
    <author><first>Sophie</first><last>Chesney</last></author>
    <author><first>Guillaume</first><last>Jacquet</last></author>
    <author><first>Ralf</first><last>Steinberger</last></author>
    <author><first>Jakub</first><last>Piskorski</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11&#8211;20</pages>
    <url>http://www.aclweb.org/anthology/W17-1702</url>
    <abstract>This paper describes an approach for the classification of millions of existing
	multi-word entities (MWEntities), such as organisation or event names, into
	thirteen category types, based only on the tokens they contain. 
	In order to classify our very large in-house collection of multilingual
	MWEntities into an application-oriented set of entity categories, we trained
	and tested distantly-supervised classifiers in 43 languages based on MWEntities
	extracted from BabelNet. The best-performing classifier was the multi-class SVM
	using a TF.IDF-weighted data representation. Interestingly, one unique
	classifier trained on a mix of all languages consistently performed better than
	classifiers trained for individual languages, reaching an averaged F1-value of
	88.8%. In this paper, we present the training and test data, including a human
	evaluation of its accuracy, describe the methods used to train the classifiers,
	and discuss the results.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chesney-EtAl:2017:MWE2017</bibkey>
  </paper>

  <paper id="1703">
    <title>Using bilingual word-embeddings for multilingual collocation extraction</title>
    <author><first>Marcos</first><last>Garcia</last></author>
    <author><first>Marcos</first><last>Garc&#237;a-Salido</last></author>
    <author><first>Margarita</first><last>Alonso-Ramos</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21&#8211;30</pages>
    <url>http://www.aclweb.org/anthology/W17-1703</url>
    <abstract>This paper presents a new strategy for multilingual collocation extraction
	which takes advantage of parallel corpora to learn bilingual word-embeddings.
	Monolingual collocation candidates are retrieved using Universal Dependencies,
	while the distributional models are then applied to search for equivalents of
	the elements of each collocation in the target languages. The proposed method
	extracts not only collocation equivalents with direct translation between
	languages, but also other cases where the collocations in the two languages are
	not literal translations of each other.
	Several experiments -evaluating collocations with three syntactic patterns- in
	English, Spanish, and Portuguese show that our approach can effectively extract
	large pairs of bilingual equivalents with an average precision of about 90%.
	Moreover, preliminary results on comparable corpora suggest that the
	distributional models can be applied for identifying new bilingual collocations
	in different domains.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>garcia-garciasalido-alonsoramos:2017:MWE2017</bibkey>
  </paper>

  <paper id="1704">
    <title>The PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions</title>
    <author><first>Agata</first><last>Savary</last></author>
    <author><first>Carlos</first><last>Ramisch</last></author>
    <author><first>Silvio</first><last>Cordeiro</last></author>
    <author><first>Federico</first><last>Sangati</last></author>
    <author><first>Veronika</first><last>Vincze</last></author>
    <author><first>Behrang</first><last>QasemiZadeh</last></author>
    <author><first>Marie</first><last>Candito</last></author>
    <author><first>Fabienne</first><last>Cap</last></author>
    <author><first>Voula</first><last>Giouli</last></author>
    <author><first>Ivelina</first><last>Stoyanova</last></author>
    <author><first>Antoine</first><last>Doucet</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31&#8211;47</pages>
    <url>http://www.aclweb.org/anthology/W17-1704</url>
    <abstract>Multiword expressions (MWEs) are
	known as a "pain in the neck" for NLP
	due to their idiosyncratic behaviour.
	While some categories of MWEs have
	been addressed by many studies, verbal
	MWEs (VMWEs), such as to take a
	decision, to break one’s heart or to turn
	off, have been rarely modelled. This is
	notably due to their syntactic variability,
	which hinders treating them as "words
	with spaces". We describe an initiative
	meant to bring about substantial progress
	in understanding, modelling and process-
	ing VMWEs. It is a joint effort, carried
	out within a European research network,
	to elaborate universal terminologies and
	annotation guidelines for 18 languages. Its
	main outcome is a multilingual 5-million-
	word annotated corpus which underlies a
	shared task on automatic identification of
	VMWEs. This paper presents the corpus
	annotation methodology and outcome, the
	shared task organisation and the results of
	the participating systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>savary-EtAl:2017:MWE2017</bibkey>
  </paper>

  <paper id="1705">
    <title>USzeged: Identifying Verbal Multiword Expressions with POS Tagging and Parsing Techniques</title>
    <author><first>Katalin Ilona</first><last>Simk&#243;</last></author>
    <author><first>Vikt&#243;ria</first><last>Kov&#225;cs</last></author>
    <author><first>Veronika</first><last>Vincze</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>48&#8211;53</pages>
    <url>http://www.aclweb.org/anthology/W17-1705</url>
    <abstract>The paper describes our system submitted for the Workshop on Multiword
	Expressions’ shared task on automatic identification of verbal multiword
	expressions. It uses POS tagging and dependency parsing to identify single- and
	multi-token verbal MWEs in text. Our system is language independent and
	competed on nine of the eighteen languages. Our paper describes how our system
	works and gives its error analysis for the languages it was submitted for.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>simko-kovacs-vincze:2017:MWE2017</bibkey>
  </paper>

  <paper id="1706">
    <title>Parsing and MWE Detection: Fips at the PARSEME Shared Task</title>
    <author><first>Luka</first><last>Nerima</last></author>
    <author><first>Vasiliki</first><last>Foufi</last></author>
    <author><first>Eric</first><last>Wehrli</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>54&#8211;59</pages>
    <url>http://www.aclweb.org/anthology/W17-1706</url>
    <abstract>Identifying multiword expressions (MWEs) in a sentence in order to ensure their
	proper processing in subsequent applications, like machine translation, and
	performing the syntactic analysis of the sentence are interrelated processes.
	In our approach, priority is given to parsing alternatives involving
	collocations, and hence collocational information helps the parser through the
	maze of alternatives, with the aim to lead to substantial improvements in the
	performance of both tasks (collocation identification and parsing), and in that
	of a subsequent task (machine translation). In this paper, we are going to
	present our system and the procedure that we have followed in order to
	participate to the open track of the PARSEME shared task on automatic
	identification of verbal multiword expressions (VMWEs) in running texts.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nerima-foufi-wehrli:2017:MWE2017</bibkey>
  </paper>

  <paper id="1707">
    <title>Neural Networks for Multi-Word Expression Detection</title>
    <author><first>Natalia</first><last>Klyueva</last></author>
    <author><first>Antoine</first><last>Doucet</last></author>
    <author><first>Milan</first><last>Straka</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>60&#8211;65</pages>
    <url>http://www.aclweb.org/anthology/W17-1707</url>
    <abstract>In this paper we describe the MUMULS system that participated to the 2017
	shared task on automatic identification of verbal multiword expressions
	(VMWEs). The MUMULS system was implemented using a supervised approach based on
	recurrent neural networks using the open source library TensorFlow. The model
	was trained on a data set containing annotated VMWEs as well as morphological
	and syntactic information. The MUMULS system performed the identification of
	VMWEs in 15 languages, it was one of few systems that could categorize VMWEs
	type in nearly all languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>klyueva-doucet-straka:2017:MWE2017</bibkey>
  </paper>

  <paper id="1708">
    <title>Factoring Ambiguity out of the Prediction of Compositionality for German Multi-Word Expressions</title>
    <author><first>Stefan</first><last>Bott</last></author>
    <author><first>Sabine</first><last>Schulte im Walde</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66&#8211;72</pages>
    <url>http://www.aclweb.org/anthology/W17-1708</url>
    <abstract>Ambiguity represents an obstacle for distributional semantic models(DSMs),
	which typically subsume the contexts of all word senses within one vector.
	While individual vector space approaches have been concerned with sense
	discrimination (e.g., Sch&#252;tze 1998, Erk 2009, Erk and Pado 2010), such
	discrimination has rarely been integrated into DSMs across semantic tasks. This
	paper presents a soft-clustering approach to sense discrimination that filters
	sense-irrelevant features when predicting the degrees of compositionality for
	German noun-noun compounds and German particle verbs.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bott-schulteimwalde:2017:MWE2017</bibkey>
  </paper>

  <paper id="1709">
    <title>Multiword expressions and lexicalism: the view from LFG</title>
    <author><first>Jamie Y.</first><last>Findlay</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>73&#8211;79</pages>
    <url>http://www.aclweb.org/anthology/W17-1709</url>
    <abstract>Multiword expressions (MWEs) pose a problem for lexicalist theories like
	Lexical Functional Grammar (LFG), since they are prima facie counterexamples to
	a strong form of the lexical integrity principle, which entails that a lexical
	item can only be realised as a single, syntactically atomic word. In this
	paper, I demonstrate some of the problems facing any strongly lexicalist
	account of MWEs, and argue that the lexical integrity principle must be
	weakened. I conclude by sketching a formalism which integrates a Tree Adjoining
	Grammar into the LFG architecture, taking advantage of this relaxation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>findlay:2017:MWE2017</bibkey>
  </paper>

  <paper id="1710">
    <title>Understanding Idiomatic Variation</title>
    <author><first>Kristina</first><last>Geeraert</last></author>
    <author><first>R. Harald</first><last>Baayen</last></author>
    <author><first>John</first><last>Newman</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>80&#8211;90</pages>
    <url>http://www.aclweb.org/anthology/W17-1710</url>
    <abstract>This study investigates the processing of idiomatic variants through an
	eye-tracking experiment. Four types of idiom variants were included, in
	addition to the canonical form and the literal meaning. Results suggest that
	modifications to idioms, modulo obvious effects of length differences, are not
	more difficult to process than the canonical forms themselves. This fits with
	recent corpus findings.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>geeraert-baayen-newman:2017:MWE2017</bibkey>
  </paper>

  <paper id="1711">
    <title>Discovering Light Verb Constructions and their Translations from Parallel Corpora without Word Alignment</title>
    <author><first>Natalie</first><last>Vargas</last></author>
    <author><first>Carlos</first><last>Ramisch</last></author>
    <author><first>Helena</first><last>Caseli</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91&#8211;96</pages>
    <url>http://www.aclweb.org/anthology/W17-1711</url>
    <abstract>We propose a method for joint unsupervised discovery of multiword expressions
	(MWEs) and their translations from parallel corpora. First, we apply
	independent monolingual MWE extraction in source and target languages
	simultaneously. Then, we calculate translation probability, association score
	and distributional  similarity of co-occurring pairs. Finally, we rank all
	translations of a given MWE using a linear combination of these features.
	Preliminary experiments on light verb constructions show promising results.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vargas-ramisch-caseli:2017:MWE2017</bibkey>
  </paper>

  <paper id="1712">
    <title>Identification of Multiword Expressions for Latvian and Lithuanian: Hybrid Approach</title>
    <author><first>Justina</first><last>Mandravickaite</last></author>
    <author><first>Tomas</first><last>Krilavi&#x10D;ius</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>97&#8211;101</pages>
    <url>http://www.aclweb.org/anthology/W17-1712</url>
    <abstract>We discuss an experiment on automatic identification of bi-gram multi-word
	expressions in parallel Latvian and Lithuanian corpora. Raw corpora, lexical
	association measures (LAMs) and supervised machine learning (ML) are used due
	to deficit and quality of lexical resources (e.g., POS-tagger, parser) and
	tools. While combining LAMs with ML is rather effective for other languages, it
	has shown some nice results for Lithuanian and Latvian as well. Combining LAMs
	with ML we have achieved 92,4% precision and 52,2% recall for Latvian and 95,1%
	precision and 77,8% recall for Lithuanian.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mandravickaite-krilavivcius:2017:MWE2017</bibkey>
  </paper>

  <paper id="1713">
    <title>Show Me Your Variance and I Tell You Who You Are - Deriving Compound Compositionality from Word Alignments</title>
    <author><first>Fabienne</first><last>Cap</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102&#8211;107</pages>
    <url>http://www.aclweb.org/anthology/W17-1713</url>
    <abstract>We use word alignment variance as an indicator for the non-compositionality of
	German and English noun compounds. Our work-in-progress results are on their
	own not competitive with state-of-the art approaches, but they show that
	alignment variance is correlated with compositionality and thus worth a
	closer look in the future.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cap:2017:MWE2017</bibkey>
  </paper>

  <paper id="1714">
    <title>Semantic annotation to characterize contextual variation in terminological noun compounds: a pilot study</title>
    <author><first>Melania</first><last>Cabezas-Garc&#237;a</last></author>
    <author><first>Antonio</first><last>San Mart&#237;n</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>108&#8211;113</pages>
    <url>http://www.aclweb.org/anthology/W17-1714</url>
    <abstract>Noun compounds (NCs) are semantically complex and not fully compositional, as
	is often assumed. This paper presents a pilot study regarding the semantic
	annotation of environmental NCs with a view to accessing their semantics and
	exploring their domain-based contextual variation. Our results showed that the
	semantic annotation of NCs afforded important insights into how context impacts
	their conceptualization.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cabezasgarcia-sanmartin:2017:MWE2017</bibkey>
  </paper>

  <paper id="1715">
    <title>Detection of Verbal Multi-Word Expressions via Conditional Random Fields with Syntactic Dependency Features and Semantic Re-Ranking</title>
    <author><first>Alfredo</first><last>Maldonado</last></author>
    <author><first>Lifeng</first><last>Han</last></author>
    <author><first>Erwan</first><last>Moreau</last></author>
    <author><first>Ashjan</first><last>Alsulaimani</last></author>
    <author><first>Koel Dutta</first><last>Chowdhury</last></author>
    <author><first>Carl</first><last>Vogel</last></author>
    <author><first>Qun</first><last>Liu</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>114&#8211;120</pages>
    <url>http://www.aclweb.org/anthology/W17-1715</url>
    <abstract>A description of a system for identifying Verbal Multi-Word Expressions (VMWEs)
	in running text is presented. The system mainly exploits universal syntactic
	dependency features through a Conditional Random Fields (CRF) sequence model.
	The system competed in the Closed Track at the PARSEME VMWE Shared Task 2017,
	ranking 2nd place in most languages on full VMWE-based evaluation and 1st in
	three languages on token-based evaluation. In addition, this paper presents an
	option to re-rank the 10 best CRF-predicted sequences via semantic vectors,
	boosting its scores above other systems in the competition. We also show that
	all systems in the competition would struggle to beat a simple lookup baseline
	system and argue for a more purpose-specific evaluation scheme.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>maldonado-EtAl:2017:MWE2017</bibkey>
  </paper>

  <paper id="1716">
    <title>A data-driven approach to verbal multiword expression detection. PARSEME Shared Task system description paper</title>
    <author><first>Tiberiu</first><last>Boro&#x15F;</last></author>
    <author><first>Sonia</first><last>Pipa</last></author>
    <author><first>Verginica</first><last>Barbu Mititelu</last></author>
    <author><first>Dan</first><last>Tufi&#x15F;</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>121&#8211;126</pages>
    <url>http://www.aclweb.org/anthology/W17-1716</url>
    <abstract>"Multiword expressions" are groups of words acting as a morphologic, syntactic
	and semantic unit in linguistic analysis. Verbal multiword expressions
	represent the subgroup of multiword expressions, namely that in which a verb is
	the syntactic head of the group considered in its canonical (or dictionary)
	form. All multiword expressions are a great challenge for natural language
	processing, but the verbal ones are particularly interesting for tasks such as
	parsing, as the verb is the central element in the syntactic organization of a
	sentence. In this paper we introduce our data-driven approach to verbal
	multiword expressions which was objectively validated during the PARSEME shared
	task on verbal multiword expressions identification. We tested our approach on
	12 languages, and we provide detailed information about corpora composition,
	feature selection process, validation procedure and performance on all
	languages.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>borocs-EtAl:2017:MWE2017</bibkey>
  </paper>

  <paper id="1717">
    <title>The ATILF-LLF System for Parseme Shared Task: a Transition-based Verbal Multiword Expression Tagger</title>
    <author><first>Hazem</first><last>Al Saied</last></author>
    <author><first>Matthieu</first><last>Constant</last></author>
    <author><first>Marie</first><last>Candito</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>127&#8211;132</pages>
    <url>http://www.aclweb.org/anthology/W17-1717</url>
    <abstract>We describe the ATILF-LLF system built for the MWE 2017 Shared Task on
	automatic identification of verbal multiword expressions. We participated in
	the closed track only, for all the 18 available languages. Our system is a
	robust greedy transition-based system, in which MWE are identified through a
	MERGE transition. The system was meant to accommodate the variety of linguistic
	resources provided for each language, in terms of accompanying morphological
	and syntactic information. Using per-MWE Fscore, the system was ranked first
	for all but two languages (Hungarian and Romanian).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>alsaied-constant-candito:2017:MWE2017</bibkey>
  </paper>

  <paper id="1718">
    <title>Investigating the Opacity of Verb-Noun Multiword Expression Usages in Context</title>
    <author><first>Shiva</first><last>Taslimipoor</last></author>
    <author><first>Omid</first><last>Rohanian</last></author>
    <author><first>Ruslan</first><last>Mitkov</last></author>
    <author><first>Afsaneh</first><last>Fazly</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>133&#8211;138</pages>
    <url>http://www.aclweb.org/anthology/W17-1718</url>
    <abstract>This study investigates the supervised
	token-based identification of Multiword
	Expressions (MWEs). This is an ongoing
	research to exploit the information contained
	in the contexts in which different instances
	of an expression could occur. This
	information is used to investigate the question
	of whether an expression is literal or
	MWE. Lexical and syntactic context features
	derived from vector representations
	are shown to be more effective over traditional
	statistical measures to identify tokens
	of MWEs.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>taslimipoor-EtAl:2017:MWE2017</bibkey>
  </paper>

  <paper id="1719">
    <title>Compositionality in Verb-Particle Constructions</title>
    <author><first>Archna</first><last>Bhatia</last></author>
    <author><first>Choh Man</first><last>Teng</last></author>
    <author><first>James</first><last>Allen</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>139&#8211;148</pages>
    <url>http://www.aclweb.org/anthology/W17-1719</url>
    <abstract>We are developing a broad-coverage deep semantic lexicon for a system that
	parses sentences into a logical form expressed in a rich ontology that supports
	reasoning. In this paper we look at verb-particle constructions (VPCs), and the
	extent to which they can be treated compositionally vs idiomatically. First we
	distinguish between the different types of VPCs based on their compositionality
	and then present a set of heuristics for classifying specific instances as
	compositional or not. We then identify a small set of general sense classes for
	particles when used compositionally and discuss the resulting lexical
	representations that are being added to the lexicon. By treating VPCs as
	compositional whenever possible, we attain broad coverage in a compact way, and
	also enable interpretations of novel VPC usages not explicitly present in the
	lexicon.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bhatia-teng-allen:2017:MWE2017</bibkey>
  </paper>

  <paper id="1720">
    <title>Rule-Based Translation of Spanish Verb-Noun Combinations into Basque</title>
    <author><first>Uxoa</first><last>I&#241;urrieta</last></author>
    <author><first>Itziar</first><last>Aduriz</last></author>
    <author><first>Arantza</first><last>Diaz de Ilarraza</last></author>
    <author><first>Gorka</first><last>Labaka</last></author>
    <author><first>Kepa</first><last>Sarasola</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>149&#8211;154</pages>
    <url>http://www.aclweb.org/anthology/W17-1720</url>
    <abstract>This paper presents a method to improve the translation of Verb-Noun
	Combinations (VNCs) in a rule-based Machine Translation (MT) system for
	Spanish-Basque. Linguistic information about a set of VNCs is gathered from the
	public database Konbitzul, and it is integrated into the MT system, leading to
	an improvement in BLEU, NIST and TER scores, as well as the results being
	evidently better according to human evaluators.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>inurrieta-EtAl:2017:MWE2017</bibkey>
  </paper>

  <paper id="1721">
    <title>Verb-Particle Constructions in Questions</title>
    <author><first>Veronika</first><last>Vincze</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>155&#8211;160</pages>
    <url>http://www.aclweb.org/anthology/W17-1721</url>
    <abstract>In this paper, we investigate the behavior of verb-particle constructions in
	English questions. We present a small dataset that contains questions and
	verb-particle
	construction candidates. We demonstrate that there are significant differences
	in the distribution of WH-words, verbs and prepositions/particles in sentences
	that contain VPCs and sentences that contain only verb + prepositional phrase
	combinations both by statistical means and in machine learning experiments.
	Hence, VPCs and non-VPCs can be effectively separated from each other by using
	a rich feature set, containing several novel features.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>vincze:2017:MWE2017</bibkey>
  </paper>

  <paper id="1722">
    <title>Simple Compound Splitting for German</title>
    <author><first>Marion</first><last>Weller-Di Marco</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>161&#8211;166</pages>
    <url>http://www.aclweb.org/anthology/W17-1722</url>
    <abstract>This paper presents a simple method for
	German compound splitting that combines
	a basic frequency-based approach with a
	form-to-lemma mapping to approximate
	morphological operations. With the exception 
	of a small set of hand-crafted rules
	for modeling transitional elements, this 
	approach is resource-poor. In our evaluation,
	the simple splitter outperforms a splitter
	relying on rich morphological resources.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wellerdimarco:2017:MWE2017</bibkey>
  </paper>

  <paper id="1723">
    <title>Identification of Ambiguous Multiword Expressions Using Sequence Models and Lexical Resources</title>
    <author><first>Manon</first><last>Scholivet</last></author>
    <author><first>Carlos</first><last>Ramisch</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>167&#8211;175</pages>
    <url>http://www.aclweb.org/anthology/W17-1723</url>
    <abstract>We present a simple and efficient tagger capable of identifying highly
	ambiguous multiword expressions (MWEs) in French texts. It is based on
	conditional random fields (CRF), using local context information as features.
	We show that this approach can obtain results that, in some cases, approach
	more sophisticated parser-based MWE identification methods without requiring
	syntactic trees from a treebank. Moreover, we study how well the CRF can take
	into account external information coming from a lexicon.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>scholivet-ramisch:2017:MWE2017</bibkey>
  </paper>

  <paper id="1724">
    <title>Comparing Recurring Lexico-Syntactic Trees (RLTs) and Ngram Techniques for Extended Phraseology Extraction</title>
    <author><first>Agn&#232;s</first><last>Tutin</last></author>
    <author><first>Olivier</first><last>Kraif</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>176&#8211;180</pages>
    <url>http://www.aclweb.org/anthology/W17-1724</url>
    <abstract>This paper aims at assessing to what extent a syntax-based method (Recurring
	Lexico-syntactic Trees                                (RLT) extraction) allows us to
	extract
	large
	phraseological units such as prefabricated routines, e.g. "as previously said"
	or "as far as we/I know" in scientific writing.  In order to evaluate this
	method, we compare it to the classical ngram extraction technique, on a subset
	of recurring segments including speech verbs in a French corpus of scientific
	writing. Results show that  the LRT extraction technique is far more efficient
	for extended MWEs such as routines or collocations but performs more poorly for
	surface phenomena such as syntactic constructions or fully frozen expressions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>tutin-kraif:2017:MWE2017</bibkey>
  </paper>

  <paper id="1725">
    <title>Benchmarking Joint Lexical and Syntactic Analysis on Multiword-Rich Data</title>
    <author><first>Matthieu</first><last>Constant</last></author>
    <author><first>H&#233;ctor</first><last>Mart&#237;nez Alonso</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>181&#8211;186</pages>
    <url>http://www.aclweb.org/anthology/W17-1725</url>
    <abstract>This article evaluates the extension of a dependency parser that performs joint
	syntactic analysis and multiword expression identification. We show that, given
	sufficient training data, the parser benefits from explicit multiword
	information and improves overall labeled accuracy score in eight of the ten
	evaluation cases.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>constant-martinezalonso:2017:MWE2017</bibkey>
  </paper>

  <paper id="1726">
    <title>Semi-Automated Resolution of Inconsistency for a Harmonized Multiword Expression and Dependency Parse Annotation</title>
    <author><first>King</first><last>Chan</last></author>
    <author><first>Julian</first><last>Brooke</last></author>
    <author><first>Timothy</first><last>Baldwin</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>187&#8211;193</pages>
    <url>http://www.aclweb.org/anthology/W17-1726</url>
    <abstract>This paper presents a methodology for identifying and resolving various kinds
	of inconsistency in the context of merging dependency and multiword expression
	(MWE) annotations, to generate a dependency treebank with comprehensive MWE
	annotations. Candidates for correction are identified using a variety of
	heuristics, including an entirely novel one which identifies violations of MWE
	constituency in the dependency tree, and resolved by arbitration with minimal
	human intervention. Using this technique, we identified and corrected several
	hundred errors across both parse and MWE annotations, representing changes to a
	significant percentage (well over 10%) of the MWE instances in the joint
	corpus.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>chan-brooke-baldwin:2017:MWE2017</bibkey>
  </paper>

  <paper id="1727">
    <title>Combining Linguistic Features for the Detection of Croatian Multiword Expressions</title>
    <author><first>Maja</first><last>Buljan</last></author>
    <author><first>Jan</first><last>&#x160;najder</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>194&#8211;199</pages>
    <url>http://www.aclweb.org/anthology/W17-1727</url>
    <abstract>As multiword expressions (MWEs) exhibit a range of idiosyncrasies, their
	automatic detection warrants the use of many different features. Tsvetkov and
	Wintner (2014) proposed a Bayesian network model that combines linguistically
	motivated features and also models their interactions. In this paper, we extend
	their model with new features and apply it to Croatian, a morphologically
	complex and a relatively free word order language, achieving a satisfactory
	performance of 0.823 F1-score. Furthermore, by comparing against (semi)naive
	Bayes models, we demonstrate that manually modeling feature interactions is
	indeed important.  We make our annotated dataset of Croatian MWEs freely
	available.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>buljan-vsnajder:2017:MWE2017</bibkey>
  </paper>

  <paper id="1728">
    <title>Complex Verbs are Different: Exploring the Visual Modality in Multi-Modal Models to Predict Compositionality</title>
    <author><first>Maximilian</first><last>K&#246;per</last></author>
    <author><first>Sabine</first><last>Schulte im Walde</last></author>
    <booktitle>Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017)</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200&#8211;206</pages>
    <url>http://www.aclweb.org/anthology/W17-1728</url>
    <abstract>This paper compares a neural network DSM relying on textual co-occurrences with
	a multi-modal model integrating visual information. We focus on nominal vs.
	verbal compounds, and zoom into lexical, empirical and perceptual target
	properties to explore the contribution of the visual modality. Our experiments
	show that (i)  visual features contribute differently for verbs than for nouns,
	and (ii) images complement textual information, if (a) the textual modality by
	itself is poor and appropriate image subsets are used, or (b) the textual
	modality by itself is rich and large (potentially noisy) images are added.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koper-schulteimwalde:2017:MWE2017</bibkey>
  </paper>

  <paper id="1800">
    <title>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</title>
    <editor>Eduardo Blanco</editor>
    <editor>Roser Morante</editor>
    <editor>Roser Saur&#237;</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-18</url>
    <bibtype>book</bibtype>
    <bibkey>SemBEaR:2017</bibkey>
  </paper>

  <paper id="1801">
    <title>Understanding the Semantics of Narratives of Interpersonal Violence through Reader Annotations and Physiological Reactions</title>
    <author><first>Alexander</first><last>Calderwood</last></author>
    <author><first>Elizabeth A.</first><last>Pruett</last></author>
    <author><first>Raymond</first><last>Ptucha</last></author>
    <author><first>Christopher</first><last>Homan</last></author>
    <author><first>Cecilia</first><last>Ovesdotter Alm</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;9</pages>
    <url>http://www.aclweb.org/anthology/W17-1801</url>
    <abstract>Interpersonal violence (IPV) is a prominent sociological problem that affects
	people of all demographic backgrounds. By analyzing how readers interpret,
	perceive, and react to experiences narrated in social media posts, we explore
	an understudied source for discourse about abuse. We asked readers to annotate
	Reddit posts about relationships with vs. without IPV for stakeholder roles and
	emotion, while measuring their galvanic skin response (GSR), pulse, and facial
	expression. We map annotations to coreference resolution output to obtain a
	labeled coreference chain for stakeholders in texts, and apply automated
	semantic role labeling for analyzing IPV discourse. Findings provide insights
	into how readers process roles and emotion in narratives. For example, abusers
	tend to be linked with violent actions and certain affect states. We train
	classifiers to predict stakeholder categories of coreference chains. We also
	find that subjects' GSR noticeably changed for IPV texts, suggesting that
	co-collected measurement-based data about annotators can be used to support
	text annotation.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>calderwood-EtAl:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1802">
    <title>Intension, Attitude, and Tense Annotation in a High-Fidelity Semantic Representation</title>
    <author><first>Gene</first><last>Kim</last></author>
    <author><first>Lenhart</first><last>Schubert</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10&#8211;15</pages>
    <url>http://www.aclweb.org/anthology/W17-1802</url>
    <abstract>This paper describes current efforts in developing an annotation schema and
	guidelines for sentences in Episodic Logic (EL).  We focus on important
	distinctions for representing modality, attitudes, and tense and present an
	annotation schema that makes these distinctions.  EL has proved competitive
	with other logical formulations in speed and inference-enablement, while
	expressing a wider array of natural language phenomena including intensional
	modification of predicates and sentences, propositional attitudes, and tense
	and aspect.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kim-schubert:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1803">
    <title>Towards a lexicon of event-selecting predicates for a French FactBank</title>
    <author><first>Ingrid</first><last>Falk</last></author>
    <author><first>Fabienne</first><last>Martin</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16&#8211;21</pages>
    <url>http://www.aclweb.org/anthology/W17-1803</url>
    <abstract>This paper presents ongoing work for the
	construction of a French FactBank and a
	lexicon of French event-selecting predi-
	cates (ESPs), by applying the factuality
	detection algorithm introduced in (Saur&#237;
	and Pustejovsky, 2012). This algorithm
	relies on a lexicon of ESPs, specifying
	how these predicates influence the polar-
	ity of their embedded events. For this pilot
	study, we focused on French factive and
	implicative verbs, and capitalised on a lex-
	ical resource for the English counterparts
	of these verbs provided by the CLSI Group
	(Nairn et al., 2006; Karttunen, 2012).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>falk-martin:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1804">
    <title>Universal Dependencies to Logical Form with Negation Scope</title>
    <author><first>Federico</first><last>Fancellu</last></author>
    <author><first>Siva</first><last>Reddy</last></author>
    <author><first>Adam</first><last>Lopez</last></author>
    <author><first>Bonnie</first><last>Webber</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>22&#8211;32</pages>
    <url>http://www.aclweb.org/anthology/W17-1804</url>
    <abstract>Many language technology applications would benefit from the ability to
	represent negation and its scope on top of widely-used linguistic resources. In
	this paper, we investigate the possibility of obtaining a first-order logic
	representation with negation scope marked using Universal Dependencies. To do
	so, we enhance UDepLambda, a framework that converts dependency graphs to
	logical forms. The resulting UDepLambdalnot is able to handle phenomena
	related to scope by means of an higher-order type theory, relevant not only to
	negation but also to universal quantification and other complex semantic
	phenomena. The initial conversion we did for English is promising, in that one
	can represent the scope of negation also in the presence of more complex
	phenomena such as universal quantifiers.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>fancellu-EtAl:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1805">
    <title>Meaning Banking beyond Events and Roles</title>
    <author><first>Johan</first><last>Bos</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>33</pages>
    <url>http://www.aclweb.org/anthology/W17-1805</url>
    <abstract>In this talk I will discuss the analysis of several semantic phenomena that
	need meaning representations that can describe attributes of propositional
	contexts. I will do this in a version of Discourse Representation Theory, using
	a universal semantic tagset developed as part of a project that aims to produce
	a large meaning bank (a semantically-annotated corpus) for four languages
	(English, Dutch, German and Italian).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>bos:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1806">
    <title>The Scope and Focus of Negation: A Complete Annotation Framework for Italian</title>
    <author><first>Bego&#241;a</first><last>Altuna</last></author>
    <author><first>Anne-Lyse</first><last>Minard</last></author>
    <author><first>Manuela</first><last>Speranza</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34&#8211;42</pages>
    <url>http://www.aclweb.org/anthology/W17-1806</url>
    <abstract>In this paper we present a complete framework for the annotation of negation in
	Italian, which accounts for both negation scope and negation focus, and also
	for language-specific phenomena such as negative concord. In our view, the
	annotation of negation complements more comprehensive Natural Language
	Processing tasks, such as temporal information processing and sentiment
	analysis. We applied the proposed framework and the guidelines built on top of
	it to the annotation of written texts, namely news articles and tweets, thus
	producing annotated data for a total of over 36,000 tokens.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>altuna-minard-speranza:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1807">
    <title>Annotation of negation in the IULA Spanish Clinical Record Corpus</title>
    <author><first>Montserrat</first><last>Marimon</last></author>
    <author><first>Jorge</first><last>Vivaldi</last></author>
    <author><first>N&#250;ria</first><last>Bel</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43&#8211;52</pages>
    <url>http://www.aclweb.org/anthology/W17-1807</url>
    <abstract>This paper presents the IULA Spanish Clinical Record Corpus, a corpus of 3,194
	sentences extracted from anonymized clinical records and manually annotated
	with negation markers and their scope. The corpus was conceived as a resource
	to support clinical text-mining systems, but it is also a useful resource for
	other Natural Language Processing systems handling clinical texts: automatic
	encoding of clinical records, diagnosis support, term extraction, among others,
	as well as for the study of clinical texts. The corpus is publicly available
	with a CC-BY-SA 3.0 license.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>marimon-vivaldi-bel:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1808">
    <title>Annotating Negation in Spanish Clinical Texts</title>
    <author><first>Noa</first><last>Cruz</last></author>
    <author><first>Roser</first><last>Morante</last></author>
    <author><first>Manuel J.</first><last>Ma&#241;a L&#243;pez</last></author>
    <author><first>Jacinto</first><last>Mata V&#225;zquez</last></author>
    <author><first>Carlos L.</first><last>Parra Calder&#243;n</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53&#8211;58</pages>
    <url>http://www.aclweb.org/anthology/W17-1808</url>
    <abstract>In this paper we present  on-going work on annotating negation in Spanish
	clinical documents. A corpus of anamnesis and radiology reports has been
	annotated by two domain expert annotators with negation markers and negated
	events. The Dice coefficient for inter-annotator agreement is higher than 0.94
	for negation markers and higher than 0.72 for negated events. The corpus will
	be publicly released when the annotation process is finished, constituting the
	first corpus annotated with negation for Spanish clinical reports available for
	the NLP community.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cruz-EtAl:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1809">
    <title>Neural Networks for Negation Cue Detection in Chinese</title>
    <author><first>Hangfeng</first><last>He</last></author>
    <author><first>Federico</first><last>Fancellu</last></author>
    <author><first>Bonnie</first><last>Webber</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59&#8211;63</pages>
    <url>http://www.aclweb.org/anthology/W17-1809</url>
    <abstract>Negation cue detection involves identifying the span inherently expressing
	negation in a negative sentence. In Chinese, negative cue detection is
	complicated by morphological proprieties of the language. Previous work has
	shown that negative cue detection in Chinese can benefit from specific lexical
	and morphemic features, as well as cross-lingual information. We show here that
	they are not necessary: A bi-directional LSTM can perform equally well, with
	minimal feature engineering. In particular, the use of a character-based model
	allows us to capture characteristics of negation cues in Chinese using
	word-embedding information only. Not only does our model performs on par with
	previous work, further error analysis clarifies what problems remain to be
	addressed.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>he-fancellu-webber:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1810">
    <title>An open-source tool for negation detection: a maximum-margin approach</title>
    <author><first>Martine</first><last>Enger</last></author>
    <author><first>Erik</first><last>Velldal</last></author>
    <author><first>Lilja</first><last>&#216;vrelid</last></author>
    <booktitle>Proceedings of the Workshop Computational Semantics Beyond Events and Roles</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>64&#8211;69</pages>
    <url>http://www.aclweb.org/anthology/W17-1810</url>
    <abstract>This paper presents an open-source toolkit for negation detection. It
	identifies negation cues and their corresponding scope in either raw or parsed
	text using maximum-margin classification. The system design draws on best
	practice from the existing literature on negation detection, aiming for a
	simple and portable system that still achieves competitive performance.
	Pre-trained models and experimental results are provided for English.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>enger-velldal-ovrelid:2017:SemBEaR</bibkey>
  </paper>

  <paper id="1900">
    <title>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</title>
    <editor>Jose Camacho-Collados</editor>
    <editor>Mohammad Taher Pilehvar</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-19</url>
    <bibtype>book</bibtype>
    <bibkey>SENSE2017:2017</bibkey>
  </paper>

  <paper id="1901">
    <title>Compositional Semantics using Feature-Based Models from WordNet</title>
    <author><first>Pablo</first><last>Gamallo</last></author>
    <author><first>Mart&#237;n</first><last>Pereira-Fari&#241;a</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;11</pages>
    <url>http://www.aclweb.org/anthology/W17-1901</url>
    <abstract>This article describes a method to build semantic representations of composite
	expressions in a compositional way by using WordNet relations to represent the
	meaning of words. The meaning of a target word is modelled as a vector in which
	its semantically related words are assigned weights according to both the type
	of the relationship and the distance to the target word. Word vectors are
	compositionally combined by syntactic dependencies. Each syntactic dependency
	triggers two complementary compositional functions: the named head function and
	 dependent function. The experiments show that the proposed compositional
	method outperforms the state-of-the-art for both intransitive subject-verb and
	transitive subject-verb-object constructions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gamallo-pereirafarina:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1902">
    <title>Automated WordNet Construction Using Word Embeddings</title>
    <author><first>Mikhail</first><last>Khodak</last></author>
    <author><first>Andrej</first><last>Risteski</last></author>
    <author><first>Christiane</first><last>Fellbaum</last></author>
    <author><first>Sanjeev</first><last>Arora</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>12&#8211;23</pages>
    <url>http://www.aclweb.org/anthology/W17-1902</url>
    <abstract>We present a fully unsupervised method for automated construction of WordNets
	based upon recent advances in distributional representations of sentences and
	word-senses combined with readily available machine translation tools. The
	approach requires very few linguistic resources and is thus extensible to
	multiple target languages. To evaluate our method we construct two 600-word
	testsets for word-to-synset matching in French and Russian using native
	speakers and evaluate the performance of our method along with several other
	recent approaches. Our method exceeds the best language-specific and
	multi-lingual automated WordNets in F-score for both languages. The databases
	we construct for French and Russian, both languages without large publicly
	available manually constructed WordNets, will be publicly released along with
	the testsets.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>khodak-EtAl:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1903">
    <title>Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses</title>
    <author><first>Maximilian</first><last>K&#246;per</last></author>
    <author><first>Sabine</first><last>Schulte im Walde</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24&#8211;30</pages>
    <url>http://www.aclweb.org/anthology/W17-1903</url>
    <abstract>Abstract words refer to things that can not be seen, heard, felt, smelled, or
	tasted as opposed to concrete words. Among other applications, the degree of
	abstractness has been shown to be a useful information for metaphor detection.
	Our
	contribution to this topic are as follows: i) we compare supervised techniques
	to
	learn and extend abstractness ratings for huge vocabularies ii) we learn and
	investigate norms for larger units by propagating abstractness to verb-noun
	pairs which lead to better metaphor detection iii) we overcome the limitation
	of learning a single rating per word and show that multi-sense abstractness
	ratings are potentially useful for metaphor detection. Finally, with this paper
	we publish automatically created abstractness norms for 3million English words
	and multi-words as well as automatically created sense specific abstractness
	ratings</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>koper-schulteimwalde:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1904">
    <title>Improving Clinical Diagnosis Inference through Integration of Structured and Unstructured Knowledge</title>
    <author><first>Yuan</first><last>Ling</last></author>
    <author><first>Yuan</first><last>An</last></author>
    <author><first>Sadid</first><last>Hasan</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31&#8211;36</pages>
    <url>http://www.aclweb.org/anthology/W17-1904</url>
    <abstract>This paper presents a novel approach to the task of automatically inferring the
	most probable diagnosis from a given clinical narrative. Structured Knowledge
	Bases (KBs) can be useful for such complex tasks but not sufficient. Hence, we
	leverage a vast amount of unstructured free text to integrate with structured
	KBs. The key innovative ideas include building a concept graph from both
	structured and unstructured knowledge sources and ranking the diagnosis
	concepts using the enhanced word embedding vectors learned from integrated
	sources. Experiments on the TREC CDS and HumanDx datasets showed that our
	methods improved the results of clinical diagnosis inference.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ling-an-hasan:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1905">
    <title>Classifying Lexical-semantic Relationships by Exploiting Sense/Concept Representations</title>
    <author><first>Kentaro</first><last>Kanada</last></author>
    <author><first>Tetsunori</first><last>Kobayashi</last></author>
    <author><first>Yoshihiko</first><last>Hayashi</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37&#8211;46</pages>
    <url>http://www.aclweb.org/anthology/W17-1905</url>
    <abstract>This paper proposes a method for classifying the type of lexical-semantic
	relation between a given pair of words. Given an inventory of target
	relationships, this task can be seen as a multi-class classification problem.
	We train a supervised classifier by assuming: (1) a specific type of
	lexical-semantic relation between a pair of words would be indicated by a
	carefully designed set of relation-specific similarities associated with the
	words; and (2) the similarities could be effectively computed by &#x201c;sense
	representations&#x201d; (sense/concept embeddings). The experimental results show
	that the proposed method clearly outperforms an existing state-of-the-art
	method that does not utilize sense/concept embeddings, thereby demonstrating
	the effectiveness of the sense representations.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kanada-kobayashi-hayashi:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1906">
    <title>Supervised and unsupervised approaches to measuring usage similarity</title>
    <author><first>Milton</first><last>King</last></author>
    <author><first>Paul</first><last>Cook</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>47&#8211;52</pages>
    <url>http://www.aclweb.org/anthology/W17-1906</url>
    <abstract>Usage similarity (USim) is an approach to determining word meaning in context
	that does not rely on a sense inventory. Instead, pairs of usages of a target
	lemma are rated on a scale. In this paper we propose unsupervised approaches to
	USim based on embeddings for words, contexts, and sentences, and achieve
	state-of-the-art results over two USim datasets. We further consider supervised
	approaches to USim, and find that although they outperform unsupervised
	approaches, they are unable to generalize to lemmas that are unseen in the
	training data.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>king-cook:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1907">
    <title>Lexical Disambiguation of Igbo using Diacritic Restoration</title>
    <author><first>Ignatius</first><last>Ezeani</last></author>
    <author><first>Mark</first><last>Hepple</last></author>
    <author><first>Ikechukwu</first><last>Onyenwe</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>53&#8211;60</pages>
    <url>http://www.aclweb.org/anthology/W17-1907</url>
    <abstract>Properly written texts in Igbo, a low-resource African language, are rich in
	both orthographic and tonal diacritics. Diacritics are essential in capturing
	the distinctions in pronunciation and meaning of words, as well as in lexical
	disambiguation. Unfortunately, most electronic texts in diacritic languages are
	written without diacritics. This makes diacritic restoration a necessary step
	in corpus building and language processing tasks for languages with diacritics.
	In our previous work, we built some n-gram models with simple smoothing
	techniques based on a closed-world assumption. However, as a classification
	task, diacritic restoration is well suited for and will be more generalisable
	with machine learning. This paper, therefore, presents a more standard approach
	to dealing with the task which involves the application of machine learning
	algorithms.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ezeani-hepple-onyenwe:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1908">
    <title>Creating and Validating Multilingual Semantic Representations for Six Languages: Expert versus Non-Expert Crowds</title>
    <author><first>Mahmoud</first><last>El-Haj</last></author>
    <author><first>Paul</first><last>Rayson</last></author>
    <author><first>Scott</first><last>Piao</last></author>
    <author><first>Stephen</first><last>Wattam</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>61&#8211;71</pages>
    <url>http://www.aclweb.org/anthology/W17-1908</url>
    <abstract>Creating high-quality wide-coverage multilingual
	semantic lexicons to support
	knowledge-based approaches is a challenging
	time-consuming manual task.
	This has traditionally been performed by
	linguistic experts: a slow and expensive
	process. We present an experiment in
	which we adapt and evaluate crowdsourcing
	methods employing native speakers to
	generate a list of coarse-grained senses under
	a common multilingual semantic taxonomy
	for sets of words in six languages.
	451 non-experts (including 427 Mechanical
	Turk workers) and 15 expert participants
	semantically annotated 250 words
	manually for Arabic, Chinese, English,
	Italian, Portuguese and Urdu lexicons. In
	order to avoid erroneous (spam) crowdsourced
	results, we used a novel task-specific
	two-phase filtering process where
	users were asked to identify synonyms in
	the target language, and remove erroneous
	senses.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>elhaj-EtAl:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1909">
    <title>Using Linked Disambiguated Distributional Networks for Word Sense Disambiguation</title>
    <author><first>Alexander</first><last>Panchenko</last></author>
    <author><first>Stefano</first><last>Faralli</last></author>
    <author><first>Simone Paolo</first><last>Ponzetto</last></author>
    <author><first>Chris</first><last>Biemann</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>72&#8211;78</pages>
    <url>http://www.aclweb.org/anthology/W17-1909</url>
    <abstract>We introduce a new method for unsupervised knowledge-based word sense
	disambiguation (WSD) based on a resource that links two types of sense-aware
	lexical networks: one is induced from a corpus using distributional semantics,
	the other is manually constructed. The combination of two networks reduces the
	sparsity of sense representations used for WSD. We evaluate these enriched
	representations within two lexical sample sense disambiguation benchmarks. Our
	results indicate that (1) features extracted from the corpus-based resource
	help to significantly outperform a model based solely on the lexical resource;
	(2) our method achieves results comparable or better to four state-of-the-art
	unsupervised knowledge-based WSD systems including three hybrid systems that
	also rely on text corpora. In contrast to these hybrid methods, our approach
	does not require access to web search engines, texts mapped to a sense
	inventory, or machine translation systems.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>panchenko-EtAl:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1910">
    <title>One Representation per Word - Does it make Sense for Composition?</title>
    <author><first>Thomas</first><last>Kober</last></author>
    <author><first>Julie</first><last>Weeds</last></author>
    <author><first>John</first><last>Wilkie</last></author>
    <author><first>Jeremy</first><last>Reffin</last></author>
    <author><first>David</first><last>Weir</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79&#8211;90</pages>
    <url>http://www.aclweb.org/anthology/W17-1910</url>
    <abstract>In this paper, we investigate whether an a priori disambiguation of word senses
	is strictly necessary or whether the meaning of a word in context can be
	disambiguated through composition alone. We evaluate the performance of
	off-the-shelf single-vector and multi-sense vector models on a benchmark phrase
	similarity task and a novel task for word-sense discrimination. We find that
	single-sense vector models perform as well or better than multi-sense vector
	models despite arguably less clean elementary representations. Our findings
	furthermore show that simple composition functions such as pointwise addition
	are able to recover sense specific information from a single-sense vector model
	remarkably well.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>kober-EtAl:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1911">
    <title>Elucidating Conceptual Properties from Word Embeddings</title>
    <author><first>Kyoung-Rok</first><last>Jang</last></author>
    <author><first>Sung-Hyon</first><last>Myaeng</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91&#8211;95</pages>
    <url>http://www.aclweb.org/anthology/W17-1911</url>
    <abstract>In this paper, we introduce a method of identifying the components (i.e.
	dimensions) of word embeddings that strongly signifies properties of a word. By
	elucidating such properties hidden in word embeddings, we could make word
	embeddings more interpretable, and also could perform property-based meaning
	comparison. With the capability, we can answer questions like "To what degree
	a given word has the property cuteness?" or "In what perspective two words
	are similar?". We verify our method by examining how the strength of
	property-signifying components correlates with the degree of prototypicality of
	a target word.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>jang-myaeng:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1912">
    <title>TTCSe: a Vectorial Resource for Computing Conceptual Similarity</title>
    <author><first>Enrico</first><last>Mensa</last></author>
    <author><first>Daniele P.</first><last>Radicioni</last></author>
    <author><first>Antonio</first><last>Lieto</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>96&#8211;101</pages>
    <url>http://www.aclweb.org/anthology/W17-1912</url>
    <abstract>In this paper we introduce the TTCSe, a linguistic resource that relies on
	BabelNet, NASARI and ConceptNet, that has now been used to compute the
	conceptual similarity between concept pairs. The conceptual representation
	herein provides uniform access to concepts based on BabelNet synset IDs, and
	consists of a vector-based semantic representation which is compliant with the
	Conceptual Spaces, a geometric framework for common-sense knowledge
	representation and reasoning. The TTCSe has been evaluated in a preliminary
	experimentation on a conceptual similarity task.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>mensa-radicioni-lieto:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1913">
    <title>Measuring the Italian-English lexical gap for action verbs and its impact on translation</title>
    <author><first>Lorenzo</first><last>Gregori</last></author>
    <author><first>Alessandro</first><last>Panunzi</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>102&#8211;109</pages>
    <url>http://www.aclweb.org/anthology/W17-1913</url>
    <abstract>This paper describes a method to measure the lexical gap of action verbs in
	Italian and English by using the IMAGACT ontology of action. The fine-grained
	categorization of action concepts of the data source allowed to have wide
	overview of the relation between concepts in the two languages. The calculated
	lexical gap for both English and Italian is about 30% of the action concepts,
	much higher than previous results. Beyond this general numbers a deeper
	analysis has been performed in order to evaluate the impact that lexical gaps
	can have on translation. In particular a distinction has been made between the
	cases in which the presence of a lexical gap affects translation correctness
	and completeness at a semantic level. The results highlight a high percentage
	of concepts that can be considered hard to translate (about 18% from English to
	Italian and 20% from Italian to English) and confirms that action verbs are a
	critical lexical class for translation tasks.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>gregori-panunzi:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1914">
    <title>Word Sense Filtering Improves Embedding-Based Lexical Substitution</title>
    <author><first>Anne</first><last>Cocos</last></author>
    <author><first>Marianna</first><last>Apidianaki</last></author>
    <author><first>Chris</first><last>Callison-Burch</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>110&#8211;119</pages>
    <url>http://www.aclweb.org/anthology/W17-1914</url>
    <abstract>The role of word sense disambiguation in lexical substitution has been
	questioned due to the high performance of vector space models which propose
	good substitutes without explicitly accounting for sense. We show that a
	filtering
	mechanism based on a sense inventory optimized for substitutability can improve
	the results of these models. Our sense inventory is constructed using a
	clustering method which generates paraphrase clusters that are congruent with
	lexical substitution annotations in a development set. The results show that
	lexical substitution can still benefit from senses which can improve the output
	of vector space paraphrase ranking models.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>cocos-apidianaki-callisonburch:2017:SENSE2017</bibkey>
  </paper>

  <paper id="1915">
    <title>Supervised and Unsupervised Word Sense Disambiguation on Word Embedding Vectors of Unambigous Synonyms</title>
    <author><first>Aleksander</first><last>Wawer</last></author>
    <author><first>Agnieszka</first><last>Mykowiecka</last></author>
    <booktitle>Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>120&#8211;125</pages>
    <url>http://www.aclweb.org/anthology/W17-1915</url>
    <abstract>This paper compares two approaches to word sense disambiguation using word
	embeddings trained on unambiguous synonyms. The first is unsupervised method
	based on computing log probability from sequences of word embedding vectors,
	taking into account ambiguous word senses and guessing correct sense from
	context. The second method is supervised. We use a multilayer neural network
	model to learn a context-sensitive transformation that maps an input vector of
	ambiguous word into an output vector representing its sense. We evaluate both
	methods on corpora with manual annotations of word senses from the Polish
	wordnet (plWordnet).</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>wawer-mykowiecka:2017:SENSE2017</bibkey>
  </paper>

  <paper id="2000">
    <title>Proceedings of the Sixth Workshop on Vision and Language</title>
    <editor>Anya Belz</editor>
    <editor>Erkut Erdem</editor>
    <editor>Katerina Pastra</editor>
    <editor>Krystian Mikolajczyk</editor>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W17-20</url>
    <bibtype>book</bibtype>
    <bibkey>VL17:2017</bibkey>
  </paper>

  <paper id="2001">
    <title>The BURCHAK corpus: a Challenge Data Set for Interactive Learning of Visually Grounded Word Meanings</title>
    <author><first>Yanchao</first><last>Yu</last></author>
    <author><first>Arash</first><last>Eshghi</last></author>
    <author><first>Gregory</first><last>Mills</last></author>
    <author><first>Oliver</first><last>Lemon</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1&#8211;10</pages>
    <url>http://www.aclweb.org/anthology/W17-2001</url>
    <abstract>We motivate and describe a new freely available human-human dialogue data set
	for interactive learning of visually grounded word meanings through ostensive
	definition by a tutor to a learner. The data has been collected using a novel,
	character-by-character variant of the DiET
	chat tool (Healey et al., 2003; anon.) with a novel task, where a Learner needs
	to learn invented visual attribute words (such as "burchak" for square)
	from a tutor. As such, the text-based interactions closely resemble
	face-to-face conversation and thus contain many of the linguistic
	phenomena encountered in natural, spontaneous dialogue. These include self- and
	other-correction, mid-sentence continuations, interruptions, turn overlaps,
	fillers, hedges and many kinds of ellipsis. We also present a generic n-gram
	framework for building user (i.e. tutor) simulations from this type of
	incremental dialogue data, which is freely available to researchers. We show
	that the simulations produce outputs that are similar to the original data
	(e.g. 78% turn match similarity). Finally, we train and evaluate a
	Reinforcement Learning dialogue control agent for learning visually grounded
	word meanings, trained from the BURCHAK corpus. The learned policy shows
	comparable performance to a rule-based system built previously.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>yu-EtAl:2017:VL17</bibkey>
  </paper>

  <paper id="2002">
    <title>The Use of Object Labels and Spatial Prepositions as Keywords in a Web-Retrieval-Based Image Caption Generation System</title>
    <author><first>Brandon</first><last>Birmingham</last></author>
    <author><first>Adrian</first><last>Muscat</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>11&#8211;20</pages>
    <url>http://www.aclweb.org/anthology/W17-2002</url>
    <abstract>In this paper, a retrieval-based caption generation system that searches the
	web for suitable image descriptions is studied. Google's reverse image search
	is used to find potentially relevant web multimedia content for query images.
	Sentences are extracted from web pages and the likelihood of the descriptions
	is
	computed to select one sentence from the retrieved text documents. The search
	mechanism is modified to replace the caption generated by Google with a caption
	composed of labels and spatial prepositions as part of the query's text
	alongside the image. The object labels are obtained using an off-the-shelf
	R-CNN and a machine learning model is developed to predict the prepositions.
	The effect on the caption generation system performance when using the
	generated text is investigated. Both human evaluations and automatic metrics
	are used to evaluate the retrieved descriptions. Results show that the
	web-retrieval-based approach performed better when describing single-object
	images with sentences extracted from stock photography websites. On the other
	hand, images with two image objects were better described with
	template-generated sentences composed of object labels and prepositions.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>birmingham-muscat:2017:VL17</bibkey>
  </paper>

  <paper id="2003">
    <title>Learning to Recognize Animals by Watching Documentaries: Using Subtitles as Weak Supervision</title>
    <author><first>Aparna</first><last>Nurani Venkitasubramanian</last></author>
    <author><first>Tinne</first><last>Tuytelaars</last></author>
    <author><first>Marie-Francine</first><last>Moens</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>21&#8211;30</pages>
    <url>http://www.aclweb.org/anthology/W17-2003</url>
    <abstract>We investigate animal recognition models learned from wildlife video
	documentaries by using the weak supervision of the textual subtitles. This is a
	particularly challenging setting, since i) the animals occur in their natural
	habitat and are often largely occluded and ii) subtitles are to a large degree
	complementary to the visual content, providing a very weak supervisory signal.
	This is in contrast to most work on integrated vision and language in the
	literature, where textual descriptions are tightly linked to the image content,
	and often generated in a curated fashion for the task at hand. In particular,
	we investigate different image representations and models, including a support
	vector machine on top of activations of a pretrained convolutional neural
	network, as well as a Naive Bayes framework on a 'bag-of-activations' image
	representation, where each element of the bag is considered separately. This
	representation allows key components in the image to be isolated, in spite of
	largely varying backgrounds and image clutter, without an object detection or
	image segmentation step. The methods are evaluated based on how well they
	transfer to unseen camera-trap images captured across diverse topographical
	regions under different environmental conditions and illumination settings,
	involving a large domain shift.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>nuranivenkitasubramanian-tuytelaars-moens:2017:VL17</bibkey>
  </paper>

  <paper id="2004">
    <title>Human Evaluation of Multi-modal Neural Machine Translation: A Case-Study on E-Commerce Listing Titles</title>
    <author><first>Iacer</first><last>Calixto</last></author>
    <author><first>Daniel</first><last>Stein</last></author>
    <author><first>Evgeny</first><last>Matusov</last></author>
    <author><first>Sheila</first><last>Castilho</last></author>
    <author><first>Andy</first><last>Way</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31&#8211;37</pages>
    <url>http://www.aclweb.org/anthology/W17-2004</url>
    <abstract>In this paper, we study how humans perceive the use of images as an additional
	knowledge source to machine-translate user-generated product listings in an
	e-commerce company. We conduct a human evaluation where we assess how a
	multi-modal neural machine translation (NMT) model compares to two text-only
	approaches: a conventional state-of-the-art attention-based NMT and a
	phrase-based statistical machine translation (PBSMT) model. We evaluate
	translations obtained with different systems and also discuss the data set of
	user-generated product listings, which in our case comprises both product
	listings and associated images. We found that humans preferred translations
	obtained with a PBSMT system to both text-only and multi-modal NMT over 56% of
	the time. Nonetheless, human evaluators ranked translations from a multi-modal
	NMT model as better than those of a text-only NMT over 88% of the time, which
	suggests that images do help NMT in this use-case.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>calixto-EtAl:2017:VL17</bibkey>
  </paper>

  <paper id="2005">
    <title>The BreakingNews Dataset</title>
    <author><first>Arnau</first><last>Ramisa</last></author>
    <author><first>Fei</first><last>Yan</last></author>
    <author><first>Francesc</first><last>Moreno-Noguer</last></author>
    <author><first>Krystian</first><last>Mikolajczyk</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38&#8211;39</pages>
    <url>http://www.aclweb.org/anthology/W17-2005</url>
    <abstract>We present BreakingNews, a novel dataset with approximately 100K news articles
	including images, text and captions, and enriched with heterogeneous meta-data
	(e.g. GPS coordinates and popularity metrics). The tenuous connection between
	the images and text in news data is appropriate to take work at the
	intersection of Computer Vision and Natural Language Processing to the next
	step, hence we hope this dataset will help spur progress in the field.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>ramisa-EtAl:2017:VL17</bibkey>
  </paper>

  <paper id="2006">
    <title>Automatic identification of head movements in video-recorded conversations: can words help?</title>
    <author><first>Patrizia</first><last>Paggio</last></author>
    <author><first>Costanza</first><last>Navarretta</last></author>
    <author><first>Bart</first><last>Jongejan</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>40&#8211;42</pages>
    <url>http://www.aclweb.org/anthology/W17-2006</url>
    <abstract>We present an approach where an SVM classifier learns to classify head
	movements based on measurements of velocity, acceleration, and the third
	derivative of position with respect to time, jerk. Consequently,
	annotations of head movements are added to new video data. The results of the
	automatic annotation are evaluated against manual annotations in the same data
	and show an accuracy of 68% with respect to these. The results also show that
	using jerk improves accuracy. We then conduct an investigation of the
	overlap between temporal sequences classified as either movement or
	non-movement and the speech
	stream of the person performing the gesture. The statistics derived from this
	analysis show that using word features may help increase the accuracy of the
	model.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>paggio-navarretta-jongejan:2017:VL17</bibkey>
  </paper>

  <paper id="2007">
    <title>Multi-Modal Fashion Product Retrieval</title>
    <author><first>Antonio</first><last>Rubio Romano</last></author>
    <author><first>LongLong</first><last>Yu</last></author>
    <author><first>Edgar</first><last>Simo-Serra</last></author>
    <author><first>Francesc</first><last>Moreno-Noguer</last></author>
    <booktitle>Proceedings of the Sixth Workshop on Vision and Language</booktitle>
    <month>April</month>
    <year>2017</year>
    <address>Valencia, Spain</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43&#8211;45</pages>
    <url>http://www.aclweb.org/anthology/W17-2007</url>
    <abstract>Finding a product in the fashion world can be a daunting task. Everyday,
	e-commerce sites are updating with thousands of images and their associated
	metadata (textual information), deepening the problem. In this paper, we
	leverage both the images and textual metadata and propose a joint multi-modal
	embedding that maps both the text and images into a common latent space.
	Distances in the latent space correspond to similarity between products,
	allowing us to effectively perform retrieval in this latent space. We compare
	against existing approaches and show significant improvements in retrieval
	tasks on a large-scale e-commerce dataset.</abstract>
    <bibtype>inproceedings</bibtype>
    <bibkey>rubioromano-EtAl:2017:VL17</bibkey>
  </paper>

</volume>

