<?xml version='1.0' encoding='UTF-8'?>
<volume id="W19">
   <paper id="0100">
        <title>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</title>
        <editor><first>Gaja</first><last>Jarosz</last></editor>
        <editor><first>Max</first><last>Nelson</last></editor>
        <editor><first>Brendan</first><last>O’Connor</last></editor>
        <editor><first>Joe</first><last>Pater</last></editor>
        <pages>i-ii</pages>
        <doi>10.7275/ntf6-xx21</doi>
   </paper>

   <paper id="0101">
        <title>Can Entropy Explain Successor Surprisal Effects in Reading?</title>
        <author><first>Marten</first><last>van Schijndel</last></author>
        <author><first>Tal</first><last>Linzen</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>1-7</pages>
        <doi>10.7275/qtbb-9d05</doi>
   </paper>

   <paper id="0102">
        <title><fixed-case>R</fixed-case>ed<fixed-case>T</fixed-case>yp: A Database of Reduplication with Computational Models</title>
        <author><first>Hossep</first><last>Dolatian</last></author>
        <author><first>Jeffrey</first><last>Heinz</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>8-18</pages>
        <doi>10.7275/ckx7-s770</doi>
   </paper>

   <paper id="0103">
        <title>Unsupervised Learning of Cross-Lingual Symbol Embeddings Without Parallel Data</title>
        <author><first>Mark</first><last>Granroth-Wilding</last></author>
        <author><first>Hannu</first><last>Toivonen</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>19-28</pages>
        <doi>10.7275/wx64-ea83</doi>
   </paper>

   <paper id="0104">
        <title>Q-Theory Representations are Logically Equivalent to Autosegmental Representations</title>
        <author><first>Nick</first><last>Danis</last></author>
        <author><first>Adam</first><last>Jardine</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>29-38</pages>
        <doi>10.7275/tvj1-k306</doi>
   </paper>

   <paper id="0105">
        <title>Modeling Clausal Complementation for a Grammar Engineering Resource</title>
        <author><first>Olga</first><last>Zamaraeva</last></author>
        <author><first>Kristen</first><last>Howell</last></author>
        <author><first>Emily M.</first><last>Bender</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>39-49</pages>
        <doi>10.7275/dygn-c796</doi>
   </paper>

   <paper id="0106">
        <title>Do <fixed-case>RNN</fixed-case>s learn human-like abstract word order preferences?</title>
        <author><first>Richard</first><last>Futrell</last></author>
        <author><first>Roger P.</first><last>Levy</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>50-59</pages>
        <doi>10.7275/jb34-9986</doi>
   </paper>

   <paper id="0107">
        <title>Segmentation and <fixed-case>UR</fixed-case> Acquisition with <fixed-case>UR</fixed-case> Constraints </title>
        <author><first>Max</first><last>Nelson</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>60-68</pages>
        <doi>10.7275/zc9d-pn56</doi>
   </paper>

   <paper id="0108">
        <title>Constraint breeding during on-line incremental learning</title>
        <author><first>Elliot</first><last>Moreton</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>69-80</pages>
        <doi>10.7275/6f9x-6411</doi>
   </paper>

   <paper id="0109">
        <title>An Incremental Iterated Response Model of Pragmatics</title>
        <author><first>Reuben</first><last>Cohn-Gordon</last></author>
        <author><first>Noah</first><last>Goodman</last></author>
        <author><first>Christopher</first><last>Potts</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>81-90</pages>
        <doi>10.7275/cprc-8x17</doi>
   </paper>

   <paper id="0110">
        <title>Learning Exceptionality and Variation with Lexically Scaled <fixed-case>M</fixed-case>ax<fixed-case>E</fixed-case>nt</title>
        <author><first>Coral</first><last>Hughto</last></author>
        <author><first>Andrew</first><last>Lamont</last></author>
        <author><first>Brandon</first><last>Prickett</last></author>
        <author><first>Gaja</first><last>Jarosz</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>91-101</pages>
        <doi>10.7275/y68s-kh12</doi>
   </paper>

   <paper id="0111">
        <title>Learning complex inflectional paradigms through blended gradient inputs</title>
        <author><first>Eric R.</first><last>Rosen</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>102-112</pages>
        <doi>10.7275/ccwf-j606</doi>
   </paper>

   <paper id="0112">
        <title>Jabberwocky Parsing: Dependency Parsing with Lexical Noise</title>
        <author><first>Jungo</first><last>Kasai</last></author>
        <author><first>Robert</first><last>Frank</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>113-123</pages>
        <doi>10.7275/h12q-k754</doi>
   </paper>

   <paper id="0113">
        <title>Learnability and Overgeneration in Computational Syntax</title>
        <author><first>Yiding</first><last>Hao</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>124-134</pages>
        <doi>10.7275/1qmz-bg76</doi>
   </paper>

   <paper id="0114">
        <title>A Conceptual Spaces Model of Socially Motivated Language Change</title>
        <author><first>Heather</first><last>Burnett</last></author>
        <author><first>Olivier</first><last>Bonami</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>135-145</pages>
        <doi>10.7275/5vmp-cs05</doi>
   </paper>

   <paper id="0115">
        <title>Identifying Participation of Individual Verbs or <fixed-case>V</fixed-case>erb<fixed-case>N</fixed-case>et Classes in the Causative Alternation</title>
        <author><first>Esther</first><last>Seyffarth</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>146-155</pages>
        <doi>10.7275/efvz-jy59</doi>
   </paper>

   <paper id="0116">
        <title>Using Sentiment Induction to Understand Variation in Gendered Online Communities</title>
        <author><first>Lucy</first><last>Li</last></author>
        <author><first>Julia</first><last>Mendelsohn</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>156-166</pages>
        <doi>10.7275/11wq-ep51</doi>
   </paper>

   <paper id="0117">
        <title>On the difficulty of a distributional semantics of spoken language</title>
        <author><first>Grzegorz</first><last>Chrupała</last></author>
        <author><first>Lieke</first><last>Gelderloos</last></author>
        <author><first>Ákos</first><last>Kádár</last></author>
        <author><first>Afra</first><last>Alishahi</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>167-173</pages>
        <doi>10.7275/extq-7546</doi>
   </paper>

   <paper id="0118">
        <title>Distributional Effects of Gender Contrasts Across Categories</title>
        <author><first>Timothee</first><last>Mickus</last></author>
        <author><first>Olivier</first><last>Bonami</last></author>
        <author><first>Denis</first><last>Paperno</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>174-184</pages>
        <doi>10.7275/g11b-3s25</doi>
   </paper>

   <paper id="0119">
        <title>Guess Who’s Coming (and Who’s Going): Bringing Perspective to the Rational Speech Acts Framework</title>
        <author><first>Carolyn Jane</first><last>Anderson</last></author>
        <author><first>Brian W.</first><last>Dillon</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>185-194</pages>
        <doi>10.7275/9bn3-8x38</doi>
   </paper>

   <paper id="0120">
        <title>The organization of sound inventories: A study on obstruent gaps</title>
        <author><first>Sheng-Fu</first><last>Wang</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>195-204</pages>
        <doi>10.7275/pbe8-zf60</doi>
   </paper>

   <paper id="0121">
        <title>C-Command Dependencies as <fixed-case>TSL</fixed-case> String Constraints</title>
        <author><first>Thomas</first><last>Graf</last></author>
        <author><first>Nazila</first><last>Shafiei</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>205-215</pages>
        <doi>10.7275/4rrx-x488</doi>
   </paper>

   <paper id="0122">
        <title>Modeling the Acquisition of Words with Multiple Meanings</title>
        <author><first>Libby</first><last>Barak</last></author>
        <author><first>Sammy</first><last>Floyd</last></author>
        <author><first>Adele</first><last>Goldberg</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>216-225</pages>
        <doi>10.7275/tr21-m273</doi>
   </paper>

   <paper id="0123">
        <title>Evaluation Order Effects in Dynamic Continuized <fixed-case>CCG</fixed-case>: From Negative Polarity Items to Balanced Punctuation</title>
        <author><first>Michael</first><last>White</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>226-235</pages>
        <doi>10.7275/kpch-rk05</doi>
   </paper>

   <paper id="0124">
        <title>Abstract Meaning Representation for Human-Robot Dialogue</title>
        <author><first>Claire N.</first><last>Bonial</last></author>
        <author><first>Lucia</first><last>Donatelli</last></author>
        <author><first>Jessica</first><last>Ervin</last></author>
        <author><first>Clare R.</first><last>Voss</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>236-246</pages>
        <doi>10.7275/v3c5-yd35</doi>
   </paper>

   <paper id="0125">
        <title>A Logical and Computational Methodology for Exploring Systems of Phonotactic Constraints</title>
        <author><first>Dakotah</first><last>Lambert</last></author>
        <author><first>James</first><last>Rogers</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>247-256</pages>
        <doi>10.7275/t0dv-9t05</doi>
   </paper>

   <paper id="0126">
        <title>Augmentic Compositional Models for Knowledge Base Completion Using Gradient Representations</title>
        <author><first>Matthias R.</first><last>Lalisse</last></author>
        <author><first>Paul</first><last>Smolensky</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>257-266</pages>
        <doi>10.7275/8et8-qd83</doi>
   </paper>

   <paper id="0127">
        <title>Case assignment in <fixed-case>TSL</fixed-case> syntax: a case study</title>
        <author><first>Mai Ha</first><last>Vu</last></author>
        <author><first>Nazila</first><last>Shafiei</last></author>
        <author><first>Thomas</first><last>Graf</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>267-276</pages>
        <doi>10.7275/sywz-xw23</doi>
   </paper>

   <paper id="0128">
        <title>On Evaluating the Generalization of <fixed-case>LSTM</fixed-case> Models in Formal Languages</title>
        <author><first>Mirac</first><last>Suzgun</last></author>
        <author><first>Yonatan</first><last>Belinkov</last></author>
        <author><first>Stuart M.</first><last>Shieber</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>277-286</pages>
        <doi>10.7275/s02b-4d91</doi>
   </paper>

   <paper id="0129">
        <title>Verb Argument Structure Alternations in Word and Sentence Embeddings</title>
        <author><first>Katharina</first><last>Kann</last></author>
        <author><first>Alex</first><last>Warstadt</last></author>
        <author><first>Adina</first><last>Williams</last></author>
        <author><first>Samuel R.</first><last>Bowman</last></author>
        <booktitle>Proceedings of the Society for Computation in Linguistics (<fixed-case>SC</fixed-case>i<fixed-case>L</fixed-case>) 2019</booktitle>
        <pages>287-297</pages>
        <doi>10.7275/q5js-4y86</doi>
   </paper>

  <paper id="0300">
    <title>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</title>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>http://www.aclweb.org/anthology/W19-03</url>
    <bibtype>book</bibtype>
    <bibkey>IWCLUL-2019:2019</bibkey>
  </paper>

  <paper id="0301">
    <title>Data-Driven Morphological Analysis for Uralic Languages</title>
    <author><first>Miikka</first><last>Silfverberg</last></author>
    <author><first>Francis</first><last>Tyers</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–14</pages>
    <url>http://www.aclweb.org/anthology/W19-0301</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>silfverberg-tyers:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0302">
    <title>North Sámi morphological segmentation with low-resource semi-supervised sequence labeling</title>
    <author><first>Stig-Arne</first><last>Grönroos</last></author>
    <author><first>Sami</first><last>Virpioja</last></author>
    <author><first>Mikko</first><last>Kurimo</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>15–26</pages>
    <url>http://www.aclweb.org/anthology/W19-0302</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>gronroos-virpioja-kurimo:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0303">
    <title>What does the Nom say? An algorithm for case disambiguation in <fixed-case>H</fixed-case>ungarian</title>
    <author><first>Noémi</first><last>Ligeti-Nagy</last></author>
    <author><first>Andrea</first><last>Dömötör</last></author>
    <author><first>Noémi</first><last>Vadász</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>27–41</pages>
    <url>http://www.aclweb.org/anthology/W19-0303</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ligetinagy-domotor-vadasz:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0304">
    <title>A Contrastive Evaluation of Word Sense Disambiguation Systems for <fixed-case>F</fixed-case>innish</title>
    <author><first>Frankie</first><last>Robertson</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>42–54</pages>
    <url>http://www.aclweb.org/anthology/W19-0304</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>robertson:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0305">
    <title>Elliptical Constructions in <fixed-case>E</fixed-case>stonian <fixed-case>UD</fixed-case> Treebank</title>
    <author><first>Kadri</first><last>Muischnek</last></author>
    <author><first>Liisi</first><last>Torga</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–65</pages>
    <url>http://www.aclweb.org/anthology/W19-0305</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>muischnek-torga:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0306">
    <title><fixed-case>F</fixed-case>i<fixed-case>ST</fixed-case> – towards a free Semantic Tagger of modern standard <fixed-case>F</fixed-case>innish</title>
    <author><first>Kimmo</first><last>Kettunen</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>66–76</pages>
    <url>http://www.aclweb.org/anthology/W19-0306</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kettunen:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0307">
    <title>An <fixed-case>OCR</fixed-case> system for the Unified Northern Alphabet</title>
    <author><first>Niko</first><last>Partanen</last></author>
    <author><first>Michael</first><last>Rießler</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>77–89</pages>
    <url>http://www.aclweb.org/anthology/W19-0307</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>partanen-rieler:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0308">
    <title><fixed-case>ELAN</fixed-case> as a search engine for hierarchically structured, tagged corpora</title>
    <author><first>Joshua</first><last>Wilbur</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>90–103</pages>
    <url>http://www.aclweb.org/anthology/W19-0308</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wilbur:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0309">
    <title>Neural and rule-based <fixed-case>F</fixed-case>innish <fixed-case>NLP</fixed-case> models—expectations, experiments and experiences</title>
    <author><first>Tommi A.</first><last>Pirinen</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>104–114</pages>
    <url>http://www.aclweb.org/anthology/W19-0309</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pirinen:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0310">
    <title>Uralic multimedia corpora: <fixed-case>ISO</fixed-case>/<fixed-case>TEI</fixed-case> corpus data in the project <fixed-case>INEL</fixed-case></title>
    <author><first>Timofey</first><last>Arkhangelskiy</last></author>
    <author><first>Anne</first><last>Ferger</last></author>
    <author><first>Hanna</first><last>Hedeland</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>115–124</pages>
    <url>http://www.aclweb.org/anthology/W19-0310</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>arkhangelskiy-ferger-hedeland:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0311">
    <title>Corpora of social media in minority Uralic languages</title>
    <author><first>Timofey</first><last>Arkhangelskiy</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>125–140</pages>
    <url>http://www.aclweb.org/anthology/W19-0311</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>arkhangelskiy:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0312">
    <title>Is this the end? Two-step tokenization of sentence boundaries</title>
    <author><first>Linda</first><last>Wiechetek</last></author>
    <author><first>Sjur Nørstebø</first><last>Moshagen</last></author>
    <author><first>Thomas</first><last>Omma</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>141–153</pages>
    <url>http://www.aclweb.org/anthology/W19-0312</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>wiechetek-moshagen-omma:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0313">
    <title>Learning multilingual topics through aspect extraction from monolingual texts</title>
    <author><first>Johannes</first><last>Huber</last></author>
    <author><first>Myra</first><last>Spiliopoulou</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>154–183</pages>
    <url>http://www.aclweb.org/anthology/W19-0313</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>huber-spiliopoulou:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0314">
    <title>Electronical resources for Livonian</title>
    <author><first>Valts</first><last>Ernštreits</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>184–191</pages>
    <url>http://www.aclweb.org/anthology/W19-0314</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>ernvstreits:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0315">
    <title>The use of Extract Morphology for Automatic Generation of Language Technology for Votic</title>
    <author><first>Kristian</first><last>Kankainen</last></author>
    <booktitle>Proceedings of the Fifth International Workshop on Computational Linguistics for Uralic Languages</booktitle>
    <month>January</month>
    <year>2019</year>
    <address>Tartu, Estonia</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>192–204</pages>
    <url>http://www.aclweb.org/anthology/W19-0315</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kankainen:2019:IWCLUL-2019</bibkey>
  </paper>

  <paper id="0400">
    <title>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</title>
    <author><first>Simon</first><last>Dobnik</last></author>
    <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-04</url>
    <bibtype>proceedings</bibtype>
    <bibkey>W19-0400</bibkey>
  </paper>

  <paper id="0401">
    <title>Projecting Temporal Properties, Events and Actions</title>
    <author><first>Tim</first><last>Fernando</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–12</pages>
    <abstract>Temporal notions based on a finite set <tex-math>A</tex-math> of properties are represented in strings, on which projections are defined that vary the granularity <tex-math>A</tex-math>. The structure of properties in <tex-math>A</tex-math> is elaborated to describe statives, events and actions, subject to a distinction in meaning (advocated by Levin and Rappaport Hovav) between what the lexicon prescribes and what a context of use supplies. The projections proposed are deployed as labels for records and record types amenable to finite-state methods.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0401</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0401</bibkey>
  </paper>

  <paper id="0402">
    <title>A Type-coherent, Expressive Representation as an Initial Step to Language Understanding</title>
    <author><first>Gene Louis</first><last>Kim</last></author>
    <author><first>Lenhart</first><last>Schubert</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>13–30</pages>
    <abstract>A growing interest in tasks involving language understanding by the NLP community has led to the need for effective semantic parsing and inference. Modern NLP systems use semantic representations that do not quite fulfill the nuanced needs for language understanding: adequately modeling language semantics, enabling general inferences, and being accurately recoverable. This document describes underspecified logical forms~(ULF) for Episodic Logic~(EL), which is an initial form for a semantic representation that balances these needs. ULFs fully resolve the semantic type structure while leaving issues such as quantifier scope, word sense, and anaphora unresolved; they provide a starting point for further resolution into EL, and enable certain structural inferences without further resolution. This document also presents preliminary results of creating a hand-annotated corpus of ULFs for the purpose of training a precise ULF parser, showing a three-person pairwise interannotator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0402</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0402</bibkey>
  </paper>

  <paper id="0403">
    <title>A Semantic Annotation Scheme for Quantification</title>
    <author><first>Harry</first><last>Bunt</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>31–42</pages>
    <abstract>This paper describes in brief the proposal called ‘QuantML’ which was accepted by the International Organisation for Standards (ISO) last February as a starting point for developing a standard for the interoperable annotation of quantification phenomena in natural language, as part of the ISO 24617 Semantic Annotation Framework. The proposal, firmly rooted in the theory of generalised quantifiers, neo-Davidsonian semantics, and DRT, covers a wide range of quantification phenomena. The QuantML scheme consists of (1) an abstract syntax which defines ‘annotation structures’ as triples and other set-theoretic constructs; (b) a compositional semantics of annotation structures; (3) an XML representation of annotation structures.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0403</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0403</bibkey>
  </paper>

  <paper id="0404">
    <title>Re-Ranking Words to Improve Interpretability of Automatically Generated Topics</title>
    <author><first>Areej</first><last>Alokaili</last></author>
    <author><first>Nikolaos</first><last>Aletras</last></author>
    <author><first>Mark</first><last>Stevenson</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>43–54</pages>
    <abstract>Topics models, such as LDA, are widely used in Natural Language Processing. Making their output interpretable is an important area of research with applications to areas such as the enhancement of exploratory search interfaces and the development of interpretable machine learning models. Conventionally, topics are represented by their n most probable words, however, these representations are often difficult for humans to interpret. This paper explores the re-ranking of topic words to generate more interpretable topic representations. A range of approaches are compared and evaluated in two experiments. The first uses crowdworkers to associate topics represented by different word rankings with related documents. The second experiment is an automatic approach based on a document retrieval task applied on multiple domains. Results in both experiments demonstrate that re-ranking words improves topic interpretability and that the most effective re-ranking schemes were those which combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0404</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0404</bibkey>
  </paper>

  <paper id="0405">
    <title>An Improved Approach for Semantic Graph Composition with <fixed-case>CCG</fixed-case></title>
    <author><first>Austin</first><last>Blodgett</last></author>
    <author><first>Nathan</first><last>Schneider</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–70</pages>
    <abstract>This paper builds on previous work using Combinatory Categorial Grammar (CCG) to derive a transparent syntax-semantics interface for Abstract Meaning Representation (AMR) parsing. We define new semantics for the CCG combinators that is better suited to deriving AMR graphs. In particular, we define relation-wise alternatives for the application and composition combinators: these require that the two constituents being combined overlap in one AMR relation. We also provide a new semantics for type raising, which is necessary for certain constructions. Using these mechanisms, we suggest an analysis of eventive nouns, which present a challenge for deriving AMR graphs. Our theoretical analysis will facilitate future work on robust and transparent AMR parsing using CCG.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0405</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0405</bibkey>
  </paper>

  <paper id="0406">
    <title>A Semantic Ontology of <fixed-case>D</fixed-case>anish Adjectives</title>
    <author><first>Eckhard</first><last>Bick</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>71–78</pages>
    <abstract>This paper presents a semantic annotation scheme for Danish adjectives, focusing both on prototypical semantic content and semantic collocational restrictions on an adjective’s head noun. The core type set comprises about 110 categories ordered in a shallow hierarchy with 14 primary and 25 secondary umbrella categories. In addition, domain information and binary sentiment tags are provided, as well as VerbNet-derived frames and semantic roles for those adjectives governing arguments. The scheme has been almost fully implemented on the lexicon of the Danish VISL parser, DanGram, containing 14,000 adjectives. We discuss the annotation scheme and its applicational perspectives, and present a statistical breakdown and coverage evaluation for three Danish reference corpora.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0406</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0406</bibkey>
  </paper>

  <paper id="0407">
    <title>Towards a Compositional Analysis of <fixed-case>G</fixed-case>erman Light Verb Constructions (<fixed-case>LVC</fixed-case>s) Combining Lexicalized Tree Adjoining Grammar (<fixed-case>LTAG</fixed-case>) with Frame Semantics</title>
    <author><first>Jens</first><last>Fleischhauer</last></author>
    <author><first>Thomas</first><last>Gamerschlag</last></author>
    <author><first>Laura</first><last>Kallmeyer</last></author>
    <author><first>Simon</first><last>Petitjean</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>79–90</pages>
    <abstract>Complex predicates formed of a semantically ‘light’ verbal head and a noun or verb which contributes the major part of the meaning are frequently referred to as ‘light verb constructions’ (LVCs). In the paper, we present a case study of LVCs with the German posture verb stehen ‘stand’. In our account, we model the syntactic as well as semantic composition of such LVCs by combining Lexicalized Tree Adjoining Grammar (LTAG) with frames. Starting from the analysis of the literal uses of posture verbs, we show how the meaning components of the literal uses are systematically exploited in the interpretation of stehen-LVCs. The paper constitutes an important step towards a compositional and computational analysis of LVCs. We show that LTAG allows us to separate constructional from lexical meaning components and that frames enable elegant generalizations over event types and related constraints.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0407</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0407</bibkey>
  </paper>

  <paper id="0408">
    <title>Words are Vectors, Dependencies are Matrices: Learning Word Embeddings from Dependency Graphs</title>
    <author><first>Paula</first><last>Czarnowska</last></author>
    <author><first>Guy</first><last>Emerson</last></author>
    <author><first>Ann</first><last>Copestake</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>91–102</pages>
    <abstract>Distributional Semantic Models (DSMs) construct vector representations of word meanings based on their contexts. Typically, the contexts of a word are defined as its closest neighbours, but they can also be retrieved from its syntactic dependency relations. In this work, we propose a new dependency-based DSM. The novelty of our model lies in associating an independent meaning representation, a matrix, with each dependency-label. This allows it to capture specifics of the relations between words and contexts, leading to good performance on both intrinsic and extrinsic evaluation tasks. In addition to that, our model has an inherent ability to represent dependency chains as products of matrices which provides a straightforward way of handling further contexts of a word.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0408</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0408</bibkey>
  </paper>

  <paper id="0409">
    <title>Temporal and Aspectual Entailment</title>
    <author><first>Thomas</first><last>Kober</last></author>
    <author><first>Sander Bijl</first><last>de Vroe</last></author>
    <author><first>Mark</first><last>Steedman</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>103–119</pages>
    <abstract>Inferences regarding “Jane’s arrival in London” from predications such as “Jane is going to London” or “Jane has gone to London” depend on tense and aspect of the predications. Tense determines the temporal location of the predication in the past, present or future of the time of utterance. The aspectual auxiliaries on the other hand specify the internal constituency of the event, i.e. whether the event of “going to London” is completed and whether its consequences hold at that time or not. While tense and aspect are among the most important factors for determining natural language inference, there has been very little work to show whether modern embedding models capture these semantic concepts. In this paper we propose a novel entailment dataset and analyse the ability of contextualised word representations to perform inference on predications across aspectual types and tenses. We show that they encode a substantial amount of information relating to tense and aspect, but fail to consistently model inferences that require reasoning with these semantic properties.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0409</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0409</bibkey>
  </paper>

  <paper id="0410">
    <title>Don’t Blame Distributional Semantics if it can’t do Entailment</title>
    <author><first>Matthijs</first><last>Westera</last></author>
    <author><first>Gemma</first><last>Boleda</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>120–133</pages>
    <abstract>Distributional semantics has had enormous empirical success in Computational Linguistics and Cognitive Science in modeling various semantic phenomena, such as semantic similarity, and distributional models are widely used in state-of-the-art Natural Language Processing systems. However, the theoretical status of distributional semantics within a broader theory of language and cognition is still unclear: What does distributional semantics model? Can it be, on its own, a fully adequate model of the meanings of linguistic expressions? The standard answer is that distributional semantics is not fully adequate in this regard, because it falls short on some of the central aspects of formal semantic approaches: truth conditions, entailment, reference, and certain aspects of compositionality. We argue that this standard answer rests on a misconception: These aspects do not belong in a theory of expression meaning, they are instead aspects of speaker meaning, i.e., communicative intentions in a particular context. In a slogan: words do not refer, speakers do. Clearing this up enables us to argue that distributional semantics on its own is an adequate model of expression meaning. Our proposal sheds light on the role of distributional semantics in a broader theory of language and cognition, its relationship to formal semantics, and its place in computational models.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0410</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0410</bibkey>
  </paper>

  <paper id="0411">
    <title>Ambiguity in Explicit Discourse Connectives</title>
    <author><first>Bonnie</first><last>Webber</last></author>
    <author><first>Rashmi</first><last>Prasad</last></author>
    <author><first>Alan</first><last>Lee</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>134–141</pages>
    <abstract>Discourse connectives are known to be subject to both usage and sense ambiguity, as has already been discussed in the literature. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four are illustrated and discussed here.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0411</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0411</bibkey>
  </paper>

  <paper id="0412">
    <title>Aligning Open <fixed-case>IE</fixed-case> Relations and <fixed-case>KB</fixed-case> Relations using a <fixed-case>S</fixed-case>iamese Network Based on Word Embedding</title>
    <author><first>Rifki Afina</first><last>Putri</last></author>
    <author><first>Giwon</first><last>Hong</last></author>
    <author><first>Sung-Hyon</first><last>Myaeng</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>142–153</pages>
    <abstract>Open Information Extraction (Open IE) aims at generating entity-relation-entity triples from a large amount of text, aiming at capturing key semantics of the text. Given a triple, the relation expresses the type of semantic relation between the entities. Although relations from an Open IE system are more extensible than those used in a traditional Information Extraction system and a Knowledge Base (KB) such as Knowledge Graphs, the former lacks in semantics; an Open IE relation is simply a sequence of words, whereas a KB relation has a predefined meaning. As a way to provide a meaning to an Open IE relation, we attempt to align it with one of the predefined set of relations used in a KB. Our approach is to use a Siamese network that compares two sequences of word embeddings representing an Open IE relation and a predefined KB relation. In order to make the approach practical, we automatically generate a training dataset using a distant supervision approach instead of relying on a hand-labeled dataset. Our experiment shows that the proposed method can capture the relational semantics better than the recent approaches.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0412</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0412</bibkey>
  </paper>

  <paper id="0413">
    <title>Language-Agnostic Model for Aspect-Based Sentiment Analysis</title>
    <author><first>Md Shad</first><last>Akhtar</last></author>
    <author><first>Abhishek</first><last>Kumar</last></author>
    <author><first>Asif</first><last>Ekbal</last></author>
    <author><first>Chris</first><last>Biemann</last></author>
    <author><first>Pushpak</first><last>Bhattacharyya</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>154–164</pages>
    <abstract>In this paper, we propose a language-agnostic deep neural network architecture for aspect-based sentiment analysis. The proposed approach is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network, which is further assisted with extra hand-crafted features. We define three different architectures for the successful combination of word embeddings and hand-crafted features. We evaluate the proposed approach for six languages (i.e. English, Spanish, French, Dutch, German and Hindi) and two problems (i.e. aspect term extraction and aspect sentiment classification). Experiments show that the proposed model attains state-of-the-art performance in most of the settings.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0413</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0413</bibkey>
  </paper>

  <paper id="0414">
    <title>The Effect of Context on Metaphor Paraphrase Aptness Judgments</title>
    <author><first>Yuri</first><last>Bizzoni</last></author>
    <author><first>Shalom</first><last>Lappin</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>165–175</pages>
    <abstract>We conduct two experiments to study the effect of context on metaphor paraphrase aptness judgments. The first is an AMT crowd source task in which speakers rank metaphor-paraphrase candidate sentence pairs in short document contexts for paraphrase aptness. In the second we train a composite DNN to predict these human judgments, first in binary classifier mode, and then as gradient ratings. We found that for both mean human judgments and our DNN’s predictions, adding document context compresses the aptness scores towards the center of the scale, raising low out-of-context ratings and decreasing high out-of-context scores. We offer a provisional explanation for this compression effect.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0414</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0414</bibkey>
  </paper>

  <paper id="0415">
    <title>Predicting Word Concreteness and Imagery</title>
    <author><first>Jean</first><last>Charbonnier</last></author>
    <author><first>Christian</first><last>Wartena</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>176–187</pages>
    <abstract>Concreteness of words has been studied extensively in psycholinguistic literature. A number of datasets have been created with average values for perceived concreteness of words. We show that we can train a regression model on these data, using word embeddings and morphological features, that can predict these concreteness values with high accuracy. We evaluate the model on 7 publicly available datasets. Only for a few small subsets of these datasets prediction of concreteness values are found in the literature. Our results clearly outperform the reported results for these datasets.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0415</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0415</bibkey>
  </paper>

  <paper id="0416">
    <title>Learning to Explicitate Connectives with <fixed-case>S</fixed-case>eq2<fixed-case>S</fixed-case>eq Network for Implicit Discourse Relation Classification</title>
    <author><first>Wei</first><last>Shi</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>188–199</pages>
    <abstract>Implicit discourse relation classification is one of the most difficult steps in discourse parsing. The difficulty stems from the fact that the coherence relation must be inferred based on the content of the discourse relational arguments. Therefore, an effective encoding of the relational arguments is of crucial importance. We here propose a new model for implicit discourse relation classification, which consists of a classifier, and a sequence-to-sequence model which is trained to generate a representation of the discourse relational arguments by trying to predict the relational arguments including a suitable implicit connective. Training is possible because such implicit connectives have been annotated as part of the PDTB corpus. Along with a memory network, our model could generate more refined representations for the task. And on the now standard 11-way classification, our method outperforms the previous state of the art systems on the PDTB benchmark on multiple settings including cross validation.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0416</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0416</bibkey>
  </paper>

  <paper id="0417">
    <title>Cross-Lingual Transfer of Semantic Roles: From Raw Text to Semantic Roles</title>
    <author><first>Maryam</first><last>Aminian</last></author>
    <author><first>Mohammad Sadegh</first><last>Rasooli</last></author>
    <author><first>Mona</first><last>Diab</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>200–210</pages>
    <abstract>We describe a transfer method based on annotation projection to develop a dependency-based semantic role labeling system for languages for which no supervised linguistic information other than parallel data is available. Unlike previous work that presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0417</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0417</bibkey>
  </paper>

  <paper id="0418">
    <title>Evaluating the Representational Hub of Language and Vision Models</title>
    <author><first>Ravi</first><last>Shekhar</last></author>
    <author><first>Ece</first><last>Takmaz</last></author>
    <author><first>Raquel</first><last>Fernández</last></author>
    <author><first>Raffaella</first><last>Bernardi</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>211–222</pages>
    <abstract>The multimodal models used in the emerging field at the intersection of computational linguistics and computer vision implement the bottom-up processing of the “Hub and Spoke” architecture proposed in cognitive science to represent how the brain processes and combines multi-sensory inputs. In particular, the Hub is implemented as a neural network encoder. We investigate the effect on this encoder of various vision-and-language tasks proposed in the literature: visual question answering, visual reference resolution, and visually grounded dialogue. To measure the quality of the representations learned by the encoder, we use two kinds of analyses. First, we evaluate the encoder pre-trained on the different vision-and-language tasks on an existing “diagnostic task” designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0418</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0418</bibkey>
  </paper>

  <paper id="0419">
    <title>The Fast and the Flexible: Training Neural Networks to Learn to Follow Instructions from Small Data</title>
    <author><first>Rezka</first><last>Leonandya</last></author>
    <author><first>Dieuwke</first><last>Hupkes</last></author>
    <author><first>Elia</first><last>Bruni</last></author>
    <author><first>Germán</first><last>Kruszewski</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>223–234</pages>
    <abstract>Learning to follow human instructions is a long-pursued goal in artificial intelligence. The task becomes particularly challenging if no prior knowledge of the employed language is assumed while relying only on a handful of examples to learn from. Work in the past has relied on hand-coded components or manually engineered features to provide strong inductive biases that make learning in such situations possible. In contrast, here we seek to establish whether this knowledge can be acquired automatically by a neural network system through a two phase training procedure: A (slow) offline learning stage where the network learns about the general structure of the task and a (fast) online adaptation phase where the network learns the language of a new given speaker. Controlled experiments show that when the network is exposed to familiar instructions but containing novel words, the model adapts very efficiently to the new vocabulary. Moreover, even for human speakers whose language usage can depart significantly from our artificial training language, our network can still make use of its automatically acquired inductive bias to learn to follow instructions more effectively.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0419</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0419</bibkey>
  </paper>

  <paper id="0420">
    <title>Fast and Discriminative Semantic Embedding</title>
    <author><first>Rob</first><last>Koopman</last></author>
    <author><first>Shenghui</first><last>Wang</last></author>
    <author><first>Gwenn</first><last>Englebienne</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>235–246</pages>
    <abstract>The embedding of words and documents in compact, semantically meaningful vector spaces is a crucial part of modern information systems. Deep Learning models are powerful but their hyperparameter selection is often complex and they are expensive to train, and while pre-trained models are available, embeddings trained on general corpora are not necessarily well-suited to domain specific tasks. We propose a novel embedding method which extends random projection by weighting and projecting raw term embeddings orthogonally to an average language vector, thus improving the discriminating power of resulting term embeddings, and build more meaningful document embeddings by assigning appropriate weights to individual terms. We describe how updating the term embeddings online as we process the training data results in an extremely efficient method, in terms of both computational and memory requirements. Our experiments show highly competitive results with various state-of-the-art embedding methods on different tasks, including the standard STS benchmark and a subject prediction task, at a fraction of the computational cost.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0420</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0420</bibkey>
  </paper>

  <paper id="0421">
    <title>Using Multi-Sense Vector Embeddings for Reverse Dictionaries</title>
    <author><first>Michael A.</first><last>Hedderich</last></author>
    <author><first>Andrew</first><last>Yates</last></author>
    <author><first>Dietrich</first><last>Klakow</last></author>
    <author><first>Gerard</first><last>de Melo</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>247–258</pages>
    <abstract>Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0421</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0421</bibkey>
  </paper>

  <paper id="0422">
    <title>Using <fixed-case>W</fixed-case>iktionary as a resource for <fixed-case>WSD</fixed-case> : the case of <fixed-case>F</fixed-case>rench verbs</title>
    <author><first>Vincent</first><last>Segonne</last></author>
    <author><first>Marie</first><last>Candito</last></author>
    <author><first>Benoit</first><last>Crabbé</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>259–270</pages>
    <abstract>As opposed to word sense induction, word sense disambiguation (WSD) has the advantage of us-ing interpretable senses, but requires annotated data, which are quite rare for most languages except English (Miller et al. 1993; Fellbaum, 1998). In this paper, we investigate which strategy to adopt to achieve WSD for languages lacking data that was annotated specifically for the task, focusing on the particular case of verb disambiguation in French. We first study the usability of Eurosense (Bovi et al. 2017) , a multilingual corpus extracted from Europarl (Kohen, 2005) and automatically annotated with BabelNet (Navigli and Ponzetto, 2010) senses. Such a resource opened up the way to supervised and semi-supervised WSD for resourceless languages like French. While this perspective looked promising, our evaluation on French verbs was inconclusive and showed the annotated senses’ quality was not sufficient for supervised WSD on French verbs. Instead, we propose to use Wiktionary, a collaboratively edited, multilingual online dictionary, as a resource for WSD. Wiktionary provides both sense inventory and manually sense tagged examples which can be used to train supervised and semi-supervised WSD systems. Yet, because senses’ distribution differ in lexicographic examples found in Wiktionary with respect to natural text, we then focus on studying the impact on WSD of the training data size and senses’ distribution. Using state-of-the art semi-supervised systems, we report experiments of Wiktionary-based WSD for French verbs, evaluated on FrenchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0422</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0422</bibkey>
  </paper>

  <paper id="0423">
    <title>A Comparison of Context-sensitive Models for Lexical Substitution</title>
    <author><first>Aina Garí</first><last>Soler</last></author>
    <author><first>Anne</first><last>Cocos</last></author>
    <author><first>Marianna</first><last>Apidianaki</last></author>
    <author><first>Chris</first><last>Callison-Burch</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>271–282</pages>
    <abstract>Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks. Embedding approaches differ as to whether and how they account for the context surrounding a word. We present a comparison of different word and context representations on the task of proposing substitutes for a target word in context (lexical substitution). We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word. We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution. This is better handled by models trained with this objective in mind, where the inter-dependence between word and context representations is explicitly modeled during training.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0423</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0423</bibkey>
  </paper>

  <paper id="0424">
    <title>Natural Language Semantics With Pictures: Some Language &amp; Vision Datasets and Potential Uses for Computational Semantics</title>
    <author><first>David</first><last>Schlangen</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>283–294</pages>
    <abstract>Propelling, and propelled by, the “deep learning revolution”, recent years have seen the introduction of ever larger corpora of images annotated with natural language expressions. We survey some of these corpora, taking a perspective that reverses the usual directionality, as it were, by viewing the images as semantic annotation of the natural language expressions. We discuss datasets that can be derived from the corpora, and tasks of potential interest for computational semanticists that can be defined on those. In this, we make use of relations provided by the corpora (namely, the link between expression and image, and that between two expressions linked to the same image) and relations that we can add (similarity relations between expressions, or between images). Specifically, we show that in this way we can create data that can be used to learn and evaluate lexical and compositional grounded semantics, and we show that the “linked to same image” relation tracks a semantic implication relation that is recognisable to annotators even in the absence of the linking image as evidence. Finally, as an example of possible benefits of this approach, we show that an exemplar-model-based approach to implication beats a (simple) distributional space-based one on some derived datasets, while lending itself to explainability.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0424</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0424</bibkey>
  </paper>

  <paper id="0425">
    <title>Frame Identification as Categorization: Exemplars vs Prototypes in Embeddingland</title>
    <author><first>Jennifer</first><last>Sikos</last></author>
    <author><first>Sebastian</first><last>Padó</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Long Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>295–306</pages>
    <abstract>Categorization is a central capability of human cognition, and a number of theories have been developed to account for properties of categorization. Even though many tasks in semantics also involve categorization of some kind, theories of categorization do not play a major role in contemporary research in computational linguistics. This paper follows the idea that embedding-based models of semantics lend themselves well to being formulated in terms of classical categorization theories. The benefit is a space of model families that enables (a) the formulation of hypotheses about the impact of major design decisions, and (b) a transparent assessment of these decisions. We instantiate this idea on the task of frame-semantic frame identification. We define four models that cross two design variables: (a) the choice of prototype vs. exemplar categorization, corresponding to different degrees of generalization applied to the input; and (b) the presence vs. absence of a fine-tuning step, corresponding to generic vs. task-adaptive categorization. We find that for frame identification, generalization and task-adaptive categorization both yield substantial benefits. Our prototype-based, fine-tuned model, which combines the best choices for these variables, establishes a new state of the art in frame identification.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0425</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0425</bibkey>
  </paper>

  <paper id="0500">
    <title>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</title>
    <author><first>Simon</first><last>Dobnik</last></author>
    <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-05</url>
    <bibtype>proceedings</bibtype>
    <bibkey>W19-0500</bibkey>
  </paper>

  <paper id="0501">
    <title>A Distributional Model of Affordances in Semantic Type Coercion</title>
    <author><first>Stephen</first><last>McGregor</last></author>
    <author><first>Elisabetta</first><last>Jezek</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–7</pages>
    <abstract>We explore a novel application for interpreting semantic type coercions, motivated by insight into the role that perceptual affordances play in the selection of the semantic roles of artefactual nouns which are observed as arguments for verbs which would stereotypically select for objects of a different type. In order to simulate affordances, which we take to be direct perceptions of context-specific opportunities for action, we preform a distributional analysis dependency relationships between target words and their modifiers and adjuncts. We use these relationships as the basis for generating on-line transformations which project semantic subspaces in which the interpretations of coercive compositions are expected to emerge as salient word-vectors. We offer some preliminary examples of how this model operates on a dataset of sentences involving coercive interactions between verbs and objects specifically designed to evaluate this work.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0501</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0501</bibkey>
  </paper>

  <paper id="0502">
    <title>Natural Language Inference with Monotonicity</title>
    <author><first>Hai</first><last>Hu</last></author>
    <author><first>Qi</first><last>Chen</last></author>
    <author><first>Larry</first><last>Moss</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>8–15</pages>
    <abstract>This paper describes a working system which performs natural language inference using polarity-marked parse trees. The system handles all of the instances of monotonicity inference in the FraCaS data set. Except for the initial parse, it is entirely deterministic. It handles multi-premise arguments, and the kind of inference performed is essentially “logical”, but it goes beyond what is representable in first-order logic. In any case, the system works on surface forms rather than on representations of any kind.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0502</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0502</bibkey>
  </paper>

  <paper id="0503">
    <title>Distributional Semantics in the Real World: Building Word Vector Representations from a Truth-Theoretic Model</title>
    <author><first>Elizaveta</first><last>Kuzmenko</last></author>
    <author><first>Aurelie</first><last>Herbelot</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>16–23</pages>
    <abstract>Distributional semantics models (DSMs) are known to produce excellent representations of word meaning, which correlate with a range of behavioural data. As lexical representations, they have been said to be fundamentally different from truth-theoretic models of semantics, where meaning is defined as a correspondence relation to the world. There are two main aspects to this difference: a) DSMs are built over corpus data which may or may not reflect ‘what is in the world’; b) they are built from word co-occurrences, that is, from lexical types rather than entities and sets. In this paper, we inspect the properties of a distributional model built over a set-theoretic approximation of ‘the real world’. To achieve this, we take the annotation a large database of images marked with objects, attributes and relations, convert the data into a representation akin to first-order logic and build several distributional models using various combinations of features. We evaluate those models over both relatedness and similarity datasets, demonstrating their effectiveness in standard evaluations. This allows us to conclude that, despite prior claims, truth-theoretic models are good candidates for building graded lexical representations of meaning.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0503</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0503</bibkey>
  </paper>

  <paper id="0504">
    <title>Linguistic Information in Neural Semantic Parsing with Multiple Encoders</title>
    <author><first>Rik</first><last>van Noord</last></author>
    <author><first>Antonio</first><last>Toral</last></author>
    <author><first>Johan</first><last>Bos</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–31</pages>
    <abstract>Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0504</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0504</bibkey>
  </paper>

  <paper id="0505">
    <title>Making Sense of Conflicting (Defeasible) Rules in the Controlled Natural Language <fixed-case>ACE</fixed-case>: Design of a System with Support for Existential Quantification Using Skolemization</title>
    <author><first>Martin</first><last>Diller</last></author>
    <author><first>Adam</first><last>Wyner</last></author>
    <author><first>Hannes</first><last>Strass</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>32–37</pages>
    <abstract>We present the design of a system for making sense of conflicting rules expressed in a fragment of the prominent controlled natural language ACE, yet extended with means of expressing defeasible rules in the form of normality assumptions. The approach we describe is ultimately based on answer-set-programming (ASP); simulating existential quantification by using skolemization in a manner resembling a translation for ASP recently formalized in the context of ∃-ASP. We discuss the advantages of this approach to building on the existing ACE interface to rule-systems, ACERules.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0505</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0505</bibkey>
  </paper>

  <paper id="0506">
    <title>Distributional Interaction of Concreteness and Abstractness in Verb–Noun Subcategorisation</title>
    <author><first>Diego</first><last>Frassinelli</last></author>
    <author><first>Sabine Schulte Im</first><last>Walde</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>38–43</pages>
    <abstract>In recent years, both cognitive and computational research has provided empirical analyses of contextual co-occurrence of concrete and abstract words, partially resulting in inconsistent pictures. In this work we provide a more fine-grained description of the distributional nature in the corpus- based interaction of verbs and nouns within subcategorisation, by investigating the concreteness of verbs and nouns that are in a specific syntactic relationship with each other, i.e., subject, direct object, and prepositional object. Overall, our experiments show consistent patterns in the distributional representation of subcategorising and subcategorised concrete and abstract words. At the same time, the studies reveal empirical evidence why contextual abstractness represents a valuable indicator for automatic non-literal language identification.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0506</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0506</bibkey>
  </paper>

  <paper id="0507">
    <title>Generating a Novel Dataset of Multimodal Referring Expressions</title>
    <author><first>Nikhil</first><last>Krishnaswamy</last></author>
    <author><first>James</first><last>Pustejovsky</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>44–51</pages>
    <abstract>Referring expressions and definite descriptions of objects in space exploit information both about object characteristics and locations. To resolve potential ambiguity, referencing strategies in language can rely on increasingly abstract concepts to distinguish an object in a given location from similar ones elsewhere, yet the description of the intended location may still be imprecise or difficult to interpret. Meanwhile, modalities such as gesture may communicate spatial information such as locations in a more concise manner. In real peer-to-peer communication, humans use language and gesture together to reference entities, with a capacity for mixing and changing modalities where needed. While recent progress in AI and human-computer interaction has created systems where a human can interact with a computer multimodally, computers often lack the capacity to intelligently mix modalities when generating referring expressions. We present a novel dataset of referring expressions combining natural language and gesture, describe its creation and evaluation, and its uses to train computational models for generating and interpreting multimodal referring expressions.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0507</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0507</bibkey>
  </paper>

  <paper id="0508">
    <title>On Learning Word Embeddings From Linguistically Augmented Text Corpora</title>
    <author><first>Amila</first><last>Silva</last></author>
    <author><first>Chathurika</first><last>Amarathunga</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–58</pages>
    <abstract>Word embedding is a technique in Natural Language Processing (NLP) to map words into vector space representations. Since it has boosted the performance of many NLP downstream tasks, the task of learning word embeddings has been addressing significantly. Nevertheless, most of the underlying word embedding methods such as word2vec and GloVe fail to produce high-quality embeddings if the text corpus is small and sparse. This paper proposes a method to generate effective word embeddings from limited data. Through experiments, we show that our proposed model outperforms existing works for the classical word similarity task and for a domain-specific application.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0508</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0508</bibkey>
  </paper>

  <paper id="0509">
    <title>Sentiment Independent Topic Detection in Rated Hospital Reviews</title>
    <author><first>Christian</first><last>Wartena</last></author>
    <author><first>Uwe</first><last>Sander</last></author>
    <author><first>Christiane</first><last>Patzelt</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>59–64</pages>
    <abstract>We present a simple method to find topics in user reviews that accompany ratings for products or services. Standard topic analysis will perform sub-optimal on such data since the word distributions in the documents are not only determined by the topics but by the sentiment as well. We reduce the influence of the sentiment on the topic selection by adding two explicit topics, representing positive and negative sentiment. We evaluate the proposed method on a set of over 15,000 hospital reviews. We show that the proposed method, Latent Semantic Analysis with explicit word features, finds topics with a much smaller bias for sentiments than other similar methods.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0509</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0509</bibkey>
  </paper>

  <paper id="0510">
    <title>Investigating the Stability of Concrete Nouns in Word Embeddings</title>
    <author><first>Bénédicte</first><last>Pierrejean</last></author>
    <author><first>Ludovic</first><last>Tanguy</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Short Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>65–70</pages>
    <abstract>We know that word embeddings trained using neural-based methods (such as word2vec SGNS) are sensitive to stability problems and that across two models trained using the exact same set of parameters, the nearest neighbors of a word are likely to change. All words are not equally impacted by this internal instability and recent studies have investigated features influencing the stability of word embeddings. This stability can be seen as a clue for the reliability of the semantic representation of a word. In this work, we investigate the influence of the degree of concreteness of nouns on the stability of their semantic representation. We show that for English generic corpora, abstract words are more affected by stability problems than concrete words. We also found that to a certain extent, the difference between the degree of concreteness of a noun and its nearest neighbors can partly explain the stability or instability of its neighbors.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0510</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0510</bibkey>
  </paper>

  <paper id="0600">
    <title>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</title>
    <author><first>Simon</first><last>Dobnik</last></author>
    <author><first>Stergios</first><last>Chatzikyriakidis</last></author>
    <author><first>Vera</first><last>Demberg</last></author>
    <author><first>Kathrein</first><last>Abu Kwaik</last></author>
    <author><first>Vladislav</first><last>Maraev</last></author>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-06</url>
    <bibtype>proceedings</bibtype>
    <bibkey>W19-0600</bibkey>
  </paper>

  <paper id="0601">
    <title>A Dynamic Semantics for Causal Counterfactuals</title>
    <author><first>Kenneth</first><last>Lai</last></author>
    <author><first>James</first><last>Pustejovsky</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–8</pages>
    <abstract>Under the standard approach to counterfactuals, to determine the meaning of a counterfactual sentence, we consider the “closest” possible world(s) where the antecedent is true, and evaluate the consequent. Building on the standard approach, some researchers have found that the set of worlds to be considered is dependent on context; it evolves with the discourse. Others have focused on how to define the “distance” between possible worlds, using ideas from causal modeling. This paper integrates the two ideas. We present a semantics for counterfactuals that uses a distance measure based on causal laws, that can also change over time. We show how our semantics can be implemented in the Haskell programming language.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0601</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0601</bibkey>
  </paper>

  <paper id="0602">
    <title>Visual <fixed-case>TTR</fixed-case> - Modelling Visual Question Answering in Type Theory with Records</title>
    <author><first>Ronja</first><last>Utescher</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>9–14</pages>
    <abstract>In this paper, I will describe a system that was developed for the task of Visual Question Answering. The system uses the rich type universe of Type Theory with Records (TTR) to model both the utterances about the image, the image itself and classifications made related to the two. At its most basic, the decision of whether any given predicate can be assigned to an object in the image is delegated to a CNN. Consequently, images can be judged as evidence for propositions. The end result is a model whose application of perceptual classifiers to a given image is guided by the accompanying utterance.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0602</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0602</bibkey>
  </paper>

  <paper id="0603">
    <title>The Lexical Gap: An Improved Measure of Automated Image Description Quality</title>
    <author><first>Austin</first><last>Kershaw</last></author>
    <author><first>Miroslaw</first><last>Bober</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>15–23</pages>
    <abstract>The challenge of automatically describing images and videos has stimulated much research in Computer Vision and Natural Language Processing. In order to test the semantic abilities of new algorithms, we need reliable and objective ways of measuring progress. We show that standard evaluation measures do not take into account the semantic richness of a description, and give the impression that sparse machine descriptions outperform rich human descriptions. We introduce and test a new measure of semantic ability based on relative lexical diversity. We show how our measure can work alongside existing measures to achieve state of the art correlation with human judgement of quality. We also introduce a new dataset: Rich-Sparse Descriptions, which provides 2K human and machine descriptions to stimulate interest into the semantic evaluation of machine descriptions.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0603</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0603</bibkey>
  </paper>

  <paper id="0604">
    <title>Modeling language constructs with fuzzy sets: some approaches, examples and interpretations</title>
    <author><first>Pavlo</first><last>Kapustin</last></author>
    <author><first>Michael</first><last>Kapustin</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>24–33</pages>
    <abstract>We present and discuss a couple of approaches, including different types of projections, and some examples, discussing the use of fuzzy sets for modeling meaning of certain types of language constructs. We are mostly focusing on words other than adjectives and linguistic hedges as these categories are the most studied from before. We discuss logical and linguistic interpretations of membership functions. We argue that using fuzzy sets for modeling meaning of words and other natural language constructs, along with situations described with natural language is interesting both from purely linguistic perspective, and also as a knowledge representation for problems of computational linguistics and natural language processing.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0604</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0604</bibkey>
  </paper>

  <paper id="0605">
    <title>Topological Data Analysis for Discourse Semantics?</title>
    <author><first>Ketki</first><last>Savle</last></author>
    <author><first>Wlodek</first><last>Zadrozny</last></author>
    <author><first>Minwoo</first><last>Lee</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>34–43</pages>
    <abstract>In this paper we present new results on applying topological data analysis to discourse structures. We show that topological information, extracted from the relationships between sentences can be used in inference, namely it can be applied to the very difficult legal entailment given in the COLIEE 2018 data set. Previous results of Doshi and Zadrozny (2018) and Gholizadeh et al. (2018) show that topological features are useful for classification. The applications of computational topology to entailment are novel in our view provide a new set of tools for discourse semantics: computational topology can perhaps provide a bridge between the brittleness of logic and the regression of neural networks. We discuss the advantages and disadvantages of using topological information, and some open problems such as explainability of the classifier decisions.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0605</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0605</bibkey>
  </paper>

  <paper id="0606">
    <title>Semantic Frame Embeddings for Detecting Relations between Software Requirements</title>
    <author><first>Waad</first><last>Alhoshan</last></author>
    <author><first>Riza</first><last>Batista-Navarro</last></author>
    <author><first>Liping</first><last>Zhao</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>44–51</pages>
    <abstract>The early phases of requirements engineering (RE) deal with a vast amount of software requirements (i.e., requirements that define characteristics of software systems), which are typically expressed in natural language. Analysing such unstructured requirements, usually obtained from users’ inputs, is considered a challenging task due to the inherent ambiguity and inconsistency of natural language. To support such a task, methods based on natural language processing (NLP) can be employed. One of the more recent advances in NLP is the use of word embeddings for capturing contextual information, which can then be applied in word analogy tasks. In this paper, we describe a new resource, i.e., embedding-based representations of semantic frames in FrameNet, which was developed to support the detection of relations between software requirements. Our embeddings, which encapsulate contextual information at the semantic frame level, were trained on a large corpus of requirements (i.e., a collection of more than three million mobile application reviews). The similarity between these frame embeddings is then used as a basis for detecting semantic relatedness between software requirements. Compared with existing resources underpinned by word-level embeddings alone, and frame embeddings built upon pre-trained vectors, our proposed frame embeddings obtained better performance against judgements of an RE expert. These encouraging results demonstrate the strong potential of the resource in supporting RE analysis tasks (e.g., traceability), which we plan to investigate as part of our future work.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0606</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0606</bibkey>
  </paper>

  <paper id="0607">
    <title>R-grams: Unsupervised Learning of Semantic Units in Natural Language</title>
    <author><first>Amaru Cuba</first><last>Gyllensten</last></author>
    <author><first>Ariel</first><last>Ekgren</last></author>
    <author><first>Magnus</first><last>Sahlgren</last></author>
    <booktitle>Proceedings of the 13th International Conference on Computational Semantics - Student Papers</booktitle>
    <month>23–27 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>52–62</pages>
    <abstract>This paper investigates data-driven segmentation using Re-Pair or Byte Pair Encoding-techniques. In contrast to previous work which has primarily been focused on subword units for machine translation, we are interested in the general properties of such segments above the word level. We call these segments r-grams, and discuss their properties and the effect they have on the token frequency distribution. The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings. We also provide a number of qualitative examples of the proposed methodology, demonstrating its viability as a language-invariant segmentation procedure.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0607</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0607</bibkey>
  </paper>

  <paper id="0800">
    <title><fixed-case>RELATIONS</fixed-case> - Workshop on meaning relations between phrases and sentences</title>
    <author><first>Venelin</first><last>Kovatchev</last></author>
    <author><first>Darina</first><last>Gold</last></author>
    <author><first>Torsten</first><last>Zesch</last></author>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-08</url>
    <bibtype>proceedings</bibtype>
    <bibkey>W19-0800</bibkey>
  </paper>

  <paper id="0801">
    <title>Assessing the Difficulty of Classifying <fixed-case>C</fixed-case>oncept<fixed-case>N</fixed-case>et Relations in a Multi-Label Classification Setting</title>
    <author><first>Maria</first><last>Becker</last></author>
    <author><first>Michael</first><last>Staniek</last></author>
    <author><first>Vivi</first><last>Nastase</last></author>
    <author><first>Anette</first><last>Frank</last></author>
    <booktitle><fixed-case>RELATIONS</fixed-case> - Workshop on meaning relations between phrases and sentences</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>Commonsense knowledge relations are crucial for advanced NLU tasks. We examine the learnability of such relations as represented in ConceptNet, taking into account their specific properties, which can make relation classification difficult: a given concept pair can be linked by multiple relation types, and relations can have multi-word arguments of diverse semantic types. We explore a neural open world multi-label classification approach that focuses on the evaluation of classification accuracy for individual relations. Based on an in-depth study of the specific properties of the ConceptNet resource, we investigate the impact of different relation representations and model variations. Our analysis reveals that the complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0801</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0801</bibkey>
  </paper>

  <paper id="0802">
    <title>Detecting Collocations Similarity via Logical-Linguistic Model</title>
    <author><first>Nina</first><last>Khairova</last></author>
    <author><first>Svitlana</first><last>Petrasova</last></author>
    <author><first>Orken</first><last>Mamyrbayev</last></author>
    <author><first>Kuralay</first><last>Mukhsina</last></author>
    <booktitle><fixed-case>RELATIONS</fixed-case> - Workshop on meaning relations between phrases and sentences</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>Semantic similarity between collocations, along with words similarity, is one of the main issues of NLP, which must be addressed, in particular, in order to facilitate the automatic thesaurus generation. In the paper, we consider the logical-linguistic model that allows defining the relation of semantic similarity of collocations via the logical-algebraic equations. We provide the model for English, Ukrainian and Russian text corpora. The implementation for each language is slightly different in the equations of the finite predicates algebra and used linguistic resources. As a dataset for our experiment, we use 5801 pairs of sentences of Microsoft Research Paraphrase Corpus for English and more than 1 000 texts of scientific papers for Russian and Ukrainian.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0802</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0802</bibkey>
  </paper>

  <paper id="0803">
    <title>Detecting Paraphrases of Standard Clause Titles in Insurance Contracts</title>
    <author><first>Frieda</first><last>Josi</last></author>
    <author><first>Christian</first><last>Wartena</last></author>
    <author><first>Ulrich</first><last>Heid</last></author>
    <booktitle><fixed-case>RELATIONS</fixed-case> - Workshop on meaning relations between phrases and sentences</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>For the analysis of contract texts, validated model texts, such as model clauses, can be used to identify reused contract clauses. This paper investigates how to calculate the similarity between titles of model clauses and headings extracted from contracts, and which similarity measure is most suitable for this. For the calculation of the similarities between title pairs we tested various variants of string similarity and token based similarity. We also compare two more semantic similarity measures based on word embeddings using pretrained embeddings and word embeddings trained on contract texts. The identification of the model clause title can be used as a starting point for the mapping of clauses found in contracts to verified clauses.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0803</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0803</bibkey>
  </paper>

  <paper id="0804">
    <title>Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications</title>
    <author><first>Mark-Christoph</first><last>Mueller</last></author>
    <booktitle><fixed-case>RELATIONS</fixed-case> - Workshop on meaning relations between phrases and sentences</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.</abstract>
    <url>https://www.aclweb.org/anthology/W19-0804</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-0804</bibkey>
  </paper>

  <paper id="0900">
    <title>Proceedings of the <fixed-case>IWCS</fixed-case> Workshop Vector Semantics for Discourse and Dialogue</title>
    <author><first>Mehrnoosh</first><last>Sadrzadeh</last></author>
    <author><first>Matthew</first><last>Purver</last></author>
    <author><first>Arash</first><last>Eshghi</last></author>
    <author><first>Julian</first><last>Hough</last></author>
    <author><first>Ruth</first><last>Kempson</last></author>
    <author><first>Patrick G. T.</first><last>Healey</last></author>
    <month>24 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-09</url>
    <bibtype>proceedings</bibtype>
    <bibkey>W19-0900</bibkey>
  </paper>

  <paper id="1000">
    <title>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</title>
    <editor><first>Rainer</first><last>Osswald</last></editor>
    <editor><first>Christian</first><last>Retoré</last></editor>
    <editor><first>Peter</first><last>Sutton</last></editor>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-10</url>
    <bibtype>book</bibtype>
    <bibkey>CSTFRS:2019</bibkey>
  </paper>

  <paper id="1001">
    <title>Underspecification and interpretive parallelism in Dependent Type Semantics</title>
    <author><first>Yusuke</first><last>Kubota</last></author>
    <author><first>Koji</first><last>Mineshima</last></author>
    <author><first>Robert</first><last>Levine</last></author>
    <author><first>Daisuke</first><last>Bekki</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>1–9</pages>
    <url>https://www.aclweb.org/anthology/W19-1001</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kubota-EtAl:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1002">
    <title>Translating a Fragment of Natural Deduction System for Natural Language into Modern Type Theory</title>
    <author><first>Ivo</first><last>Pezlar</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>10–18</pages>
    <url>https://www.aclweb.org/anthology/W19-1002</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>pezlar:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1003">
    <title>Modeling the Induced Action Alternation and the Caused-Motion Construction with Tree Adjoining Grammar (<fixed-case>TAG</fixed-case>) and Semantic Frames</title>
    <author><first>Esther</first><last>Seyffarth</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>19–27</pages>
    <url>https://www.aclweb.org/anthology/W19-1003</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>seyffarth:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1004">
    <title>Complex event representation in a typed feature structure implementation of Role and Reference Grammar</title>
    <author><first>Erika</first><last>Bellingham</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>28–36</pages>
    <url>https://www.aclweb.org/anthology/W19-1004</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>bellingham:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1005">
    <title>Computational Syntax-Semantics Interface with Type-Theory of Acyclic Recursion for Underspecified Semantics</title>
    <author><first>Roussanka</first><last>Loukanova</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>37–48</pages>
    <url>https://www.aclweb.org/anthology/W19-1005</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>loukanova:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1006">
    <title>Modeling language constructs with compatibility intervals</title>
    <author><first>Pavlo</first><last>Kapustin</last></author>
    <author><first>Michael</first><last>Kapustin</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>49–54</pages>
    <url>https://www.aclweb.org/anthology/W19-1006</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>kapustin-kapustin:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1007">
    <title><fixed-case>I</fixed-case>mage<fixed-case>TTR</fixed-case>: Grounding Type Theory with Records in Image Classification for Visual Question Answering</title>
    <author><first>Arild</first><last>Matsson</last></author>
    <author><first>Simon</first><last>Dobnik</last></author>
    <author><first>Staffan</first><last>Larsson</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>55–64</pages>
    <url>https://www.aclweb.org/anthology/W19-1007</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>matsson-dobnik-larsson:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1008">
    <title>Enthymemetic Conditionals: Topoi as a guide for acceptability</title>
    <author><first>Eimear</first><last>Maguire</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> 2019 Workshop on Computing Semantics with Types, Frames and Related Structures</booktitle>
    <month>June</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <pages>65–74</pages>
    <url>https://www.aclweb.org/anthology/W19-1008</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>maguire:2019:CSTFRS</bibkey>
  </paper>

  <paper id="1200">
    <title>Proceedings of the <fixed-case>IWCS</fixed-case> Shared Task on Semantic Parsing</title>
    <author><first>Lasha</first><last>Abzianidze</last></author>
    <author><first>Rik</first><last>van Noord</last></author>
    <author><first>Hessel</first><last>Haagsma</last></author>
    <author><first>Johan</first><last>Bos</last></author>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <url>https://www.aclweb.org/anthology/W19-12</url>
    <bibtype>proceedings</bibtype>
    <bibkey>W19-1200</bibkey>
  </paper>

  <paper id="1201">
    <title>The First Shared Task on Discourse Representation Structure Parsing</title>
    <author><first>Lasha</first><last>Abzianidze</last></author>
    <author><first>Rik</first><last>van Noord</last></author>
    <author><first>Hessel</first><last>Haagsma</last></author>
    <author><first>Johan</first><last>Bos</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> Shared Task on Semantic Parsing</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>The paper presents the IWCS 2019 shared task on semantic parsing where the goal is to produce Discourse Representation Structures (DRSs) for English sentences. DRSs originate from Discourse Representation Theory and represent scoped meaning representations that capture the semantics of negation, modals, quantification, and presupposition triggers. Additionally, concepts and event-participants in DRSs are described with WordNet synsets and the thematic roles from VerbNet. To measure similarity between two DRSs, they are represented in a clausal form, i.e. as a set of tuples. Participant systems were expected to produce DRSs in this clausal form. Taking into account the rich lexical information, explicit scope marking, a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser.</abstract>
    <url>https://www.aclweb.org/anthology/W19-1201</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-1201</bibkey>
  </paper>

  <paper id="1202">
    <title>Transition-based <fixed-case>DRS</fixed-case> Parsing Using Stack-<fixed-case>LSTM</fixed-case>s</title>
    <author><first>Kilian</first><last>Evang</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> Shared Task on Semantic Parsing</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>We present our submission to the IWCS 2019 shared task on semantic parsing, a transition-based parser that uses explicit word-meaning pairings, but no explicit representation of syntax. Parsing decisions are made based on vector representations of parser states, encoded via stack-LSTMs (Ballesteros et al., 2017), as well as some heuristic rules. Our system reaches 70.88% f-score in the competition.</abstract>
    <url>https://www.aclweb.org/anthology/W19-1202</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-1202</bibkey>
  </paper>

  <paper id="1203">
    <title>Discourse Representation Structure Parsing with Recurrent Neural Networks and the Transformer Model</title>
    <author><first>Jiangming</first><last>Liu</last></author>
    <author><first>Shay B.</first><last>Cohen</last></author>
    <author><first>Mirella</first><last>Lapata</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> Shared Task on Semantic Parsing</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>We describe the systems we developed for Discourse Representation Structure (DRS) parsing as part of the IWCS-2019 Shared Task of DRS Parsing.1 Our systems are based on sequence-to- sequence modeling. To implement our model, we use the open-source neural machine translation system implemented in PyTorch, OpenNMT-py. We experimented with a variety of encoder-decoder models based on recurrent neural networks and the Transformer model. We conduct experiments on the standard benchmark of the Parallel Meaning Bank (PMB 2.2). Our best system achieves a score of 84.8% F1 in the DRS parsing shared task.</abstract>
    <url>https://www.aclweb.org/anthology/W19-1203</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-1203</bibkey>
  </paper>

  <paper id="1204">
    <title>Neural Boxer at the <fixed-case>IWCS</fixed-case> Shared Task on <fixed-case>DRS</fixed-case> Parsing</title>
    <author><first>Rik</first><last>van Noord</last></author>
    <booktitle>Proceedings of the <fixed-case>IWCS</fixed-case> Shared Task on Semantic Parsing</booktitle>
    <month>23 May</month>
    <year>2019</year>
    <address>Gothenburg, Sweden</address>
    <publisher>Association for Computational Linguistics</publisher>
    <abstract>This paper describes our participation in the shared task of Discourse Representation Structure parsing. It follows the work of Van Noord et al. (2018), who employed a neural sequence-to-sequence model to produce DRSs, also exploiting linguistic information with multiple encoders. We provide a detailed look in the performance of this model and show that (i) the benefit of the linguistic features is evident across a number of experiments which vary the amount of training data and (ii) the model can be improved by applying a number of postprocessing methods to fix ill-formed output. Our model ended up in second place in the competition, with an F-score of 84.5.</abstract>
    <url>https://www.aclweb.org/anthology/W19-1204</url>
    <bibtype>inproceedings</bibtype>
    <bibkey>W19-1204</bibkey>
  </paper>

</volume>
